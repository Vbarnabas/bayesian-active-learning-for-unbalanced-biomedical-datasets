{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj9P4P-WZ2OQ",
        "outputId": "6972d536-c185-4d83-89db-e9a175a5555a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyro-ppl in ./.venv/lib/python3.12/site-packages (1.9.1)\n",
            "Requirement already satisfied: numpy>=1.7 in ./.venv/lib/python3.12/site-packages (from pyro-ppl) (2.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.12/site-packages (from pyro-ppl) (3.4.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in ./.venv/lib/python3.12/site-packages (from pyro-ppl) (0.1.2)\n",
            "Requirement already satisfied: torch>=2.0 in ./.venv/lib/python3.12/site-packages (from pyro-ppl) (2.4.1)\n",
            "Requirement already satisfied: tqdm>=4.36 in ./.venv/lib/python3.12/site-packages (from pyro-ppl) (4.66.5)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch>=2.0->pyro-ppl) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->pyro-ppl) (4.12.2)\n",
            "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch>=2.0->pyro-ppl) (1.13.3)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0->pyro-ppl) (3.4)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->pyro-ppl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch>=2.0->pyro-ppl) (2024.9.0)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0->pyro-ppl) (75.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0->pyro-ppl) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->torch>=2.0->pyro-ppl) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyro-ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "FO0GqqXEOHRv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocessing import get_context\n",
        "import torch\n",
        "import pyro\n",
        "from pyro.infer import MCMC, NUTS\n",
        "import pyro.distributions as dist\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from typing import Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3IJBCCJLEe4_"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load the dataset\n",
        "# Assuming the dataset is in CSV format and you have the path to it\n",
        "data = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9seRbzzzDfpV"
      },
      "outputs": [],
      "source": [
        "# Step 2: Prepare features (X) and target (y)\n",
        "X = data.drop(columns=['vital.status'])  # Drop the target column to get the features\n",
        "y = data['vital.status']  # Target variable (0 = survived, 1 = died)\n",
        "\n",
        "# Step 3: Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AWf20XFEmBx",
        "outputId": "6499ded5-bbb8-4a79-f2c9-218381e91dfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training time: 0.12465906143188477 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 4: Initialize and train the Logistic Regression model\n",
        "start_time = time.time()\n",
        "asd = 1;\n",
        "log_reg = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "log_reg.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Training time: {end_time - start_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ24NHcHEokV",
        "outputId": "f7bf7c1a-1e01-47c1-e069-26f4138e114e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 82.98%\n",
            "Confusion Matrix:\n",
            "[[115  10]\n",
            " [ 14   2]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.92      0.91       125\n",
            "           1       0.17      0.12      0.14        16\n",
            "\n",
            "    accuracy                           0.83       141\n",
            "   macro avg       0.53      0.52      0.52       141\n",
            "weighted avg       0.81      0.83      0.82       141\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsv6HzLcH_rY"
      },
      "source": [
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "b3RLMiNlIBxu",
        "outputId": "ab747d36-78d5-44b4-f5c4-c5ab4fdf3eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Training time: 1.3066012859344482 seconds\n",
            "MLP Accuracy: 0.8652482269503546\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.93       125\n",
            "           1       0.36      0.25      0.30        16\n",
            "\n",
            "    accuracy                           0.87       141\n",
            "   macro avg       0.64      0.60      0.61       141\n",
            "weighted avg       0.85      0.87      0.85       141\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# MLP Classifier (Multi-Layer Perceptron)\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)  # You can adjust hidden_layer_sizes\n",
        "\n",
        "# Timing the MLP training\n",
        "start_time = time.time()\n",
        "mlp_model.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "mlp_train_time = end_time - start_time\n",
        "\n",
        "# Predict and evaluate MLP model\n",
        "mlp_preds = mlp_model.predict(X_test)\n",
        "mlp_accuracy = accuracy_score(y_test, mlp_preds)\n",
        "\n",
        "print(f\"MLP Training time: {mlp_train_time} seconds\")\n",
        "print(f\"MLP Accuracy: {mlp_accuracy}\")\n",
        "\n",
        "# Confusion Matrix for MLP Model\n",
        "mlp_cm = confusion_matrix(y_test, mlp_preds)\n",
        "\n",
        "mlp_class_report = classification_report(y_test, mlp_preds)\n",
        "\n",
        "print(mlp_class_report)\n",
        "\n",
        "# Plotting the confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(mlp_cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix for MLP Classifier')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcH7UliGKBrG"
      },
      "source": [
        "Bayesian Logical Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kaW-lEpaeOoL"
      },
      "outputs": [],
      "source": [
        "scaler=StandardScaler()\n",
        "\n",
        "X = data.drop(columns=['vital.status'])\n",
        "y = data['vital.status']\n",
        "scaled_data=scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Prepare features (X) and target (y)\n",
        "  # Drop the target column to get the features\n",
        "X = torch.tensor(X.values, dtype=torch.float32)\n",
        "y = torch.tensor(y.values, dtype=torch.int32)\n",
        "  # Target variable (0 = survived, 1 = died)\n",
        "\n",
        "# Step 3: Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jn3MFVOkJP6"
      },
      "source": [
        "MCMC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VGUL7uVwkIJa"
      },
      "outputs": [],
      "source": [
        "NUM_OF_SAMPLES = 1000\n",
        "NUM_OF_CHAINS = 1\n",
        "\n",
        "\n",
        "\n",
        "def logistic_regression_model(features: torch.Tensor, target: torch.Tensor):\n",
        "    # sample from prior\n",
        "    W = pyro.sample(\n",
        "        \"weight\", dist.Normal(torch.zeros(1, features.shape[1]), torch.ones(1, features.shape[1])).independent(1)\n",
        "    )\n",
        "    b = pyro.sample(\n",
        "        \"bias\", dist.Normal(torch.zeros(1), torch.ones(1)).independent(1)\n",
        "    )\n",
        "    with pyro.plate(\"data\", features.size(0)):\n",
        "        # Generate raw model logits\n",
        "        model_logits = (torch.matmul(features, W.permute(1, 0)) + b).squeeze()\n",
        "        # Apply sigmoid function\n",
        "        probs = torch.sigmoid(model_logits)\n",
        "        pyro.sample(\n",
        "            \"obs\",\n",
        "            dist.Bernoulli(probs=probs),\n",
        "            obs=target.squeeze())\n",
        "\n",
        "def run_inference(features: torch.Tensor, target: torch.Tensor, number_of_samples: int, number_of_chains:int) -> MCMC:\n",
        "    # Initialise NUTS sampler\n",
        "    kernel = NUTS(logistic_regression_model)\n",
        "    # Initialise MCMC class\n",
        "    mcmc = MCMC(kernel, num_samples = number_of_samples, warmup_steps = 200, num_chains=number_of_chains)\n",
        "    # Run sampling\n",
        "    mcmc.run(features.float(), target.float())\n",
        "    return mcmc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8DjZjnJoe3b",
        "outputId": "93f7b049-77d7-4629-fb85-34b6b614882e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sample: 100%|██████████| 1200/1200 [3:17:39,  9.88s/it, step size=3.88e-03, acc. prob=0.956]\n"
          ]
        }
      ],
      "source": [
        "mcmc = run_inference(X_train, y_train, number_of_samples=NUM_OF_SAMPLES, number_of_chains=NUM_OF_CHAINS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UU_axSlu8lQk",
        "outputId": "3f948165-7e30-4c8f-9ac9-f831d58006e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                    mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
            "       bias[0]     -0.04      1.00     -0.05     -1.74      1.53   2710.31      1.00\n",
            "   weight[0,0]      0.57      0.58      0.58     -0.45      1.44    606.72      1.00\n",
            "   weight[0,1]     -0.59      0.69     -0.57     -1.62      0.61    694.55      1.00\n",
            "   weight[0,2]      0.19      0.80      0.19     -1.14      1.45    880.13      1.00\n",
            "   weight[0,3]      0.26      0.81      0.22     -1.02      1.57    648.17      1.00\n",
            "   weight[0,4]     -0.89      0.83     -0.88     -2.16      0.56    920.03      1.00\n",
            "   weight[0,5]      0.46      0.77      0.44     -0.85      1.64   1060.07      1.00\n",
            "   weight[0,6]     -0.68      0.55     -0.68     -1.63      0.14    463.47      1.00\n",
            "   weight[0,7]      0.25      0.80      0.24     -1.00      1.60    747.44      1.00\n",
            "   weight[0,8]     -0.10      0.92     -0.10     -1.51      1.49   1422.60      1.00\n",
            "   weight[0,9]      0.66      0.88      0.67     -0.79      2.03   1096.24      1.00\n",
            "  weight[0,10]      0.73      0.78      0.71     -0.53      2.04   1043.58      1.00\n",
            "  weight[0,11]      0.35      0.82      0.35     -1.10      1.64    842.33      1.00\n",
            "  weight[0,12]     -0.04      0.80     -0.07     -1.31      1.30    692.59      1.00\n",
            "  weight[0,13]      0.04      0.79      0.00     -1.26      1.31    829.29      1.00\n",
            "  weight[0,14]      0.96      0.71      0.98     -0.34      1.98    594.07      1.00\n",
            "  weight[0,15]     -0.38      0.71     -0.39     -1.42      0.92   1002.71      1.00\n",
            "  weight[0,16]     -0.65      0.81     -0.66     -1.91      0.69    781.46      1.00\n",
            "  weight[0,17]      0.18      0.91      0.16     -1.21      1.82   1180.86      1.00\n",
            "  weight[0,18]     -0.23      0.79     -0.22     -1.51      1.03   1351.29      1.00\n",
            "  weight[0,19]      0.58      0.83      0.58     -0.62      2.13    512.78      1.00\n",
            "  weight[0,20]     -0.32      0.79     -0.33     -1.68      0.93    767.84      1.00\n",
            "  weight[0,21]     -0.95      0.77     -0.94     -2.15      0.32    863.32      1.00\n",
            "  weight[0,22]     -0.82      0.79     -0.82     -2.12      0.46    707.21      1.00\n",
            "  weight[0,23]      0.73      0.81      0.73     -0.60      2.07    941.22      1.00\n",
            "  weight[0,24]      0.26      0.88      0.24     -1.12      1.65   1055.65      1.00\n",
            "  weight[0,25]      1.25      0.73      1.28      0.13      2.43    628.04      1.01\n",
            "  weight[0,26]     -0.42      0.78     -0.45     -1.59      0.91    694.72      1.00\n",
            "  weight[0,27]      0.22      0.90      0.20     -1.19      1.70    543.31      1.00\n",
            "  weight[0,28]     -0.16      0.88     -0.15     -1.52      1.32   1409.64      1.00\n",
            "  weight[0,29]      0.35      0.73      0.33     -0.76      1.69    737.08      1.00\n",
            "  weight[0,30]      0.12      0.82      0.10     -1.30      1.37    828.46      1.00\n",
            "  weight[0,31]     -0.09      0.73     -0.05     -1.28      1.18    695.94      1.00\n",
            "  weight[0,32]     -0.63      0.74     -0.66     -1.83      0.57   1059.70      1.00\n",
            "  weight[0,33]     -0.09      0.92     -0.07     -1.55      1.42    444.37      1.00\n",
            "  weight[0,34]      0.19      0.94      0.19     -1.30      1.67   1080.05      1.00\n",
            "  weight[0,35]      0.21      0.83      0.19     -1.27      1.49    867.99      1.00\n",
            "  weight[0,36]     -0.56      0.79     -0.54     -1.65      0.85   1042.82      1.00\n",
            "  weight[0,37]      0.30      0.85      0.32     -0.99      1.71    901.42      1.00\n",
            "  weight[0,38]     -0.27      0.60     -0.26     -1.23      0.70    697.19      1.00\n",
            "  weight[0,39]     -0.33      0.96     -0.34     -1.78      1.24   1423.95      1.01\n",
            "  weight[0,40]     -0.83      0.85     -0.86     -2.21      0.54    610.54      1.00\n",
            "  weight[0,41]     -0.21      0.90     -0.23     -1.68      1.20   1101.80      1.00\n",
            "  weight[0,42]      0.11      0.83      0.11     -1.32      1.40    645.95      1.00\n",
            "  weight[0,43]     -0.07      0.91     -0.07     -1.55      1.42   1204.57      1.00\n",
            "  weight[0,44]     -0.51      0.93     -0.49     -2.08      1.00    838.34      1.00\n",
            "  weight[0,45]      0.33      0.79      0.30     -0.81      1.69    576.95      1.00\n",
            "  weight[0,46]     -0.12      0.80     -0.13     -1.44      1.14    737.99      1.00\n",
            "  weight[0,47]      0.54      0.76      0.52     -0.60      1.89   1215.47      1.00\n",
            "  weight[0,48]     -0.42      0.85     -0.43     -1.85      1.01   1210.67      1.00\n",
            "  weight[0,49]      0.28      0.92      0.28     -1.29      1.69   1291.91      1.00\n",
            "  weight[0,50]     -0.13      0.79     -0.14     -1.37      1.13    907.19      1.00\n",
            "  weight[0,51]      0.10      0.86      0.13     -1.29      1.54    748.85      1.00\n",
            "  weight[0,52]     -0.01      0.76     -0.01     -1.17      1.27    536.53      1.01\n",
            "  weight[0,53]     -0.66      0.72     -0.69     -1.75      0.57    805.23      1.00\n",
            "  weight[0,54]      0.16      0.81      0.14     -1.21      1.40   1265.12      1.00\n",
            "  weight[0,55]     -0.48      0.90     -0.50     -2.03      0.91   1103.50      1.00\n",
            "  weight[0,56]     -0.30      0.79     -0.33     -1.52      0.97   1657.60      1.00\n",
            "  weight[0,57]     -0.53      0.77     -0.52     -1.81      0.72    783.84      1.00\n",
            "  weight[0,58]      0.23      0.87      0.24     -1.13      1.72   1407.46      1.00\n",
            "  weight[0,59]     -0.21      0.82     -0.24     -1.73      1.02    764.45      1.00\n",
            "  weight[0,60]     -0.02      0.88     -0.03     -1.53      1.35    832.47      1.00\n",
            "  weight[0,61]      0.57      0.77      0.54     -0.59      1.90    239.16      1.00\n",
            "  weight[0,62]     -0.92      0.86     -0.95     -2.34      0.52   1675.04      1.00\n",
            "  weight[0,63]     -0.20      0.92     -0.18     -1.75      1.26   1059.05      1.00\n",
            "  weight[0,64]      0.35      0.86      0.36     -0.95      1.74   1584.17      1.00\n",
            "  weight[0,65]     -0.21      0.82     -0.21     -1.54      1.18    707.26      1.01\n",
            "  weight[0,66]     -0.10      0.94     -0.10     -1.56      1.48    713.40      1.00\n",
            "  weight[0,67]      0.48      0.81      0.44     -0.82      1.89    896.00      1.00\n",
            "  weight[0,68]      1.06      0.83      1.09     -0.19      2.54   1108.83      1.00\n",
            "  weight[0,69]      0.21      0.87      0.19     -1.20      1.60   1035.06      1.00\n",
            "  weight[0,70]      0.64      0.78      0.65     -0.62      1.90    754.33      1.00\n",
            "  weight[0,71]     -0.37      0.76     -0.41     -1.51      0.98    712.88      1.00\n",
            "  weight[0,72]      0.61      0.74      0.62     -0.59      1.87    624.49      1.00\n",
            "  weight[0,73]     -0.48      0.75     -0.48     -1.73      0.66    931.40      1.00\n",
            "  weight[0,74]     -0.01      0.77     -0.00     -1.33      1.16    582.45      1.00\n",
            "  weight[0,75]      0.02      0.83      0.04     -1.26      1.43   1145.58      1.00\n",
            "  weight[0,76]     -0.25      0.79     -0.23     -1.56      0.96    614.02      1.00\n",
            "  weight[0,77]     -0.49      0.75     -0.45     -1.62      0.80    596.88      1.00\n",
            "  weight[0,78]     -0.29      0.83     -0.31     -1.59      1.12   1239.59      1.00\n",
            "  weight[0,79]      0.11      0.89      0.12     -1.25      1.62   1030.09      1.00\n",
            "  weight[0,80]      1.28      0.75      1.28     -0.00      2.43    733.79      1.00\n",
            "  weight[0,81]      0.24      0.90      0.25     -1.34      1.59   1096.10      1.00\n",
            "  weight[0,82]     -0.03      0.81     -0.03     -1.31      1.25    940.88      1.00\n",
            "  weight[0,83]      0.04      0.94      0.03     -1.63      1.47   1325.28      1.00\n",
            "  weight[0,84]     -0.04      0.93     -0.06     -1.39      1.60   1028.49      1.00\n",
            "  weight[0,85]     -0.09      0.93     -0.07     -1.55      1.47   2105.55      1.00\n",
            "  weight[0,86]     -0.53      0.81     -0.51     -1.96      0.63   1001.56      1.00\n",
            "  weight[0,87]      0.34      0.76      0.34     -0.86      1.59   1395.51      1.00\n",
            "  weight[0,88]      0.48      0.84      0.49     -0.78      1.94   1497.94      1.00\n",
            "  weight[0,89]      0.42      0.87      0.40     -0.94      1.81   1851.22      1.00\n",
            "  weight[0,90]     -0.30      0.80     -0.31     -1.73      0.94    930.58      1.00\n",
            "  weight[0,91]     -0.39      0.83     -0.35     -1.78      0.96   1333.01      1.00\n",
            "  weight[0,92]      0.38      0.88      0.38     -1.01      1.79    941.65      1.00\n",
            "  weight[0,93]      0.70      0.92      0.68     -0.81      2.16    941.38      1.00\n",
            "  weight[0,94]     -0.60      0.87     -0.62     -2.03      0.73   1466.13      1.00\n",
            "  weight[0,95]      0.60      0.76      0.61     -0.58      1.84    693.64      1.00\n",
            "  weight[0,96]      0.04      0.87      0.04     -1.49      1.33   1161.48      1.00\n",
            "  weight[0,97]      0.13      0.94      0.16     -1.38      1.62   1479.74      1.00\n",
            "  weight[0,98]     -0.34      0.82     -0.34     -1.50      1.15   1062.95      1.00\n",
            "  weight[0,99]     -0.07      0.92     -0.09     -1.58      1.47    642.98      1.00\n",
            " weight[0,100]      1.04      0.76      1.06     -0.19      2.32    709.46      1.00\n",
            " weight[0,101]      0.01      0.80      0.04     -1.29      1.29   1125.40      1.00\n",
            " weight[0,102]      0.70      0.79      0.69     -0.74      1.85    838.48      1.01\n",
            " weight[0,103]      0.86      0.92      0.86     -0.52      2.49    754.19      1.00\n",
            " weight[0,104]      0.42      0.93      0.44     -1.08      1.96   1040.56      1.00\n",
            " weight[0,105]      0.20      0.91      0.18     -1.37      1.61   1052.39      1.00\n",
            " weight[0,106]      0.36      0.81      0.38     -0.84      1.77    511.70      1.00\n",
            " weight[0,107]     -0.13      0.88     -0.12     -1.69      1.17    734.67      1.00\n",
            " weight[0,108]     -0.97      0.69     -0.97     -2.08      0.13   1119.54      1.00\n",
            " weight[0,109]     -0.20      0.92     -0.20     -1.74      1.19   1449.72      1.00\n",
            " weight[0,110]     -0.72      0.89     -0.73     -2.14      0.82    948.38      1.00\n",
            " weight[0,111]      0.39      0.79      0.41     -0.78      1.79   1276.72      1.00\n",
            " weight[0,112]     -1.03      0.84     -1.06     -2.34      0.41    671.50      1.00\n",
            " weight[0,113]      0.10      0.74      0.08     -0.94      1.42    901.75      1.00\n",
            " weight[0,114]     -0.65      0.92     -0.65     -2.22      0.83    855.86      1.00\n",
            " weight[0,115]      0.15      0.84      0.17     -1.25      1.51   1010.96      1.00\n",
            " weight[0,116]      0.97      0.79      0.96     -0.35      2.21   1135.95      1.00\n",
            " weight[0,117]      0.17      0.88      0.18     -1.29      1.61   1042.27      1.00\n",
            " weight[0,118]      0.21      0.95      0.21     -1.27      1.90    800.19      1.00\n",
            " weight[0,119]     -0.40      0.85     -0.43     -1.91      0.87   1096.87      1.00\n",
            " weight[0,120]      0.04      0.77      0.04     -1.18      1.28    809.17      1.00\n",
            " weight[0,121]     -0.39      0.80     -0.39     -1.70      0.89    578.17      1.00\n",
            " weight[0,122]     -0.42      0.91     -0.44     -1.92      1.05   1879.09      1.00\n",
            " weight[0,123]     -0.53      0.84     -0.53     -2.03      0.78   1370.58      1.00\n",
            " weight[0,124]     -0.15      0.94     -0.12     -1.62      1.45   1614.13      1.00\n",
            " weight[0,125]      0.03      0.76      0.06     -1.10      1.38    599.32      1.00\n",
            " weight[0,126]      0.31      0.81      0.29     -0.84      1.83   1164.86      1.00\n",
            " weight[0,127]      0.32      0.88      0.32     -1.20      1.70   1548.10      1.00\n",
            " weight[0,128]     -0.28      0.79     -0.28     -1.55      1.00    537.84      1.00\n",
            " weight[0,129]     -0.31      0.83     -0.33     -1.75      0.98    604.89      1.00\n",
            " weight[0,130]      0.32      0.83      0.29     -0.99      1.74    510.12      1.00\n",
            " weight[0,131]      0.45      0.90      0.40     -1.08      1.86    866.92      1.00\n",
            " weight[0,132]      0.75      0.81      0.75     -0.65      2.00    642.93      1.00\n",
            " weight[0,133]      0.21      0.87      0.22     -1.38      1.46    862.69      1.00\n",
            " weight[0,134]      0.14      0.78      0.12     -0.93      1.63    805.45      1.00\n",
            " weight[0,135]      0.61      0.88      0.60     -0.71      2.25   1127.09      1.00\n",
            " weight[0,136]     -0.04      0.66     -0.03     -1.20      0.96    636.97      1.00\n",
            " weight[0,137]      0.36      0.97      0.34     -1.16      2.01    874.39      1.00\n",
            " weight[0,138]     -0.23      0.85     -0.22     -1.74      0.99   1605.57      1.00\n",
            " weight[0,139]     -0.09      0.82     -0.05     -1.52      1.20    637.68      1.00\n",
            " weight[0,140]      0.74      0.87      0.73     -0.64      2.12   1186.49      1.00\n",
            " weight[0,141]      0.25      0.77      0.22     -0.86      1.61    567.06      1.00\n",
            " weight[0,142]     -0.06      0.85     -0.05     -1.26      1.60   1135.64      1.00\n",
            " weight[0,143]     -0.40      0.83     -0.40     -1.68      0.95    625.16      1.00\n",
            " weight[0,144]      0.47      0.80      0.48     -0.93      1.71    737.35      1.00\n",
            " weight[0,145]     -0.40      0.87     -0.41     -1.71      1.15   1663.71      1.00\n",
            " weight[0,146]     -0.37      0.83     -0.37     -1.69      0.98    694.89      1.00\n",
            " weight[0,147]      0.03      0.83      0.02     -1.32      1.40    706.82      1.01\n",
            " weight[0,148]      0.35      0.74      0.34     -0.89      1.55    993.55      1.00\n",
            " weight[0,149]      0.41      0.81      0.43     -0.86      1.77   1006.94      1.00\n",
            " weight[0,150]     -0.01      0.96     -0.05     -1.66      1.43   1080.67      1.00\n",
            " weight[0,151]     -0.30      0.82     -0.31     -1.59      1.03    661.21      1.00\n",
            " weight[0,152]     -0.61      0.87     -0.63     -1.98      0.79    824.13      1.00\n",
            " weight[0,153]     -0.54      0.91     -0.54     -2.10      0.85   1075.39      1.00\n",
            " weight[0,154]     -0.24      0.87     -0.24     -1.80      1.03   1464.59      1.00\n",
            " weight[0,155]      0.81      0.90      0.77     -0.68      2.22   1014.37      1.00\n",
            " weight[0,156]     -0.44      0.95     -0.45     -1.91      1.15   1338.19      1.00\n",
            " weight[0,157]     -0.58      0.95     -0.59     -2.15      0.97   1175.78      1.00\n",
            " weight[0,158]     -0.39      0.82     -0.38     -1.76      0.93    770.33      1.00\n",
            " weight[0,159]     -0.09      0.88     -0.08     -1.56      1.33   1033.78      1.00\n",
            " weight[0,160]      0.12      0.86      0.15     -1.35      1.49    829.86      1.00\n",
            " weight[0,161]     -0.23      0.80     -0.19     -1.58      1.01    509.04      1.00\n",
            " weight[0,162]      0.56      0.79      0.54     -0.65      1.96    887.55      1.00\n",
            " weight[0,163]      0.60      0.86      0.59     -0.75      1.96    909.35      1.00\n",
            " weight[0,164]      0.15      1.00      0.10     -1.34      1.90    713.45      1.00\n",
            " weight[0,165]      0.07      0.88      0.09     -1.40      1.46    543.42      1.00\n",
            " weight[0,166]      0.66      0.80      0.66     -0.59      2.02    368.57      1.00\n",
            " weight[0,167]      0.60      0.88      0.63     -0.87      1.94   1342.78      1.00\n",
            " weight[0,168]     -0.16      0.88     -0.16     -1.63      1.31   1114.89      1.00\n",
            " weight[0,169]     -0.73      0.80     -0.74     -1.96      0.61    800.07      1.00\n",
            " weight[0,170]     -0.46      0.79     -0.50     -1.60      0.91    798.47      1.00\n",
            " weight[0,171]      0.55      0.93      0.53     -1.00      1.98   1628.07      1.00\n",
            " weight[0,172]      0.46      0.87      0.47     -1.12      1.69    751.70      1.00\n",
            " weight[0,173]     -0.17      0.95     -0.15     -1.77      1.29    588.94      1.00\n",
            " weight[0,174]      0.62      0.91      0.62     -0.81      2.13   1582.82      1.00\n",
            " weight[0,175]     -0.50      0.86     -0.48     -2.05      0.80   1233.86      1.00\n",
            " weight[0,176]      0.18      0.78      0.18     -1.10      1.38    795.56      1.00\n",
            " weight[0,177]     -0.28      0.81     -0.27     -1.68      0.92   1205.74      1.00\n",
            " weight[0,178]      0.30      0.82      0.34     -1.03      1.64    976.00      1.00\n",
            " weight[0,179]      0.68      0.87      0.70     -0.77      2.07   1313.12      1.00\n",
            " weight[0,180]      0.56      0.76      0.57     -0.70      1.78   1109.69      1.00\n",
            " weight[0,181]      0.87      0.82      0.84     -0.48      2.10    491.54      1.00\n",
            " weight[0,182]     -0.50      0.89     -0.51     -2.10      0.82   1215.17      1.00\n",
            " weight[0,183]      0.01      0.91     -0.03     -1.58      1.43   1125.68      1.00\n",
            " weight[0,184]     -1.09      0.92     -1.14     -2.52      0.61    790.47      1.00\n",
            " weight[0,185]     -0.14      0.91     -0.16     -1.61      1.38   1883.97      1.00\n",
            " weight[0,186]      0.39      0.78      0.39     -0.91      1.56    685.28      1.00\n",
            " weight[0,187]      0.33      0.79      0.35     -0.99      1.56    902.78      1.00\n",
            " weight[0,188]     -0.32      0.84     -0.27     -1.79      0.94   1208.84      1.00\n",
            " weight[0,189]     -0.68      0.86     -0.68     -2.09      0.69    875.74      1.00\n",
            " weight[0,190]      0.12      0.84      0.11     -1.16      1.55    739.85      1.00\n",
            " weight[0,191]      0.36      0.86      0.36     -0.88      1.87   1115.01      1.00\n",
            " weight[0,192]      0.71      0.88      0.71     -0.62      2.27   1188.19      1.00\n",
            " weight[0,193]     -0.78      0.88     -0.78     -2.22      0.50   1085.72      1.00\n",
            " weight[0,194]      0.29      0.84      0.29     -0.95      1.85    640.26      1.00\n",
            " weight[0,195]      0.40      0.87      0.37     -1.04      1.79   1205.17      1.00\n",
            " weight[0,196]     -0.46      0.92     -0.46     -2.00      1.03   1087.73      1.00\n",
            " weight[0,197]     -0.10      0.92     -0.08     -1.47      1.49   1127.32      1.00\n",
            " weight[0,198]     -0.30      0.90     -0.31     -1.86      1.06   1225.35      1.00\n",
            " weight[0,199]     -0.59      0.80     -0.58     -1.81      0.75   1009.81      1.00\n",
            " weight[0,200]      0.38      0.87      0.38     -0.92      1.89   1198.31      1.00\n",
            " weight[0,201]     -0.42      0.82     -0.42     -1.82      0.80    731.78      1.00\n",
            " weight[0,202]     -0.02      0.92     -0.01     -1.44      1.54    775.73      1.00\n",
            " weight[0,203]     -0.22      0.85     -0.24     -1.60      1.12   1147.86      1.00\n",
            " weight[0,204]     -0.35      0.79     -0.33     -1.51      1.13    697.71      1.00\n",
            " weight[0,205]      0.05      0.83      0.02     -1.45      1.35   1019.27      1.00\n",
            " weight[0,206]      0.03      0.96      0.07     -1.54      1.62    817.89      1.00\n",
            " weight[0,207]     -0.26      0.90     -0.28     -1.83      1.03   1397.42      1.00\n",
            " weight[0,208]     -0.75      0.86     -0.75     -2.07      0.69    811.25      1.00\n",
            " weight[0,209]     -0.41      0.86     -0.42     -1.95      0.76   1197.87      1.00\n",
            " weight[0,210]      0.09      0.80      0.07     -1.17      1.38   1015.57      1.00\n",
            " weight[0,211]      0.11      0.81      0.14     -1.21      1.40    762.74      1.00\n",
            " weight[0,212]     -0.70      0.80     -0.70     -2.04      0.58    715.39      1.00\n",
            " weight[0,213]     -1.38      0.91     -1.35     -2.94      0.01    635.58      1.01\n",
            " weight[0,214]     -0.30      0.96     -0.28     -1.92      1.14    506.57      1.00\n",
            " weight[0,215]     -0.23      0.87     -0.21     -1.71      1.06    996.65      1.00\n",
            " weight[0,216]      0.12      0.88      0.11     -1.20      1.70    887.39      1.00\n",
            " weight[0,217]     -0.40      0.89     -0.38     -1.81      1.05   1863.95      1.00\n",
            " weight[0,218]      0.25      0.82      0.28     -1.18      1.52   1222.22      1.00\n",
            " weight[0,219]      0.40      0.85      0.44     -0.92      1.92    902.86      1.00\n",
            " weight[0,220]     -0.57      0.90     -0.60     -2.07      0.83    908.86      1.00\n",
            " weight[0,221]      0.97      0.88      0.98     -0.33      2.46   1439.46      1.00\n",
            " weight[0,222]      0.59      0.89      0.62     -1.12      1.77    664.59      1.00\n",
            " weight[0,223]      0.17      0.89      0.18     -1.16      1.67   1488.96      1.00\n",
            " weight[0,224]      0.22      0.90      0.24     -1.27      1.72   1164.81      1.00\n",
            " weight[0,225]      0.16      0.85      0.15     -1.30      1.48   1642.15      1.00\n",
            " weight[0,226]      1.52      0.87      1.48      0.17      3.03    862.02      1.00\n",
            " weight[0,227]      0.53      0.77      0.58     -0.75      1.76    606.89      1.00\n",
            " weight[0,228]      0.08      0.80      0.08     -1.19      1.46   1054.47      1.00\n",
            " weight[0,229]     -0.07      0.86     -0.05     -1.63      1.24    764.72      1.00\n",
            " weight[0,230]     -0.36      0.92     -0.32     -1.79      1.22   1124.61      1.00\n",
            " weight[0,231]      0.82      0.86      0.82     -0.69      2.11   1126.47      1.00\n",
            " weight[0,232]     -0.05      0.86     -0.04     -1.50      1.29    872.68      1.00\n",
            " weight[0,233]     -0.45      0.91     -0.43     -1.92      1.01    974.86      1.00\n",
            " weight[0,234]      0.40      0.87      0.40     -1.18      1.70   1054.16      1.00\n",
            " weight[0,235]     -0.35      0.98     -0.34     -1.90      1.28    705.24      1.00\n",
            " weight[0,236]     -0.11      0.76     -0.11     -1.22      1.27    805.99      1.00\n",
            " weight[0,237]     -0.80      0.88     -0.82     -2.36      0.54    525.20      1.00\n",
            " weight[0,238]      0.91      0.93      0.88     -0.60      2.38    702.85      1.00\n",
            " weight[0,239]     -0.74      0.80     -0.73     -1.98      0.55    993.27      1.00\n",
            " weight[0,240]      0.15      0.87      0.16     -1.18      1.63   1312.72      1.00\n",
            " weight[0,241]     -0.09      0.95     -0.11     -1.70      1.45    998.76      1.01\n",
            " weight[0,242]      0.01      0.97      0.00     -1.62      1.58   1664.84      1.00\n",
            " weight[0,243]     -0.88      0.87     -0.89     -2.37      0.45   1004.67      1.00\n",
            " weight[0,244]      1.46      0.84      1.45      0.08      2.83    831.81      1.00\n",
            " weight[0,245]     -0.03      0.91     -0.01     -1.61      1.34   1197.33      1.00\n",
            " weight[0,246]     -0.23      0.90     -0.22     -1.92      1.06    537.29      1.00\n",
            " weight[0,247]      0.32      0.87      0.33     -1.04      1.75    925.43      1.00\n",
            " weight[0,248]     -0.52      0.95     -0.51     -1.95      1.04   1010.30      1.00\n",
            " weight[0,249]     -0.15      0.82     -0.18     -1.42      1.25   1112.42      1.00\n",
            " weight[0,250]     -0.32      0.90     -0.31     -1.85      1.08   1203.24      1.00\n",
            " weight[0,251]     -0.13      0.88     -0.16     -1.49      1.40    770.38      1.00\n",
            " weight[0,252]      0.11      0.85      0.14     -1.26      1.51    911.77      1.00\n",
            " weight[0,253]     -0.19      0.81     -0.21     -1.48      1.13    816.30      1.00\n",
            " weight[0,254]     -0.39      0.91     -0.38     -1.96      1.01    867.73      1.00\n",
            " weight[0,255]      0.15      0.84      0.19     -1.34      1.46    852.77      1.00\n",
            " weight[0,256]      0.82      0.78      0.81     -0.38      2.20    904.22      1.00\n",
            " weight[0,257]      0.08      0.88      0.06     -1.51      1.38    739.71      1.00\n",
            " weight[0,258]      0.40      0.79      0.40     -0.95      1.58    826.44      1.00\n",
            " weight[0,259]     -0.20      0.85     -0.21     -1.47      1.25    962.99      1.00\n",
            " weight[0,260]     -0.46      0.81     -0.46     -1.74      0.87    962.04      1.00\n",
            " weight[0,261]     -0.78      0.85     -0.76     -2.10      0.72    545.28      1.00\n",
            " weight[0,262]     -0.10      0.87     -0.07     -1.56      1.25    484.24      1.00\n",
            " weight[0,263]      0.28      0.82      0.30     -1.16      1.54   1184.17      1.00\n",
            " weight[0,264]     -0.03      0.91     -0.05     -1.59      1.38    841.93      1.00\n",
            " weight[0,265]      0.24      0.85      0.22     -1.23      1.61   1120.86      1.00\n",
            " weight[0,266]      0.19      0.83      0.17     -1.30      1.51    800.17      1.00\n",
            " weight[0,267]      0.60      0.92      0.58     -0.83      2.14    774.20      1.00\n",
            " weight[0,268]     -0.08      0.85     -0.10     -1.53      1.26    506.53      1.00\n",
            " weight[0,269]     -0.22      0.85     -0.23     -1.65      1.09   1144.28      1.00\n",
            " weight[0,270]      0.39      0.80      0.40     -0.91      1.69    871.20      1.00\n",
            " weight[0,271]      0.60      0.86      0.60     -0.97      1.86    932.84      1.00\n",
            " weight[0,272]      0.37      0.86      0.35     -1.03      1.79   1113.41      1.00\n",
            " weight[0,273]     -0.75      0.81     -0.75     -1.95      0.69    767.97      1.00\n",
            " weight[0,274]      0.17      0.96      0.14     -1.20      1.92    562.62      1.00\n",
            " weight[0,275]     -0.41      0.82     -0.39     -1.74      0.87    995.15      1.01\n",
            " weight[0,276]      0.60      0.85      0.57     -0.77      2.00    620.65      1.00\n",
            " weight[0,277]      0.26      0.95      0.30     -1.19      1.93   1073.28      1.00\n",
            " weight[0,278]     -0.08      0.88     -0.11     -1.59      1.39   1065.11      1.00\n",
            " weight[0,279]     -0.11      0.89     -0.09     -1.55      1.28    748.10      1.00\n",
            " weight[0,280]      0.35      0.87      0.38     -1.10      1.74   1227.96      1.00\n",
            " weight[0,281]     -0.97      0.84     -0.96     -2.36      0.42    754.44      1.00\n",
            " weight[0,282]      0.77      0.79      0.79     -0.45      2.17    601.94      1.00\n",
            " weight[0,283]     -0.32      0.96     -0.28     -1.83      1.25   1043.21      1.00\n",
            " weight[0,284]     -0.67      0.81     -0.66     -2.01      0.64    711.62      1.00\n",
            " weight[0,285]      0.83      0.92      0.83     -0.80      2.21   1215.31      1.00\n",
            " weight[0,286]     -0.13      0.84     -0.12     -1.51      1.16    937.90      1.00\n",
            " weight[0,287]     -0.30      0.87     -0.31     -1.68      1.12    914.34      1.00\n",
            " weight[0,288]      0.11      0.84      0.07     -1.24      1.50    596.71      1.00\n",
            " weight[0,289]      0.10      0.98      0.08     -1.39      1.88   1472.20      1.00\n",
            " weight[0,290]      0.36      0.88      0.38     -1.17      1.63    346.13      1.00\n",
            " weight[0,291]     -0.29      0.94     -0.30     -1.74      1.34   1443.94      1.00\n",
            " weight[0,292]     -0.28      0.81     -0.28     -1.76      0.92   1028.75      1.00\n",
            " weight[0,293]     -0.35      0.96     -0.38     -1.83      1.26   1383.17      1.00\n",
            " weight[0,294]     -0.16      0.95     -0.18     -1.63      1.55   1642.26      1.00\n",
            " weight[0,295]      0.76      0.90      0.79     -0.71      2.16   1970.82      1.00\n",
            " weight[0,296]      0.89      0.85      0.87     -0.66      2.19   1005.21      1.00\n",
            " weight[0,297]      0.02      0.88      0.02     -1.37      1.43   1241.47      1.00\n",
            " weight[0,298]     -0.20      0.97     -0.20     -1.84      1.27   1564.68      1.00\n",
            " weight[0,299]     -0.03      0.89     -0.07     -1.48      1.41   1322.15      1.00\n",
            " weight[0,300]      0.00      0.96      0.01     -1.46      1.62   1119.84      1.00\n",
            " weight[0,301]     -0.72      0.82     -0.71     -2.06      0.70   1044.92      1.00\n",
            " weight[0,302]      0.12      0.93      0.11     -1.51      1.49   1036.97      1.00\n",
            " weight[0,303]      0.35      0.85      0.36     -1.10      1.66    925.65      1.00\n",
            " weight[0,304]     -0.73      0.92     -0.72     -2.37      0.70   1891.50      1.00\n",
            " weight[0,305]     -0.54      0.83     -0.55     -1.79      0.90   1530.00      1.00\n",
            " weight[0,306]     -0.09      0.87     -0.13     -1.53      1.26    578.65      1.00\n",
            " weight[0,307]      0.01      0.93      0.06     -1.41      1.63    690.37      1.01\n",
            " weight[0,308]      0.80      0.87      0.83     -0.69      2.18    954.79      1.00\n",
            " weight[0,309]     -0.20      0.89     -0.20     -1.60      1.25   1544.59      1.00\n",
            " weight[0,310]     -0.57      0.94     -0.59     -2.21      0.86    566.48      1.00\n",
            " weight[0,311]      1.03      0.83      1.04     -0.30      2.35   1176.75      1.00\n",
            " weight[0,312]     -0.93      0.88     -0.92     -2.53      0.38    998.93      1.00\n",
            " weight[0,313]     -0.98      0.86     -0.99     -2.42      0.36    783.11      1.00\n",
            " weight[0,314]      0.56      0.88      0.58     -0.97      2.02    602.29      1.00\n",
            " weight[0,315]      0.23      0.84      0.26     -1.26      1.47   1342.27      1.00\n",
            " weight[0,316]     -0.07      0.80     -0.09     -1.36      1.29    842.42      1.00\n",
            " weight[0,317]     -1.08      0.88     -1.09     -2.48      0.35    894.19      1.00\n",
            " weight[0,318]      0.38      0.84      0.39     -0.96      1.81    860.33      1.00\n",
            " weight[0,319]      0.60      0.88      0.61     -1.00      1.91   1610.39      1.00\n",
            " weight[0,320]     -0.10      0.87     -0.11     -1.59      1.27   1553.61      1.00\n",
            " weight[0,321]      0.46      0.88      0.43     -0.85      2.07    640.12      1.00\n",
            " weight[0,322]      0.00      0.90      0.01     -1.67      1.34   1041.41      1.00\n",
            " weight[0,323]      0.22      0.89      0.19     -1.15      1.76   1239.98      1.00\n",
            " weight[0,324]      0.08      0.90      0.12     -1.37      1.54    505.70      1.00\n",
            " weight[0,325]     -0.83      0.83     -0.84     -2.10      0.54   1438.61      1.00\n",
            " weight[0,326]      0.25      0.76      0.24     -1.02      1.50   1102.94      1.00\n",
            " weight[0,327]      0.34      0.83      0.32     -0.88      1.78    672.72      1.00\n",
            " weight[0,328]     -1.67      0.85     -1.63     -3.00     -0.28    989.73      1.00\n",
            " weight[0,329]     -1.02      0.73     -1.06     -2.15      0.24    752.51      1.00\n",
            " weight[0,330]     -0.73      0.84     -0.74     -2.16      0.63    973.17      1.00\n",
            " weight[0,331]     -0.33      0.83     -0.32     -1.69      1.06   1115.36      1.00\n",
            " weight[0,332]     -0.94      0.83     -0.95     -2.31      0.41   1574.22      1.00\n",
            " weight[0,333]      0.48      0.86      0.46     -0.71      2.09    862.59      1.00\n",
            " weight[0,334]     -0.32      0.91     -0.36     -1.82      1.14   1150.80      1.00\n",
            " weight[0,335]      0.67      0.88      0.65     -0.71      2.07    826.02      1.00\n",
            " weight[0,336]      0.38      0.85      0.35     -0.92      1.79   1333.58      1.00\n",
            " weight[0,337]      0.52      0.88      0.52     -0.93      1.90   1280.40      1.00\n",
            " weight[0,338]     -0.11      0.91     -0.14     -1.42      1.56    737.42      1.00\n",
            " weight[0,339]      0.10      0.93      0.11     -1.30      1.68   1414.49      1.00\n",
            " weight[0,340]      0.24      0.79      0.23     -1.08      1.58    713.82      1.00\n",
            " weight[0,341]     -0.42      0.86     -0.43     -1.79      0.99    914.17      1.00\n",
            " weight[0,342]     -0.28      0.97     -0.30     -1.91      1.31    585.86      1.00\n",
            " weight[0,343]     -0.25      0.91     -0.22     -1.78      1.18    855.16      1.00\n",
            " weight[0,344]      0.21      0.89      0.17     -1.13      1.80   1536.09      1.00\n",
            " weight[0,345]      0.53      0.88      0.53     -0.96      1.88    498.94      1.00\n",
            " weight[0,346]      0.60      0.95      0.59     -0.98      2.12   1299.59      1.00\n",
            " weight[0,347]      0.25      0.92      0.25     -1.24      1.76   1423.75      1.00\n",
            " weight[0,348]     -0.21      0.86     -0.21     -1.63      1.15    682.78      1.00\n",
            " weight[0,349]      0.44      0.83      0.47     -1.05      1.70    933.03      1.00\n",
            " weight[0,350]     -0.60      0.86     -0.62     -1.92      0.89    653.28      1.00\n",
            " weight[0,351]     -0.14      0.83     -0.14     -1.53      1.16    699.49      1.00\n",
            " weight[0,352]     -0.32      0.89     -0.34     -1.76      1.17   1183.59      1.00\n",
            " weight[0,353]      0.65      0.80      0.65     -0.61      1.99    899.10      1.00\n",
            " weight[0,354]      0.60      0.90      0.62     -0.84      2.05    836.55      1.00\n",
            " weight[0,355]     -0.22      0.84     -0.20     -1.47      1.25    854.90      1.00\n",
            " weight[0,356]     -0.36      0.88     -0.38     -1.75      1.17   1281.98      1.00\n",
            " weight[0,357]      0.55      0.94      0.53     -1.07      1.98   1603.81      1.00\n",
            " weight[0,358]      0.52      0.84      0.55     -0.89      1.84    826.31      1.00\n",
            " weight[0,359]      0.17      0.83      0.11     -1.03      1.66    729.70      1.00\n",
            " weight[0,360]      0.10      0.85      0.11     -1.41      1.30   1157.20      1.00\n",
            " weight[0,361]      0.76      0.74      0.77     -0.43      2.00    774.51      1.00\n",
            " weight[0,362]     -0.26      0.89     -0.24     -1.77      1.10   1057.47      1.00\n",
            " weight[0,363]      0.24      0.88      0.26     -1.07      1.76   1143.08      1.00\n",
            " weight[0,364]      0.67      0.87      0.66     -0.70      2.09   1406.28      1.00\n",
            " weight[0,365]      0.16      0.92      0.12     -1.44      1.55   1060.02      1.00\n",
            " weight[0,366]     -0.67      0.84     -0.69     -2.09      0.69    496.01      1.00\n",
            " weight[0,367]     -0.22      0.88     -0.21     -1.60      1.21   1284.59      1.00\n",
            " weight[0,368]      1.10      0.84      1.08     -0.32      2.41   1133.08      1.00\n",
            " weight[0,369]     -0.21      0.84     -0.23     -1.62      1.13    980.21      1.00\n",
            " weight[0,370]      1.21      0.96      1.19     -0.22      2.89   1011.72      1.00\n",
            " weight[0,371]     -0.75      0.80     -0.74     -2.05      0.52    889.52      1.00\n",
            " weight[0,372]      0.34      0.86      0.34     -1.04      1.85    682.80      1.00\n",
            " weight[0,373]      0.66      0.81      0.64     -0.53      2.12    682.86      1.00\n",
            " weight[0,374]      0.67      0.87      0.69     -0.63      2.07   1351.18      1.00\n",
            " weight[0,375]      0.37      0.92      0.41     -1.19      1.75    721.34      1.00\n",
            " weight[0,376]     -0.35      0.87     -0.35     -1.70      1.04    969.95      1.00\n",
            " weight[0,377]      0.12      0.95      0.14     -1.44      1.70   1656.30      1.00\n",
            " weight[0,378]     -0.00      0.81     -0.02     -1.18      1.50   1249.90      1.00\n",
            " weight[0,379]     -0.41      0.89     -0.40     -1.87      1.00    876.30      1.00\n",
            " weight[0,380]     -0.26      0.84     -0.22     -1.49      1.21    781.86      1.00\n",
            " weight[0,381]      0.77      0.91      0.76     -0.81      2.07   1510.10      1.00\n",
            " weight[0,382]      0.81      0.88      0.79     -0.57      2.24   1113.37      1.00\n",
            " weight[0,383]      1.48      0.87      1.48     -0.04      2.83    786.30      1.00\n",
            " weight[0,384]     -0.21      0.91     -0.20     -1.75      1.26   1115.08      1.00\n",
            " weight[0,385]     -0.20      0.95     -0.21     -1.63      1.50   1924.51      1.00\n",
            " weight[0,386]      0.31      0.91      0.31     -1.14      1.74    762.48      1.00\n",
            " weight[0,387]     -0.31      0.94     -0.28     -1.83      1.16    982.43      1.00\n",
            " weight[0,388]      0.37      0.88      0.38     -0.90      2.04   1360.96      1.00\n",
            " weight[0,389]     -0.41      0.92     -0.41     -2.00      1.00   1086.47      1.00\n",
            " weight[0,390]      0.24      0.78      0.24     -1.03      1.44    904.77      1.00\n",
            " weight[0,391]      0.39      0.86      0.39     -0.94      1.84    863.46      1.00\n",
            " weight[0,392]     -1.06      0.77     -1.05     -2.29      0.20    455.18      1.01\n",
            " weight[0,393]      0.04      0.82      0.06     -1.28      1.25   1165.46      1.00\n",
            " weight[0,394]      0.13      0.92      0.17     -1.37      1.63    842.05      1.00\n",
            " weight[0,395]      1.01      0.88      1.00     -0.38      2.47   1151.22      1.00\n",
            " weight[0,396]      0.37      0.89      0.44     -1.15      1.83   1253.34      1.00\n",
            " weight[0,397]      0.22      0.84      0.23     -1.14      1.53   1350.56      1.00\n",
            " weight[0,398]      0.23      0.87      0.27     -1.24      1.60   1134.02      1.00\n",
            " weight[0,399]      0.10      0.90      0.11     -1.24      1.65    671.83      1.00\n",
            " weight[0,400]     -0.39      0.84     -0.40     -1.65      1.06   1402.00      1.00\n",
            " weight[0,401]     -0.25      0.90     -0.26     -1.83      1.13   1160.50      1.00\n",
            " weight[0,402]      0.41      0.94      0.39     -1.11      1.95   1143.21      1.00\n",
            " weight[0,403]     -0.87      0.87     -0.89     -2.19      0.61   1311.77      1.00\n",
            " weight[0,404]     -0.50      0.75     -0.50     -1.62      0.81   1266.01      1.00\n",
            " weight[0,405]     -1.13      0.89     -1.10     -2.47      0.42   1036.54      1.00\n",
            " weight[0,406]     -0.51      0.91     -0.47     -1.94      0.99    748.43      1.00\n",
            " weight[0,407]      0.43      0.87      0.42     -0.81      2.01    765.35      1.00\n",
            " weight[0,408]     -0.38      0.85     -0.37     -1.96      0.91   1335.38      1.00\n",
            " weight[0,409]      0.11      0.93      0.12     -1.30      1.72    631.04      1.00\n",
            " weight[0,410]     -0.70      0.87     -0.70     -1.96      0.78    736.94      1.00\n",
            " weight[0,411]      0.15      0.86      0.15     -1.35      1.47    859.51      1.00\n",
            " weight[0,412]     -0.65      0.83     -0.66     -1.95      0.75    970.62      1.00\n",
            " weight[0,413]     -0.03      0.84     -0.04     -1.46      1.30    926.53      1.00\n",
            " weight[0,414]      0.96      0.87      0.98     -0.49      2.33   1088.06      1.00\n",
            " weight[0,415]      0.76      0.94      0.77     -0.75      2.30   1109.92      1.00\n",
            " weight[0,416]     -0.26      0.91     -0.25     -1.79      1.20   1475.79      1.00\n",
            " weight[0,417]     -0.76      0.81     -0.74     -2.21      0.47    816.69      1.00\n",
            " weight[0,418]     -0.77      0.91     -0.73     -2.14      0.78   1876.88      1.00\n",
            " weight[0,419]      0.28      0.85      0.29     -1.07      1.65    837.20      1.00\n",
            " weight[0,420]     -1.10      0.85     -1.10     -2.49      0.32    898.59      1.00\n",
            " weight[0,421]     -0.29      0.82     -0.28     -1.55      1.10    874.61      1.00\n",
            " weight[0,422]     -0.92      0.87     -0.95     -2.37      0.45    652.42      1.00\n",
            " weight[0,423]      0.25      0.77      0.27     -1.02      1.46    629.37      1.00\n",
            " weight[0,424]      0.34      0.85      0.34     -0.97      1.76    666.63      1.00\n",
            " weight[0,425]      0.59      0.92      0.57     -0.95      2.11    986.49      1.00\n",
            " weight[0,426]     -0.12      0.81     -0.15     -1.40      1.13   1046.93      1.00\n",
            " weight[0,427]      0.93      0.88      0.96     -0.62      2.27    933.27      1.00\n",
            " weight[0,428]      0.01      0.96      0.03     -1.56      1.52   1385.63      1.00\n",
            " weight[0,429]      0.08      0.93      0.09     -1.38      1.64   1179.62      1.00\n",
            " weight[0,430]     -0.06      0.87     -0.06     -1.48      1.29   1273.60      1.00\n",
            " weight[0,431]      0.40      0.84      0.40     -0.96      1.78   1029.48      1.00\n",
            " weight[0,432]     -0.03      0.91     -0.02     -1.44      1.43   1203.05      1.00\n",
            " weight[0,433]     -0.48      0.81     -0.46     -1.74      0.84    990.47      1.00\n",
            " weight[0,434]     -0.11      0.83     -0.13     -1.43      1.23    686.48      1.00\n",
            " weight[0,435]      0.03      0.78      0.01     -1.10      1.40    688.22      1.00\n",
            " weight[0,436]      0.50      0.84      0.47     -1.06      1.71    833.97      1.00\n",
            " weight[0,437]     -0.41      0.92     -0.39     -1.80      1.13    987.62      1.00\n",
            " weight[0,438]     -0.24      0.93     -0.24     -1.78      1.26    883.25      1.00\n",
            " weight[0,439]      0.21      0.84      0.20     -1.15      1.52    815.45      1.00\n",
            " weight[0,440]     -0.68      0.83     -0.68     -2.03      0.61   1429.78      1.00\n",
            " weight[0,441]      0.02      0.96     -0.01     -1.58      1.57   1269.10      1.00\n",
            " weight[0,442]      0.09      0.90      0.06     -1.30      1.62   1286.77      1.00\n",
            " weight[0,443]     -0.26      0.83     -0.27     -1.42      1.31    808.02      1.00\n",
            " weight[0,444]     -0.15      0.91     -0.12     -1.70      1.34    973.83      1.00\n",
            " weight[0,445]     -0.05      0.93     -0.05     -1.50      1.46    513.37      1.00\n",
            " weight[0,446]      0.26      0.92      0.25     -1.11      1.91   1197.11      1.00\n",
            " weight[0,447]     -0.29      0.85     -0.32     -1.56      1.12    680.55      1.00\n",
            " weight[0,448]     -0.90      0.86     -0.87     -2.20      0.51   1103.15      1.00\n",
            " weight[0,449]     -0.26      0.80     -0.26     -1.43      1.16    714.33      1.00\n",
            " weight[0,450]     -0.46      0.84     -0.46     -1.85      0.77    903.74      1.00\n",
            " weight[0,451]     -0.32      0.89     -0.36     -1.73      1.11    747.81      1.00\n",
            " weight[0,452]      0.21      0.93      0.21     -1.40      1.58    721.15      1.00\n",
            " weight[0,453]     -0.14      0.88     -0.18     -1.59      1.22   1414.88      1.00\n",
            " weight[0,454]      0.53      0.98      0.50     -1.07      2.13   1183.63      1.00\n",
            " weight[0,455]      0.52      0.84      0.54     -0.97      1.80   1126.05      1.00\n",
            " weight[0,456]     -0.21      0.83     -0.21     -1.40      1.36    646.96      1.00\n",
            " weight[0,457]     -1.27      0.86     -1.26     -2.75      0.03   1218.82      1.00\n",
            " weight[0,458]      0.67      0.88      0.70     -0.76      2.09    717.67      1.00\n",
            " weight[0,459]     -0.35      0.86     -0.36     -1.69      1.16   1385.59      1.00\n",
            " weight[0,460]     -0.19      0.87     -0.16     -1.68      1.10    829.04      1.00\n",
            " weight[0,461]     -0.79      0.82     -0.77     -2.13      0.55    628.16      1.00\n",
            " weight[0,462]     -0.23      0.94     -0.26     -1.66      1.39   1238.26      1.00\n",
            " weight[0,463]     -0.40      0.95     -0.43     -1.91      1.21   1001.18      1.00\n",
            " weight[0,464]     -1.08      0.86     -1.06     -2.38      0.41    899.86      1.00\n",
            " weight[0,465]      0.26      0.89      0.23     -1.24      1.64   1178.21      1.00\n",
            " weight[0,466]      0.58      0.86      0.57     -0.71      2.16    566.71      1.00\n",
            " weight[0,467]     -0.07      0.83     -0.07     -1.27      1.42    907.15      1.00\n",
            " weight[0,468]     -0.04      0.88     -0.03     -1.37      1.48   1127.41      1.00\n",
            " weight[0,469]      0.13      0.89      0.13     -1.20      1.75    900.39      1.00\n",
            " weight[0,470]     -0.57      0.89     -0.53     -1.94      0.92    792.40      1.00\n",
            " weight[0,471]     -1.16      0.89     -1.16     -2.64      0.28    602.63      1.00\n",
            " weight[0,472]     -0.12      0.86     -0.11     -1.46      1.37   1119.48      1.00\n",
            " weight[0,473]      0.80      0.88      0.76     -0.69      2.25   1012.48      1.00\n",
            " weight[0,474]      1.09      0.88      1.12     -0.44      2.39    602.84      1.00\n",
            " weight[0,475]      1.57      0.80      1.57      0.24      2.82    801.99      1.00\n",
            " weight[0,476]     -0.32      0.84     -0.28     -1.63      1.13    871.55      1.00\n",
            " weight[0,477]     -0.84      0.97     -0.82     -2.29      0.92   1803.85      1.00\n",
            " weight[0,478]      0.27      0.91      0.26     -1.23      1.77    723.65      1.00\n",
            " weight[0,479]      0.52      0.85      0.53     -0.93      1.79    667.37      1.00\n",
            " weight[0,480]      0.08      0.86      0.09     -1.33      1.51    547.38      1.00\n",
            " weight[0,481]     -0.01      0.91     -0.04     -1.52      1.47    809.71      1.00\n",
            " weight[0,482]      0.35      0.90      0.35     -1.06      1.94   1265.31      1.00\n",
            " weight[0,483]     -0.00      0.87      0.03     -1.46      1.33    973.64      1.00\n",
            " weight[0,484]     -0.27      0.93     -0.25     -1.72      1.32   1358.92      1.00\n",
            " weight[0,485]      0.11      0.86      0.09     -1.37      1.40   1051.44      1.00\n",
            " weight[0,486]     -0.00      0.83     -0.01     -1.29      1.37   1205.25      1.00\n",
            " weight[0,487]      0.13      1.00      0.14     -1.46      1.77   1236.97      1.00\n",
            " weight[0,488]      0.06      0.89      0.06     -1.30      1.57    594.03      1.00\n",
            " weight[0,489]     -0.02      0.91     -0.02     -1.46      1.50    909.02      1.00\n",
            " weight[0,490]      1.39      0.76      1.39     -0.01      2.52    923.57      1.00\n",
            " weight[0,491]      0.76      0.80      0.77     -0.73      1.89   1349.95      1.00\n",
            " weight[0,492]     -0.35      0.89     -0.36     -1.84      1.10    594.54      1.00\n",
            " weight[0,493]     -0.27      0.93     -0.33     -1.93      1.13    606.96      1.00\n",
            " weight[0,494]      0.57      0.85      0.58     -0.89      1.85   1093.24      1.00\n",
            " weight[0,495]     -0.61      0.88     -0.59     -2.15      0.76    978.57      1.00\n",
            " weight[0,496]     -1.01      0.84     -0.99     -2.38      0.36    827.82      1.00\n",
            " weight[0,497]     -1.06      0.90     -1.10     -2.50      0.38   1012.99      1.00\n",
            " weight[0,498]     -1.03      0.84     -1.04     -2.49      0.28   1001.98      1.00\n",
            " weight[0,499]     -0.26      0.98     -0.27     -1.78      1.33   1680.70      1.00\n",
            " weight[0,500]      0.45      0.87      0.45     -0.92      1.84    814.24      1.00\n",
            " weight[0,501]     -0.37      0.86     -0.38     -1.90      0.93   1069.72      1.00\n",
            " weight[0,502]      0.19      0.94      0.20     -1.36      1.65   1540.53      1.00\n",
            " weight[0,503]     -0.15      0.90     -0.21     -1.60      1.34    784.36      1.00\n",
            " weight[0,504]      0.62      0.82      0.60     -0.69      1.95   1105.31      1.00\n",
            " weight[0,505]     -0.05      0.85     -0.07     -1.29      1.40    846.59      1.01\n",
            " weight[0,506]      0.65      0.90      0.62     -0.82      2.03    389.05      1.02\n",
            " weight[0,507]     -0.01      0.85      0.03     -1.31      1.41   1155.67      1.00\n",
            " weight[0,508]     -1.06      0.87     -1.07     -2.59      0.23   1475.90      1.00\n",
            " weight[0,509]     -0.13      0.88     -0.15     -1.38      1.42    918.76      1.00\n",
            " weight[0,510]      0.08      0.92      0.08     -1.27      1.81   1128.97      1.00\n",
            " weight[0,511]      0.27      0.94      0.29     -1.30      1.71   1447.88      1.00\n",
            " weight[0,512]      0.19      0.78      0.21     -1.15      1.44    750.02      1.01\n",
            " weight[0,513]     -0.24      0.87     -0.28     -1.71      1.15   1286.97      1.00\n",
            " weight[0,514]      0.13      0.83      0.16     -1.12      1.54   1521.90      1.00\n",
            " weight[0,515]      0.62      0.80      0.61     -0.66      1.93   1377.28      1.00\n",
            " weight[0,516]     -0.41      0.84     -0.42     -1.79      0.93    850.05      1.00\n",
            " weight[0,517]     -0.19      0.94     -0.16     -1.58      1.52   1143.28      1.00\n",
            " weight[0,518]     -0.74      0.80     -0.73     -1.97      0.61   1078.95      1.00\n",
            " weight[0,519]      0.52      0.86      0.48     -0.99      1.87    477.54      1.00\n",
            " weight[0,520]      0.15      0.86      0.16     -1.42      1.40   1212.74      1.00\n",
            " weight[0,521]     -0.55      0.80     -0.53     -1.78      0.86   1007.26      1.00\n",
            " weight[0,522]      0.07      0.92      0.11     -1.48      1.55    940.44      1.00\n",
            " weight[0,523]     -0.58      0.88     -0.59     -1.91      0.91   1110.45      1.00\n",
            " weight[0,524]      0.05      0.86      0.02     -1.49      1.32   1250.81      1.00\n",
            " weight[0,525]     -0.25      0.97     -0.22     -1.78      1.40   1055.44      1.00\n",
            " weight[0,526]     -0.15      0.87     -0.15     -1.54      1.29   1096.63      1.00\n",
            " weight[0,527]      0.38      0.85      0.39     -0.97      1.71    694.51      1.00\n",
            " weight[0,528]      0.07      0.95      0.09     -1.43      1.67   1181.19      1.00\n",
            " weight[0,529]      0.01      0.83     -0.03     -1.19      1.45    434.25      1.01\n",
            " weight[0,530]      0.22      0.94      0.18     -1.31      1.70   1393.39      1.00\n",
            " weight[0,531]      0.34      0.94      0.32     -1.31      1.72   1273.35      1.00\n",
            " weight[0,532]      0.10      0.95      0.10     -1.52      1.57   1627.67      1.00\n",
            " weight[0,533]      0.09      0.85      0.06     -1.28      1.56    775.52      1.00\n",
            " weight[0,534]      0.58      0.90      0.55     -0.90      2.01   1149.53      1.00\n",
            " weight[0,535]     -0.01      0.83     -0.01     -1.46      1.27    612.42      1.00\n",
            " weight[0,536]      0.66      0.94      0.63     -0.84      2.22   1515.62      1.00\n",
            " weight[0,537]     -0.73      0.84     -0.75     -2.07      0.69   1005.49      1.00\n",
            " weight[0,538]     -0.01      0.78      0.02     -1.16      1.35   1257.85      1.00\n",
            " weight[0,539]     -0.44      0.84     -0.46     -1.90      0.82   1398.82      1.00\n",
            " weight[0,540]      0.34      0.94      0.33     -1.19      1.81    571.32      1.00\n",
            " weight[0,541]      0.51      0.98      0.50     -1.07      2.11   1408.77      1.00\n",
            " weight[0,542]     -0.02      0.88     -0.02     -1.39      1.47   1305.23      1.00\n",
            " weight[0,543]      0.24      0.87      0.24     -1.08      1.64   1276.92      1.00\n",
            " weight[0,544]     -1.45      0.83     -1.46     -2.80     -0.09    913.00      1.00\n",
            " weight[0,545]      0.59      0.84      0.59     -0.73      2.01   1177.39      1.00\n",
            " weight[0,546]     -0.81      0.88     -0.75     -2.31      0.47   1461.89      1.00\n",
            " weight[0,547]      0.17      0.85      0.19     -1.21      1.57   1212.01      1.00\n",
            " weight[0,548]     -0.17      1.00     -0.18     -1.81      1.40   1073.84      1.00\n",
            " weight[0,549]      0.44      0.86      0.44     -0.85      1.91    692.23      1.00\n",
            " weight[0,550]      0.00      0.82     -0.04     -1.21      1.53    437.10      1.00\n",
            " weight[0,551]     -0.31      0.85     -0.32     -1.58      1.18   1141.94      1.00\n",
            " weight[0,552]      0.31      0.98      0.33     -1.41      1.75    943.79      1.00\n",
            " weight[0,553]     -0.62      0.86     -0.64     -2.06      0.69   1350.18      1.00\n",
            " weight[0,554]     -0.07      0.82     -0.09     -1.35      1.36   1238.36      1.00\n",
            " weight[0,555]     -0.08      0.84     -0.08     -1.42      1.30   1075.46      1.00\n",
            " weight[0,556]     -0.01      0.89     -0.04     -1.44      1.51    982.15      1.00\n",
            " weight[0,557]     -0.28      0.91     -0.33     -1.81      1.17   1001.32      1.00\n",
            " weight[0,558]     -0.38      0.87     -0.37     -1.78      1.06    956.85      1.00\n",
            " weight[0,559]     -0.16      0.92     -0.15     -1.67      1.40    853.02      1.00\n",
            " weight[0,560]     -0.35      0.91     -0.33     -1.84      1.05   2358.06      1.00\n",
            " weight[0,561]     -0.62      0.89     -0.62     -2.04      0.90   1198.01      1.00\n",
            " weight[0,562]     -0.12      0.93     -0.13     -1.61      1.42   1851.55      1.00\n",
            " weight[0,563]     -0.53      0.92     -0.50     -1.95      1.01   1829.06      1.00\n",
            " weight[0,564]      0.50      0.87      0.48     -0.88      2.00    671.10      1.00\n",
            " weight[0,565]     -0.37      0.92     -0.38     -1.85      1.13   2548.02      1.00\n",
            " weight[0,566]      0.23      0.94      0.21     -1.17      1.83    541.51      1.00\n",
            " weight[0,567]     -0.21      0.88     -0.24     -1.68      1.14   1215.46      1.00\n",
            " weight[0,568]      0.50      0.80      0.50     -0.71      1.85   1602.61      1.00\n",
            " weight[0,569]      0.39      0.99      0.40     -1.30      1.92   1847.78      1.00\n",
            " weight[0,570]      0.27      0.85      0.22     -1.22      1.56   1353.52      1.00\n",
            " weight[0,571]      0.26      0.88      0.27     -1.18      1.63    776.71      1.00\n",
            " weight[0,572]     -0.32      0.91     -0.31     -1.71      1.25   1711.95      1.00\n",
            " weight[0,573]      0.15      0.90      0.15     -1.21      1.66    666.56      1.00\n",
            " weight[0,574]     -0.57      0.90     -0.57     -2.10      0.85   1734.66      1.00\n",
            " weight[0,575]      0.16      0.92      0.16     -1.35      1.58   1138.32      1.00\n",
            " weight[0,576]     -0.44      0.92     -0.42     -2.02      1.00   2025.15      1.00\n",
            " weight[0,577]     -0.22      0.94     -0.20     -1.83      1.20   1251.62      1.00\n",
            " weight[0,578]     -0.05      0.96     -0.08     -1.55      1.62   1224.34      1.00\n",
            " weight[0,579]      0.45      0.94      0.48     -1.11      2.04    780.86      1.00\n",
            " weight[0,580]      0.20      0.85      0.21     -1.24      1.53    699.59      1.00\n",
            " weight[0,581]      0.20      1.01      0.26     -1.46      1.81   1512.59      1.00\n",
            " weight[0,582]     -0.26      0.91     -0.27     -1.79      1.18    939.18      1.00\n",
            " weight[0,583]      0.09      0.87      0.11     -1.24      1.54   1102.07      1.00\n",
            " weight[0,584]     -0.30      0.94     -0.30     -1.64      1.50   1020.86      1.00\n",
            " weight[0,585]      0.10      0.85      0.07     -1.27      1.49   1126.56      1.00\n",
            " weight[0,586]     -0.19      0.91     -0.17     -1.66      1.35    963.96      1.00\n",
            " weight[0,587]     -0.75      0.96     -0.76     -2.42      0.81    717.46      1.00\n",
            " weight[0,588]     -0.09      1.01     -0.07     -1.76      1.49   1651.72      1.00\n",
            " weight[0,589]      0.33      0.87      0.33     -1.13      1.67   1287.64      1.00\n",
            " weight[0,590]     -0.12      0.91     -0.09     -1.51      1.46    906.14      1.00\n",
            " weight[0,591]      0.08      0.96      0.12     -1.37      1.67   1106.88      1.00\n",
            " weight[0,592]      0.18      0.89      0.18     -1.35      1.55    454.41      1.00\n",
            " weight[0,593]     -0.21      0.90     -0.18     -1.52      1.37   1663.81      1.00\n",
            " weight[0,594]     -0.14      0.91     -0.16     -1.68      1.32   1072.55      1.00\n",
            " weight[0,595]      0.10      0.84      0.13     -1.35      1.43    974.00      1.00\n",
            " weight[0,596]     -0.22      0.98     -0.23     -2.04      1.18    480.91      1.00\n",
            " weight[0,597]      0.28      0.88      0.27     -1.35      1.56    993.09      1.00\n",
            " weight[0,598]     -0.04      0.94     -0.07     -1.57      1.46   1718.48      1.00\n",
            " weight[0,599]     -0.19      0.91     -0.19     -1.84      1.12    615.25      1.01\n",
            " weight[0,600]      0.61      0.95      0.59     -0.87      2.19   1492.53      1.01\n",
            " weight[0,601]     -0.36      0.91     -0.36     -1.72      1.28   1454.33      1.00\n",
            " weight[0,602]      0.40      0.86      0.38     -0.97      1.80    912.37      1.00\n",
            " weight[0,603]     -0.27      0.98     -0.27     -1.86      1.32    530.74      1.01\n",
            " weight[0,604]      0.08      0.94      0.08     -1.50      1.54   1293.25      1.00\n",
            " weight[0,605]      0.05      0.99      0.05     -1.64      1.59    821.02      1.00\n",
            " weight[0,606]      0.05      1.01      0.05     -1.60      1.62    894.10      1.00\n",
            " weight[0,607]      0.07      0.97      0.09     -1.56      1.63   1551.42      1.00\n",
            " weight[0,608]      0.01      0.98      0.04     -1.73      1.50   1079.60      1.00\n",
            " weight[0,609]     -0.05      0.98     -0.04     -1.71      1.52    756.96      1.00\n",
            " weight[0,610]     -0.06      0.95     -0.07     -1.47      1.68   1459.13      1.00\n",
            " weight[0,611]     -0.00      0.98     -0.02     -1.62      1.53   1459.92      1.00\n",
            " weight[0,612]     -0.07      1.00     -0.07     -1.64      1.64   2478.41      1.00\n",
            " weight[0,613]     -0.06      1.00     -0.09     -1.70      1.59   1278.63      1.00\n",
            " weight[0,614]     -0.05      1.00     -0.06     -1.71      1.60   1043.31      1.00\n",
            " weight[0,615]      0.08      1.03      0.03     -1.64      1.65    773.70      1.00\n",
            " weight[0,616]     -0.16      1.00     -0.16     -1.82      1.40   1018.31      1.00\n",
            " weight[0,617]     -0.22      1.00     -0.21     -1.85      1.36    832.99      1.00\n",
            " weight[0,618]     -0.06      0.99     -0.08     -1.65      1.59   1843.91      1.00\n",
            " weight[0,619]     -0.06      0.96     -0.07     -1.73      1.42   2087.34      1.00\n",
            " weight[0,620]      0.05      1.02      0.04     -1.50      1.82   1669.42      1.00\n",
            " weight[0,621]      0.06      0.96      0.07     -1.51      1.62   1393.15      1.00\n",
            " weight[0,622]     -0.02      1.01     -0.04     -1.76      1.52   1243.34      1.00\n",
            " weight[0,623]      0.08      1.00      0.08     -1.54      1.63    426.20      1.00\n",
            " weight[0,624]      0.07      1.04      0.11     -1.62      1.83   2211.33      1.00\n",
            " weight[0,625]      0.17      1.05      0.17     -1.51      1.80   1602.10      1.00\n",
            " weight[0,626]     -0.06      1.00     -0.04     -1.56      1.60    399.05      1.00\n",
            " weight[0,627]     -0.15      0.98     -0.15     -1.61      1.69    840.72      1.00\n",
            " weight[0,628]     -0.15      0.96     -0.13     -1.69      1.54    860.39      1.00\n",
            " weight[0,629]     -0.01      0.97      0.04     -1.50      1.60   1008.74      1.00\n",
            " weight[0,630]     -0.10      0.96     -0.13     -1.68      1.43    970.46      1.00\n",
            " weight[0,631]     -0.02      1.02     -0.01     -1.67      1.61   1439.96      1.00\n",
            " weight[0,632]     -0.29      0.98     -0.26     -1.99      1.23   1238.31      1.00\n",
            " weight[0,633]     -0.15      0.99     -0.16     -1.72      1.49    961.61      1.00\n",
            " weight[0,634]     -0.14      1.03     -0.14     -1.88      1.43   2507.52      1.00\n",
            " weight[0,635]     -0.20      1.03     -0.17     -1.83      1.48   1071.14      1.00\n",
            " weight[0,636]     -0.18      1.00     -0.21     -1.63      1.52   2823.25      1.00\n",
            " weight[0,637]     -0.17      0.96     -0.17     -1.65      1.40   2357.25      1.00\n",
            " weight[0,638]     -0.13      0.95     -0.14     -1.66      1.46   1317.84      1.00\n",
            " weight[0,639]     -0.11      1.00     -0.11     -1.75      1.50    883.44      1.00\n",
            " weight[0,640]     -0.22      1.01     -0.21     -1.87      1.44    779.46      1.00\n",
            " weight[0,641]     -0.34      0.97     -0.34     -1.94      1.26    719.11      1.00\n",
            " weight[0,642]     -0.35      0.99     -0.34     -1.99      1.21   1929.76      1.00\n",
            " weight[0,643]     -0.33      0.98     -0.32     -1.78      1.41   1890.19      1.00\n",
            " weight[0,644]     -0.34      1.00     -0.39     -2.11      1.17   1341.37      1.00\n",
            " weight[0,645]     -0.24      1.02     -0.26     -1.84      1.47   1749.10      1.00\n",
            " weight[0,646]     -0.38      0.98     -0.38     -1.91      1.20   1492.77      1.00\n",
            " weight[0,647]     -0.45      1.00     -0.45     -1.99      1.15   1006.24      1.00\n",
            " weight[0,648]     -0.25      1.05     -0.21     -2.06      1.34   1404.87      1.00\n",
            " weight[0,649]     -0.24      0.96     -0.24     -1.77      1.34   1435.06      1.00\n",
            " weight[0,650]     -0.24      0.99     -0.22     -1.71      1.59   1617.65      1.00\n",
            " weight[0,651]     -0.15      1.02     -0.14     -1.80      1.52   1263.34      1.00\n",
            " weight[0,652]      0.05      1.02      0.07     -1.67      1.59   1468.65      1.00\n",
            " weight[0,653]     -0.16      0.97     -0.19     -1.69      1.43    747.35      1.00\n",
            " weight[0,654]     -0.19      0.98     -0.23     -1.73      1.46   1371.66      1.00\n",
            " weight[0,655]     -0.05      0.95     -0.02     -1.67      1.44   1268.04      1.00\n",
            " weight[0,656]     -0.09      1.00     -0.08     -1.65      1.55    533.15      1.00\n",
            " weight[0,657]     -0.06      1.00     -0.08     -1.71      1.50   1603.56      1.00\n",
            " weight[0,658]     -0.04      1.00     -0.05     -1.74      1.53   1931.14      1.00\n",
            " weight[0,659]     -0.05      1.00     -0.09     -1.65      1.59   1466.17      1.00\n",
            " weight[0,660]     -0.01      0.97     -0.02     -1.58      1.59   1176.14      1.00\n",
            " weight[0,661]      0.07      1.05      0.06     -1.72      1.81   1052.17      1.00\n",
            " weight[0,662]     -0.04      0.98     -0.01     -1.61      1.53    907.13      1.00\n",
            " weight[0,663]      0.20      1.02      0.20     -1.43      1.91    717.62      1.01\n",
            " weight[0,664]      0.18      0.96      0.15     -1.09      2.04   1139.99      1.00\n",
            " weight[0,665]     -0.14      0.94     -0.15     -1.63      1.39   1044.43      1.00\n",
            " weight[0,666]      0.05      1.01      0.03     -1.58      1.74   2513.36      1.00\n",
            " weight[0,667]      0.40      0.95      0.40     -1.20      1.86   1092.06      1.00\n",
            " weight[0,668]      0.43      0.91      0.40     -1.14      1.87    766.74      1.00\n",
            " weight[0,669]      0.18      0.96      0.18     -1.44      1.64    384.65      1.00\n",
            " weight[0,670]      0.18      0.95      0.21     -1.24      1.90   1336.64      1.00\n",
            " weight[0,671]      0.18      1.00      0.19     -1.58      1.76    659.09      1.00\n",
            " weight[0,672]     -0.19      0.98     -0.16     -1.74      1.42    563.27      1.00\n",
            " weight[0,673]     -0.54      1.01     -0.51     -2.16      1.18   1600.76      1.00\n",
            " weight[0,674]      0.14      1.00      0.11     -1.43      1.82   1524.95      1.00\n",
            " weight[0,675]      0.13      1.02      0.15     -1.55      1.75   1980.28      1.00\n",
            " weight[0,676]      0.07      0.96      0.09     -1.51      1.53   2159.45      1.00\n",
            " weight[0,677]      0.19      1.03      0.17     -1.47      1.93   1159.70      1.00\n",
            " weight[0,678]      0.23      0.97      0.25     -1.33      1.85   1423.79      1.00\n",
            " weight[0,679]     -0.10      1.00     -0.12     -1.70      1.50   1269.21      1.00\n",
            " weight[0,680]      0.06      0.95      0.08     -1.44      1.68   1321.29      1.00\n",
            " weight[0,681]      0.18      0.98      0.21     -1.63      1.62    723.86      1.00\n",
            " weight[0,682]      0.15      1.00      0.10     -1.41      1.79   1821.17      1.00\n",
            " weight[0,683]      0.14      1.01      0.17     -1.57      1.70   2070.52      1.00\n",
            " weight[0,684]      0.18      1.01      0.17     -1.63      1.65    642.86      1.00\n",
            " weight[0,685]      0.17      0.95      0.14     -1.25      1.80   1203.23      1.00\n",
            " weight[0,686]      0.24      0.97      0.23     -1.46      1.71    859.50      1.00\n",
            " weight[0,687]      0.26      0.99      0.25     -1.40      1.86   1338.91      1.01\n",
            " weight[0,688]      0.23      1.01      0.22     -1.37      1.92   1771.30      1.00\n",
            " weight[0,689]      0.17      0.95      0.18     -1.20      1.89   1253.84      1.00\n",
            " weight[0,690]      0.15      0.94      0.15     -1.29      1.73   1897.27      1.00\n",
            " weight[0,691]      0.21      1.02      0.21     -1.48      1.84   3297.91      1.00\n",
            " weight[0,692]      0.15      0.96      0.15     -1.39      1.78   1207.34      1.00\n",
            " weight[0,693]      0.17      1.00      0.18     -1.37      1.94    429.94      1.00\n",
            " weight[0,694]      0.22      0.94      0.26     -1.30      1.73   1664.98      1.00\n",
            " weight[0,695]      0.12      1.00      0.06     -1.37      1.88   1423.45      1.00\n",
            " weight[0,696]      0.06      1.01      0.02     -1.52      1.77    821.17      1.00\n",
            " weight[0,697]      0.24      0.93      0.27     -1.13      1.93   1359.16      1.00\n",
            " weight[0,698]      0.26      1.00      0.25     -1.48      1.81    965.77      1.00\n",
            " weight[0,699]      0.26      0.99      0.30     -1.36      1.94   1328.25      1.00\n",
            " weight[0,700]      0.21      1.01      0.21     -1.48      1.83   1522.82      1.00\n",
            " weight[0,701]      0.19      0.96      0.18     -1.44      1.69   1420.18      1.00\n",
            " weight[0,702]      0.21      1.01      0.23     -1.59      1.71    954.59      1.00\n",
            " weight[0,703]      0.06      0.97      0.04     -1.58      1.59   1759.83      1.00\n",
            " weight[0,704]      0.07      1.00      0.04     -1.57      1.71    977.76      1.00\n",
            " weight[0,705]      0.04      0.94      0.01     -1.45      1.59   1407.20      1.00\n",
            " weight[0,706]      0.14      0.99      0.14     -1.50      1.68   1036.30      1.00\n",
            " weight[0,707]      0.09      1.01      0.10     -1.58      1.63    655.74      1.00\n",
            " weight[0,708]      0.03      1.01      0.00     -1.64      1.67   1313.43      1.00\n",
            " weight[0,709]     -0.14      0.96     -0.15     -1.72      1.45   1848.77      1.00\n",
            " weight[0,710]      0.09      0.97      0.12     -1.57      1.55   1913.66      1.00\n",
            " weight[0,711]      0.09      0.94      0.09     -1.46      1.59   1371.14      1.00\n",
            " weight[0,712]      0.09      1.00      0.06     -1.71      1.67   1773.52      1.00\n",
            " weight[0,713]      0.05      1.03      0.03     -1.63      1.71   1635.47      1.00\n",
            " weight[0,714]      0.23      0.97      0.26     -1.45      1.82   1265.58      1.00\n",
            " weight[0,715]     -0.16      0.94     -0.17     -1.71      1.31   1175.23      1.00\n",
            " weight[0,716]     -0.18      1.02     -0.19     -1.81      1.53   2053.83      1.00\n",
            " weight[0,717]     -0.26      1.00     -0.27     -1.97      1.28   1044.53      1.00\n",
            " weight[0,718]     -0.15      0.96     -0.15     -1.60      1.54   2228.89      1.00\n",
            " weight[0,719]     -0.07      0.99     -0.12     -1.68      1.47   1013.33      1.00\n",
            " weight[0,720]     -0.03      1.02     -0.00     -1.67      1.65    921.80      1.00\n",
            " weight[0,721]      0.01      0.98      0.01     -1.45      1.69   1065.42      1.00\n",
            " weight[0,722]     -0.15      0.95     -0.18     -1.65      1.39   1962.04      1.00\n",
            " weight[0,723]     -0.21      0.99     -0.23     -1.72      1.37    610.23      1.00\n",
            " weight[0,724]     -0.11      0.94     -0.10     -1.63      1.42   1103.76      1.00\n",
            " weight[0,725]     -0.22      1.01     -0.26     -1.76      1.60   2143.11      1.00\n",
            " weight[0,726]     -0.10      0.94     -0.11     -1.62      1.42    529.76      1.00\n",
            " weight[0,727]     -0.11      0.99     -0.15     -1.66      1.56   1421.01      1.00\n",
            " weight[0,728]     -0.13      0.97     -0.13     -1.66      1.48   2092.36      1.00\n",
            " weight[0,729]     -0.03      1.03     -0.08     -1.80      1.63   1744.56      1.00\n",
            " weight[0,730]     -0.14      1.01     -0.12     -1.74      1.56    845.07      1.00\n",
            " weight[0,731]     -0.14      0.96     -0.14     -1.77      1.36   1398.08      1.00\n",
            " weight[0,732]     -0.22      0.97     -0.20     -1.78      1.38    886.68      1.00\n",
            " weight[0,733]     -0.26      1.00     -0.27     -1.79      1.49   1200.14      1.00\n",
            " weight[0,734]     -0.06      0.98     -0.04     -1.62      1.54   2985.07      1.00\n",
            " weight[0,735]     -0.18      0.96     -0.18     -1.77      1.38    507.57      1.00\n",
            " weight[0,736]     -0.14      0.96     -0.08     -1.70      1.42   1108.50      1.00\n",
            " weight[0,737]     -0.06      0.95     -0.08     -1.54      1.47    472.14      1.00\n",
            " weight[0,738]     -0.16      1.03     -0.15     -1.74      1.58    927.85      1.00\n",
            " weight[0,739]     -0.09      1.01     -0.08     -1.81      1.46   1167.45      1.00\n",
            " weight[0,740]     -0.01      0.99      0.03     -1.62      1.51   3277.48      1.00\n",
            " weight[0,741]      0.17      0.94      0.16     -1.31      1.71   1941.55      1.00\n",
            " weight[0,742]     -0.01      0.98     -0.01     -1.62      1.58   1040.38      1.00\n",
            " weight[0,743]      0.04      0.95      0.08     -1.48      1.63    833.87      1.00\n",
            " weight[0,744]      0.10      0.94      0.10     -1.28      1.74   1299.02      1.00\n",
            " weight[0,745]     -0.08      0.97     -0.08     -1.57      1.63   2500.18      1.00\n",
            " weight[0,746]     -0.13      0.95     -0.13     -1.76      1.32    942.68      1.00\n",
            " weight[0,747]     -0.09      0.98     -0.11     -1.72      1.45    719.84      1.00\n",
            " weight[0,748]     -0.06      0.96     -0.07     -1.65      1.55   1672.95      1.00\n",
            " weight[0,749]     -0.10      1.00     -0.10     -1.66      1.58   1202.97      1.00\n",
            " weight[0,750]     -0.04      0.96     -0.04     -1.73      1.36   2140.93      1.00\n",
            " weight[0,751]     -0.01      1.01     -0.02     -1.65      1.60    530.73      1.00\n",
            " weight[0,752]      0.00      1.03      0.02     -1.70      1.54   1163.03      1.00\n",
            " weight[0,753]     -0.10      1.01     -0.09     -1.70      1.50    882.47      1.00\n",
            " weight[0,754]     -0.02      0.97     -0.05     -1.64      1.50   1525.37      1.00\n",
            " weight[0,755]      0.03      0.97      0.01     -1.64      1.57   1091.52      1.00\n",
            " weight[0,756]      0.00      0.98     -0.01     -1.74      1.50   1161.41      1.00\n",
            " weight[0,757]     -0.17      0.99     -0.16     -1.84      1.32   1656.47      1.00\n",
            " weight[0,758]     -0.14      1.00     -0.12     -1.72      1.49   1356.27      1.00\n",
            " weight[0,759]     -0.10      0.98     -0.08     -1.60      1.53   1758.54      1.00\n",
            " weight[0,760]      0.10      0.96      0.10     -1.54      1.61    511.12      1.00\n",
            " weight[0,761]     -0.12      0.99     -0.14     -1.92      1.33   1538.14      1.00\n",
            " weight[0,762]     -0.01      0.99      0.01     -1.61      1.67   1573.00      1.00\n",
            " weight[0,763]     -0.02      0.95     -0.01     -1.46      1.55   2504.37      1.00\n",
            " weight[0,764]      0.01      1.01     -0.01     -1.68      1.56   1195.80      1.00\n",
            " weight[0,765]     -0.04      0.95     -0.01     -1.66      1.48   1583.52      1.00\n",
            " weight[0,766]     -0.02      0.97     -0.07     -1.54      1.62    986.63      1.00\n",
            " weight[0,767]     -0.04      1.00     -0.04     -1.47      1.77   1459.46      1.00\n",
            " weight[0,768]      0.02      0.96      0.01     -1.49      1.68   1392.37      1.00\n",
            " weight[0,769]      0.04      1.02      0.08     -1.78      1.60   1368.54      1.00\n",
            " weight[0,770]      0.10      0.96      0.12     -1.33      1.81   1164.36      1.00\n",
            " weight[0,771]      0.07      1.02      0.06     -1.55      1.81   2260.24      1.00\n",
            " weight[0,772]      0.04      0.97      0.01     -1.56      1.58    809.88      1.00\n",
            " weight[0,773]      0.15      0.98      0.19     -1.40      1.75   1097.65      1.00\n",
            " weight[0,774]      0.33      0.96      0.30     -1.14      1.92    967.51      1.00\n",
            " weight[0,775]      0.16      0.96      0.16     -1.50      1.66   1433.60      1.00\n",
            " weight[0,776]     -0.17      0.97     -0.16     -1.87      1.27   1268.56      1.00\n",
            " weight[0,777]     -0.28      0.95     -0.26     -1.85      1.25   1198.11      1.00\n",
            " weight[0,778]      0.00      0.98      0.03     -1.65      1.49   1673.08      1.00\n",
            " weight[0,779]     -0.23      0.92     -0.21     -1.79      1.21   1885.22      1.00\n",
            " weight[0,780]     -0.03      0.98     -0.04     -1.54      1.52   1425.39      1.00\n",
            " weight[0,781]     -0.01      1.00      0.03     -1.63      1.67   1324.01      1.00\n",
            " weight[0,782]      0.04      0.99      0.06     -1.64      1.58   1252.85      1.00\n",
            " weight[0,783]      0.04      1.00      0.05     -1.52      1.69    880.26      1.00\n",
            " weight[0,784]      0.04      1.04      0.02     -1.48      1.89    807.16      1.00\n",
            " weight[0,785]      0.01      0.99     -0.00     -1.50      1.70    686.89      1.00\n",
            " weight[0,786]     -0.04      0.96     -0.05     -1.51      1.63   1803.85      1.00\n",
            " weight[0,787]     -0.04      0.98     -0.08     -1.68      1.60    781.83      1.00\n",
            " weight[0,788]     -0.04      0.95     -0.02     -1.59      1.47    837.12      1.00\n",
            " weight[0,789]     -0.08      0.93     -0.06     -1.38      1.63    954.14      1.00\n",
            " weight[0,790]      0.19      0.96      0.20     -1.33      1.76   1206.58      1.00\n",
            " weight[0,791]      0.17      0.94      0.16     -1.36      1.81    678.69      1.00\n",
            " weight[0,792]      0.17      0.97      0.16     -1.58      1.62   1000.00      1.00\n",
            " weight[0,793]      0.15      0.95      0.17     -1.34      1.78   1445.45      1.00\n",
            " weight[0,794]     -0.02      1.00      0.01     -1.58      1.67   1164.36      1.00\n",
            " weight[0,795]      0.19      1.06      0.20     -1.62      1.81   2863.64      1.00\n",
            " weight[0,796]      0.09      1.04      0.11     -1.59      1.76   1323.35      1.00\n",
            " weight[0,797]      0.10      0.99      0.10     -1.53      1.74   1931.12      1.00\n",
            " weight[0,798]      0.21      0.97      0.21     -1.36      1.75    601.86      1.00\n",
            " weight[0,799]      0.19      0.98      0.17     -1.40      1.65   1142.80      1.00\n",
            " weight[0,800]      0.18      0.98      0.15     -1.43      1.88    775.92      1.00\n",
            " weight[0,801]      0.18      1.00      0.20     -1.54      1.74   1243.80      1.00\n",
            " weight[0,802]      0.32      0.99      0.35     -1.34      1.94   1013.90      1.00\n",
            " weight[0,803]      0.35      0.93      0.37     -1.12      1.83   1460.34      1.00\n",
            " weight[0,804]      0.52      1.00      0.54     -1.27      1.91   1431.05      1.00\n",
            " weight[0,805]      0.45      0.98      0.43     -1.11      2.14   1488.84      1.00\n",
            " weight[0,806]      0.37      0.99      0.37     -1.30      1.96   1333.56      1.00\n",
            " weight[0,807]      0.30      0.98      0.30     -1.32      1.93   1288.93      1.00\n",
            " weight[0,808]      0.26      0.98      0.23     -1.26      1.96    682.02      1.00\n",
            " weight[0,809]      0.30      0.94      0.28     -1.21      1.89   1067.70      1.00\n",
            " weight[0,810]      0.17      1.01      0.17     -1.32      2.01   1148.75      1.00\n",
            " weight[0,811]      0.16      0.96      0.13     -1.39      1.72    942.49      1.00\n",
            " weight[0,812]      0.13      0.99      0.16     -1.47      1.72    608.47      1.00\n",
            " weight[0,813]      0.04      1.00      0.04     -1.57      1.74   1196.70      1.00\n",
            " weight[0,814]      0.23      0.96      0.22     -1.38      1.72   1182.13      1.00\n",
            " weight[0,815]      0.20      0.98      0.17     -1.23      1.87   1418.62      1.00\n",
            " weight[0,816]      0.26      1.05      0.27     -1.36      2.10   1141.39      1.00\n",
            " weight[0,817]      0.19      1.01      0.22     -1.55      1.79   1636.59      1.00\n",
            " weight[0,818]      0.07      0.96      0.06     -1.64      1.57   2008.42      1.00\n",
            " weight[0,819]      0.06      0.92      0.07     -1.42      1.52   1252.14      1.00\n",
            " weight[0,820]      0.11      0.95      0.13     -1.42      1.71   1089.36      1.00\n",
            " weight[0,821]      0.24      0.98      0.21     -1.28      2.01   2532.74      1.00\n",
            " weight[0,822]      0.37      1.02      0.35     -1.32      2.01    644.10      1.00\n",
            " weight[0,823]      0.46      0.98      0.45     -1.12      2.03    782.91      1.00\n",
            " weight[0,824]      0.32      0.96      0.36     -1.13      1.90   1863.80      1.00\n",
            " weight[0,825]      0.28      1.03      0.30     -1.53      1.85   1294.81      1.01\n",
            " weight[0,826]     -0.32      0.97     -0.32     -2.02      1.19    861.72      1.00\n",
            " weight[0,827]     -0.08      0.95     -0.07     -1.55      1.58   2050.13      1.00\n",
            " weight[0,828]     -0.26      0.99     -0.28     -1.87      1.39   1240.30      1.00\n",
            " weight[0,829]     -0.15      0.97     -0.15     -1.77      1.44   1460.99      1.00\n",
            " weight[0,830]     -0.06      1.01     -0.09     -1.75      1.60    972.90      1.00\n",
            " weight[0,831]      0.03      0.99      0.04     -1.58      1.70   1399.84      1.00\n",
            " weight[0,832]     -0.07      0.97     -0.10     -1.82      1.34   1696.15      1.00\n",
            " weight[0,833]     -0.07      0.97     -0.06     -1.80      1.35    841.05      1.00\n",
            " weight[0,834]     -0.13      0.91     -0.13     -1.72      1.29    812.21      1.00\n",
            " weight[0,835]     -0.11      0.93     -0.11     -1.64      1.30    925.44      1.00\n",
            " weight[0,836]     -0.22      1.03     -0.22     -1.81      1.53   1078.09      1.00\n",
            " weight[0,837]     -0.12      0.99     -0.14     -1.82      1.39    948.60      1.00\n",
            " weight[0,838]     -0.22      0.99     -0.22     -1.58      1.66   2271.21      1.00\n",
            " weight[0,839]     -0.12      0.89     -0.09     -1.67      1.21   1030.75      1.00\n",
            " weight[0,840]     -0.22      0.93     -0.23     -1.95      1.08    963.50      1.00\n",
            " weight[0,841]     -0.32      0.97     -0.35     -1.80      1.30    970.59      1.00\n",
            " weight[0,842]     -0.08      0.97     -0.05     -1.98      1.30   1708.75      1.00\n",
            " weight[0,843]     -0.11      0.97     -0.15     -1.67      1.51    758.93      1.00\n",
            " weight[0,844]     -0.13      1.00     -0.15     -1.70      1.56   1490.95      1.00\n",
            " weight[0,845]     -0.25      1.02     -0.22     -1.86      1.44   1909.84      1.00\n",
            " weight[0,846]     -0.29      0.99     -0.30     -1.89      1.26   2843.58      1.00\n",
            " weight[0,847]     -0.19      0.96     -0.18     -1.84      1.31   1007.43      1.02\n",
            " weight[0,848]     -0.09      0.94     -0.11     -1.75      1.32    814.69      1.00\n",
            " weight[0,849]     -0.20      1.01     -0.21     -1.87      1.41   1096.98      1.00\n",
            " weight[0,850]     -0.18      0.98     -0.21     -1.67      1.47    992.70      1.00\n",
            " weight[0,851]     -0.04      0.94      0.01     -1.62      1.34   1439.32      1.00\n",
            " weight[0,852]     -0.14      1.02     -0.10     -1.85      1.38    795.75      1.00\n",
            " weight[0,853]      0.07      0.98      0.04     -1.49      1.64   1473.89      1.00\n",
            " weight[0,854]      0.04      0.95      0.03     -1.51      1.55    819.72      1.00\n",
            " weight[0,855]      0.03      0.97      0.04     -1.58      1.47    825.84      1.00\n",
            " weight[0,856]     -0.18      0.97     -0.17     -1.83      1.36   1085.55      1.00\n",
            " weight[0,857]      0.59      0.97      0.59     -1.13      1.97    824.87      1.00\n",
            " weight[0,858]      0.84      0.99      0.85     -0.67      2.58   1475.27      1.00\n",
            " weight[0,859]      0.72      0.99      0.72     -0.95      2.37   1388.35      1.00\n",
            " weight[0,860]      0.37      0.95      0.36     -1.21      1.91   1276.17      1.00\n",
            " weight[0,861]      0.33      0.93      0.33     -1.28      1.72   1269.12      1.00\n",
            " weight[0,862]      0.11      0.96      0.13     -1.54      1.61    958.38      1.00\n",
            " weight[0,863]     -0.30      0.98     -0.34     -1.93      1.33   1021.19      1.01\n",
            " weight[0,864]     -0.13      0.97     -0.16     -1.88      1.35    848.77      1.00\n",
            " weight[0,865]     -0.10      0.94     -0.11     -1.40      1.67   1418.70      1.00\n",
            " weight[0,866]     -0.11      0.96     -0.09     -1.77      1.42   1577.28      1.00\n",
            " weight[0,867]     -0.25      1.00     -0.22     -1.87      1.47    789.83      1.00\n",
            " weight[0,868]     -0.16      0.94     -0.13     -1.74      1.33    835.02      1.00\n",
            " weight[0,869]     -0.27      0.97     -0.29     -1.88      1.22    694.92      1.00\n",
            " weight[0,870]     -0.17      1.00     -0.19     -1.72      1.50   1332.68      1.00\n",
            " weight[0,871]     -0.30      0.97     -0.29     -1.83      1.26   1286.34      1.00\n",
            " weight[0,872]     -0.24      0.97     -0.22     -1.79      1.28   1391.43      1.00\n",
            " weight[0,873]     -0.28      1.00     -0.32     -1.86      1.36    981.22      1.00\n",
            " weight[0,874]     -0.18      1.01     -0.19     -1.94      1.33   1576.92      1.00\n",
            " weight[0,875]     -0.16      0.98     -0.13     -1.64      1.49   1474.48      1.00\n",
            " weight[0,876]      0.01      0.99      0.01     -1.66      1.59   1269.87      1.00\n",
            " weight[0,877]     -0.15      1.00     -0.14     -1.84      1.38   1318.74      1.00\n",
            " weight[0,878]     -0.05      0.99     -0.05     -1.56      1.61   1209.69      1.00\n",
            " weight[0,879]     -0.07      0.94     -0.05     -1.76      1.31   4146.04      1.00\n",
            " weight[0,880]     -0.05      0.95     -0.03     -1.56      1.51   1742.58      1.00\n",
            " weight[0,881]     -0.29      0.99     -0.30     -1.91      1.33   1479.49      1.00\n",
            " weight[0,882]      0.08      0.93      0.05     -1.36      1.69   1022.16      1.00\n",
            " weight[0,883]      0.19      0.99      0.20     -1.57      1.67   1410.73      1.00\n",
            " weight[0,884]      0.18      1.01      0.18     -1.60      1.72    604.14      1.00\n",
            " weight[0,885]     -0.04      0.98     -0.02     -1.69      1.46   1354.44      1.00\n",
            " weight[0,886]     -0.07      0.97     -0.09     -1.54      1.66   1301.62      1.00\n",
            " weight[0,887]     -0.15      0.94     -0.14     -1.57      1.45   1037.68      1.00\n",
            " weight[0,888]      0.27      0.99      0.29     -1.29      1.89   1134.42      1.00\n",
            " weight[0,889]      0.15      1.05      0.18     -1.79      1.65   1543.56      1.00\n",
            " weight[0,890]      0.24      1.00      0.30     -1.43      1.70   1409.65      1.00\n",
            " weight[0,891]      0.38      0.88      0.38     -0.96      1.96   1685.52      1.00\n",
            " weight[0,892]      0.45      1.01      0.47     -1.23      2.10   1371.43      1.00\n",
            " weight[0,893]      0.55      1.03      0.54     -1.14      2.17    889.61      1.00\n",
            " weight[0,894]      0.32      0.95      0.35     -1.24      1.83   1465.54      1.00\n",
            " weight[0,895]      0.38      0.95      0.40     -1.18      1.90   1580.14      1.00\n",
            " weight[0,896]      0.46      0.98      0.48     -1.07      2.14    752.58      1.00\n",
            " weight[0,897]      0.06      0.98      0.07     -1.47      1.69   1988.72      1.00\n",
            " weight[0,898]     -0.09      0.89     -0.11     -1.34      1.50    934.01      1.00\n",
            " weight[0,899]     -0.06      0.94     -0.06     -1.92      1.22   1369.71      1.00\n",
            " weight[0,900]      0.03      0.98      0.03     -1.51      1.71    881.32      1.00\n",
            " weight[0,901]      0.00      1.00      0.02     -1.49      1.75   1031.36      1.00\n",
            " weight[0,902]      0.04      0.97      0.10     -1.53      1.65   1205.19      1.00\n",
            " weight[0,903]      0.07      1.06      0.02     -1.63      1.72   1327.73      1.00\n",
            " weight[0,904]      0.02      0.98      0.03     -1.73      1.54   1593.08      1.00\n",
            " weight[0,905]      0.00      0.96      0.00     -1.71      1.46    899.48      1.00\n",
            " weight[0,906]     -0.04      0.98     -0.04     -1.64      1.50   1220.73      1.00\n",
            " weight[0,907]     -0.04      0.97     -0.05     -1.62      1.54   1169.94      1.00\n",
            " weight[0,908]     -0.04      0.97     -0.05     -1.62      1.52   1426.66      1.00\n",
            " weight[0,909]     -0.07      0.98     -0.02     -1.76      1.43   2229.53      1.00\n",
            " weight[0,910]     -0.05      0.98     -0.07     -1.57      1.58    882.71      1.00\n",
            " weight[0,911]     -0.10      0.99     -0.09     -1.66      1.57   1332.37      1.00\n",
            " weight[0,912]     -0.09      0.99     -0.09     -1.66      1.59   1334.19      1.00\n",
            " weight[0,913]     -0.06      0.99     -0.04     -1.65      1.59   1483.07      1.00\n",
            " weight[0,914]     -0.09      0.95     -0.10     -1.78      1.31   1212.58      1.00\n",
            " weight[0,915]      0.27      1.06      0.28     -1.60      1.87   1328.35      1.00\n",
            " weight[0,916]     -0.05      0.99     -0.01     -1.75      1.42    622.28      1.00\n",
            " weight[0,917]     -0.00      1.05      0.06     -1.78      1.66    728.78      1.00\n",
            " weight[0,918]      0.14      0.98      0.15     -1.56      1.62    744.47      1.00\n",
            " weight[0,919]      0.12      0.96      0.10     -1.55      1.59    915.52      1.00\n",
            " weight[0,920]      0.00      1.02      0.02     -1.72      1.58   2389.96      1.00\n",
            " weight[0,921]     -0.00      1.02      0.02     -1.65      1.62    994.96      1.00\n",
            " weight[0,922]      0.01      1.03      0.06     -1.55      1.79    780.76      1.00\n",
            " weight[0,923]      0.09      1.00      0.07     -1.45      1.69   1376.57      1.00\n",
            " weight[0,924]      0.09      1.00      0.08     -1.57      1.71   2568.40      1.00\n",
            " weight[0,925]      0.07      0.96      0.08     -1.47      1.75   2317.27      1.00\n",
            " weight[0,926]      0.09      1.04      0.11     -1.56      1.87    613.90      1.00\n",
            " weight[0,927]      0.07      0.91      0.06     -1.63      1.45   1240.69      1.00\n",
            " weight[0,928]      0.05      0.97      0.01     -1.63      1.55   1423.43      1.00\n",
            " weight[0,929]     -0.02      1.02     -0.06     -1.71      1.70   2800.82      1.00\n",
            " weight[0,930]     -0.07      1.00     -0.12     -1.57      1.66   1903.05      1.00\n",
            " weight[0,931]     -0.09      0.97     -0.05     -1.72      1.43   1699.75      1.00\n",
            " weight[0,932]     -0.04      0.94     -0.03     -1.56      1.49   1506.02      1.00\n",
            " weight[0,933]     -0.03      0.96     -0.03     -1.45      1.66   1997.16      1.00\n",
            " weight[0,934]     -0.06      0.99     -0.06     -1.67      1.50   2110.16      1.00\n",
            " weight[0,935]      0.48      1.00      0.46     -1.08      2.16   1736.61      1.00\n",
            " weight[0,936]      0.16      0.97      0.21     -1.36      1.74   1222.47      1.00\n",
            " weight[0,937]      0.18      0.96      0.19     -1.26      1.72   1006.54      1.00\n",
            " weight[0,938]     -0.04      0.97     -0.04     -1.61      1.61   1863.64      1.00\n",
            " weight[0,939]     -0.06      0.99     -0.06     -1.77      1.41   1154.75      1.00\n",
            " weight[0,940]      0.10      0.96      0.09     -1.44      1.69   1096.38      1.00\n",
            " weight[0,941]      0.08      0.92      0.13     -1.47      1.54   1207.67      1.00\n",
            " weight[0,942]      0.06      0.94      0.07     -1.53      1.55    745.14      1.00\n",
            " weight[0,943]      0.25      0.97      0.29     -1.41      1.78   2051.99      1.00\n",
            " weight[0,944]      0.23      1.00      0.24     -1.36      1.85    953.85      1.00\n",
            " weight[0,945]      0.12      1.00      0.11     -1.68      1.63   1451.94      1.00\n",
            " weight[0,946]      0.10      1.00      0.11     -1.53      1.71   1321.67      1.00\n",
            " weight[0,947]      0.02      1.00      0.06     -1.59      1.60   1064.75      1.00\n",
            " weight[0,948]      0.01      0.93      0.02     -1.51      1.55   1434.65      1.00\n",
            " weight[0,949]     -0.04      0.97     -0.01     -1.65      1.64    918.69      1.00\n",
            " weight[0,950]     -0.08      0.99     -0.08     -1.85      1.42   2372.91      1.00\n",
            " weight[0,951]     -0.18      0.99     -0.23     -1.67      1.54    909.17      1.00\n",
            " weight[0,952]     -0.20      1.00     -0.25     -1.82      1.51    634.88      1.00\n",
            " weight[0,953]     -0.04      0.99     -0.08     -1.69      1.54   2652.14      1.00\n",
            " weight[0,954]     -0.24      1.02     -0.25     -1.76      1.56    926.11      1.00\n",
            " weight[0,955]     -0.59      0.98     -0.62     -2.10      1.09    665.08      1.00\n",
            " weight[0,956]     -0.44      0.96     -0.40     -1.92      1.13    969.24      1.00\n",
            " weight[0,957]     -0.33      1.00     -0.33     -1.89      1.30   1795.64      1.00\n",
            " weight[0,958]     -0.37      0.93     -0.36     -1.84      1.09   1170.81      1.00\n",
            " weight[0,959]     -0.31      0.97     -0.35     -1.91      1.25    678.86      1.00\n",
            " weight[0,960]     -0.35      1.06     -0.34     -2.16      1.29   2568.21      1.00\n",
            " weight[0,961]      0.02      0.99      0.03     -1.46      1.74   1536.15      1.00\n",
            " weight[0,962]      0.06      0.99      0.06     -1.76      1.55    776.59      1.00\n",
            " weight[0,963]      0.14      0.97      0.15     -1.52      1.67   1189.23      1.00\n",
            " weight[0,964]      0.19      0.86      0.17     -1.16      1.61    922.95      1.00\n",
            " weight[0,965]     -0.04      0.97      0.00     -1.58      1.56   1779.72      1.00\n",
            " weight[0,966]      0.05      0.95      0.08     -1.35      1.79    898.77      1.00\n",
            " weight[0,967]      0.12      0.98      0.12     -1.26      1.86   1122.40      1.00\n",
            " weight[0,968]     -0.02      0.95     -0.03     -1.53      1.55    798.29      1.00\n",
            " weight[0,969]      0.09      1.03      0.07     -1.54      1.71   1165.77      1.00\n",
            " weight[0,970]     -0.37      0.99     -0.34     -2.08      1.15   1172.51      1.00\n",
            " weight[0,971]     -0.30      0.95     -0.27     -1.82      1.25    882.02      1.00\n",
            " weight[0,972]     -0.37      1.01     -0.35     -2.03      1.30   2364.75      1.00\n",
            " weight[0,973]     -0.01      1.00      0.02     -1.74      1.52   2689.52      1.00\n",
            " weight[0,974]     -0.15      0.95     -0.16     -1.71      1.38   1241.81      1.00\n",
            " weight[0,975]     -0.18      1.01     -0.18     -1.95      1.41   1252.73      1.00\n",
            " weight[0,976]     -0.39      0.96     -0.40     -1.92      1.18   1051.18      1.00\n",
            " weight[0,977]     -0.10      0.97     -0.15     -1.74      1.44   1979.99      1.00\n",
            " weight[0,978]      0.17      0.94      0.13     -1.34      1.61   1039.05      1.00\n",
            " weight[0,979]      0.11      0.97      0.11     -1.43      1.73   1968.96      1.00\n",
            " weight[0,980]      0.34      1.01      0.31     -1.24      2.07    787.99      1.00\n",
            " weight[0,981]      0.26      1.03      0.23     -1.41      1.91    923.92      1.00\n",
            " weight[0,982]      0.18      0.98      0.19     -1.40      1.74    885.71      1.00\n",
            " weight[0,983]      0.44      0.99      0.45     -1.16      2.11   2024.99      1.00\n",
            " weight[0,984]      0.29      0.94      0.27     -1.05      1.99   1133.57      1.00\n",
            " weight[0,985]      0.30      0.99      0.30     -1.32      1.91   1199.05      1.00\n",
            " weight[0,986]      0.02      0.95      0.03     -1.43      1.66   1463.07      1.00\n",
            " weight[0,987]     -0.15      0.97     -0.15     -1.81      1.35   1330.95      1.00\n",
            " weight[0,988]     -0.24      1.00     -0.23     -1.90      1.28   2048.46      1.00\n",
            " weight[0,989]     -0.23      1.04     -0.23     -1.86      1.46    856.34      1.00\n",
            " weight[0,990]     -0.22      0.93     -0.22     -1.67      1.39    963.34      1.00\n",
            " weight[0,991]     -0.22      0.96     -0.22     -1.80      1.31   2780.42      1.00\n",
            " weight[0,992]     -0.23      0.95     -0.23     -1.74      1.30   1517.51      1.00\n",
            " weight[0,993]     -0.28      0.96     -0.34     -1.79      1.34   1062.21      1.00\n",
            " weight[0,994]     -0.31      1.01     -0.32     -1.90      1.32   1544.73      1.00\n",
            " weight[0,995]     -0.24      0.96     -0.25     -1.73      1.48   1733.57      1.00\n",
            " weight[0,996]     -0.29      1.03     -0.29     -2.07      1.32   1188.48      1.00\n",
            " weight[0,997]     -0.25      0.99     -0.27     -1.80      1.46    819.18      1.00\n",
            " weight[0,998]     -0.12      0.98     -0.11     -1.76      1.44   1393.15      1.00\n",
            " weight[0,999]     -0.10      1.00     -0.08     -1.68      1.63    979.51      1.00\n",
            "weight[0,1000]     -0.05      0.99     -0.04     -1.75      1.49   1494.49      1.00\n",
            "weight[0,1001]      0.12      1.05      0.10     -1.57      1.84   1028.49      1.00\n",
            "weight[0,1002]      0.10      0.99      0.12     -1.50      1.72   1497.46      1.00\n",
            "weight[0,1003]      0.11      1.02      0.09     -1.60      1.74    695.25      1.00\n",
            "weight[0,1004]      0.07      1.01      0.06     -1.49      1.76    746.18      1.01\n",
            "weight[0,1005]      0.03      1.02      0.04     -1.77      1.56    884.96      1.00\n",
            "weight[0,1006]      0.01      1.03      0.01     -1.64      1.70   1702.24      1.00\n",
            "weight[0,1007]     -0.05      1.02     -0.02     -1.62      1.77   1287.74      1.00\n",
            "weight[0,1008]     -0.01      1.02     -0.03     -1.61      1.71   2045.00      1.00\n",
            "weight[0,1009]     -0.09      0.99     -0.12     -1.60      1.60   1532.65      1.00\n",
            "weight[0,1010]     -0.03      1.05     -0.07     -1.84      1.68   1334.82      1.00\n",
            "weight[0,1011]     -0.10      1.00     -0.13     -1.69      1.51   1215.48      1.00\n",
            "weight[0,1012]     -0.08      1.00     -0.10     -1.78      1.40   1650.30      1.00\n",
            "weight[0,1013]     -0.12      1.00     -0.12     -1.84      1.40   1074.69      1.00\n",
            "weight[0,1014]     -0.05      1.03     -0.06     -1.70      1.64    817.83      1.00\n",
            "weight[0,1015]     -0.13      1.00     -0.15     -1.77      1.47   1086.69      1.00\n",
            "weight[0,1016]      0.03      1.02     -0.02     -1.60      1.70   1107.97      1.00\n",
            "weight[0,1017]     -0.04      0.98     -0.05     -1.55      1.63    985.41      1.00\n",
            "weight[0,1018]      0.01      0.99      0.04     -1.64      1.65    955.55      1.00\n",
            "weight[0,1019]     -0.11      1.01     -0.12     -1.77      1.56   2510.86      1.00\n",
            "weight[0,1020]     -0.07      0.95     -0.08     -1.76      1.39    643.97      1.00\n",
            "weight[0,1021]     -0.03      1.07     -0.01     -1.65      1.87   2463.42      1.00\n",
            "weight[0,1022]     -0.03      1.04     -0.02     -1.72      1.63   1117.68      1.00\n",
            "weight[0,1023]     -0.09      0.97     -0.06     -1.77      1.50    923.99      1.00\n",
            "weight[0,1024]     -0.14      1.00     -0.12     -1.93      1.32   1050.35      1.00\n",
            "weight[0,1025]      0.03      0.96      0.04     -1.41      1.60   1331.71      1.00\n",
            "weight[0,1026]      0.19      0.98      0.19     -1.49      1.74   2224.65      1.00\n",
            "weight[0,1027]      0.22      0.95      0.22     -1.29      1.77    960.65      1.00\n",
            "weight[0,1028]      0.15      1.01      0.14     -1.40      1.89   1449.36      1.00\n",
            "weight[0,1029]      0.15      0.99      0.13     -1.48      1.72   2862.42      1.00\n",
            "weight[0,1030]      0.06      0.98      0.06     -1.61      1.58   2198.26      1.00\n",
            "weight[0,1031]     -0.07      1.01     -0.11     -1.72      1.53   1367.85      1.00\n",
            "weight[0,1032]     -0.06      0.98     -0.07     -1.66      1.54   1677.42      1.00\n",
            "weight[0,1033]     -0.03      0.98     -0.01     -1.71      1.51   1603.28      1.00\n",
            "weight[0,1034]      0.02      0.97      0.05     -1.54      1.55    781.39      1.00\n",
            "weight[0,1035]      0.23      0.95      0.21     -1.47      1.68   1121.88      1.00\n",
            "weight[0,1036]      0.17      0.99      0.19     -1.36      1.85   1341.93      1.00\n",
            "weight[0,1037]     -0.32      0.90     -0.33     -1.62      1.29   1646.07      1.00\n",
            "weight[0,1038]     -0.24      0.95     -0.22     -1.91      1.14   1359.01      1.00\n",
            "weight[0,1039]     -0.24      1.00     -0.23     -1.72      1.58   2070.77      1.00\n",
            "weight[0,1040]     -0.15      1.04     -0.14     -1.88      1.52   2860.41      1.00\n",
            "weight[0,1041]     -0.27      0.97     -0.28     -1.78      1.37    315.00      1.00\n",
            "weight[0,1042]     -0.35      1.00     -0.37     -2.03      1.22   2022.78      1.00\n",
            "weight[0,1043]     -0.34      0.99     -0.32     -1.93      1.34    762.59      1.00\n",
            "weight[0,1044]     -0.35      0.93     -0.35     -1.74      1.31   1535.58      1.00\n",
            "weight[0,1045]     -0.37      0.98     -0.37     -1.93      1.25   1407.83      1.00\n",
            "weight[0,1046]     -0.11      0.98     -0.10     -1.75      1.46   1784.20      1.00\n",
            "weight[0,1047]      0.11      0.98      0.14     -1.66      1.54    727.59      1.00\n",
            "weight[0,1048]      0.04      0.97      0.05     -1.41      1.73   1572.10      1.00\n",
            "weight[0,1049]      0.01      1.02     -0.04     -1.65      1.69    968.05      1.00\n",
            "weight[0,1050]      0.04      1.00      0.04     -1.55      1.58   1015.99      1.00\n",
            "weight[0,1051]      0.03      1.04      0.05     -1.73      1.66    889.44      1.00\n",
            "weight[0,1052]     -0.10      0.99     -0.13     -1.62      1.54   1367.15      1.00\n",
            "weight[0,1053]     -0.14      0.95     -0.17     -1.54      1.52   1916.71      1.00\n",
            "weight[0,1054]     -0.09      0.99     -0.10     -1.70      1.56   1062.22      1.00\n",
            "weight[0,1055]      0.06      1.00      0.05     -1.41      1.86   1467.01      1.00\n",
            "weight[0,1056]      0.02      1.02      0.01     -1.66      1.66   2226.74      1.00\n",
            "weight[0,1057]      0.02      1.00      0.03     -1.52      1.77    938.02      1.00\n",
            "weight[0,1058]      0.00      0.97      0.01     -1.66      1.47    469.14      1.00\n",
            "weight[0,1059]     -0.11      0.98     -0.12     -1.84      1.40   1906.75      1.00\n",
            "weight[0,1060]     -0.06      0.96     -0.06     -1.58      1.54   1729.67      1.00\n",
            "weight[0,1061]     -0.09      1.09     -0.07     -2.02      1.60   2780.18      1.00\n",
            "weight[0,1062]     -0.00      1.04      0.01     -1.71      1.75    890.51      1.00\n",
            "weight[0,1063]      0.00      1.00      0.05     -1.51      1.69    857.04      1.00\n",
            "weight[0,1064]      0.01      0.92     -0.01     -1.51      1.47   2282.50      1.00\n",
            "weight[0,1065]     -0.01      0.95     -0.02     -1.52      1.55   1075.45      1.00\n",
            "weight[0,1066]     -0.06      0.99     -0.08     -1.75      1.56    836.73      1.00\n",
            "weight[0,1067]     -0.01      0.99      0.01     -1.75      1.52   2372.08      1.00\n",
            "weight[0,1068]      0.30      0.93      0.35     -1.25      1.74    682.66      1.00\n",
            "weight[0,1069]     -0.03      1.02     -0.01     -1.74      1.56   2123.49      1.00\n",
            "weight[0,1070]      0.17      0.96      0.16     -1.43      1.67    650.87      1.00\n",
            "weight[0,1071]     -0.11      0.98     -0.09     -1.48      1.59   1206.14      1.00\n",
            "weight[0,1072]     -0.07      1.00     -0.08     -1.59      1.62   1020.48      1.00\n",
            "weight[0,1073]     -0.23      0.97     -0.25     -1.82      1.32   1620.48      1.00\n",
            "weight[0,1074]     -0.22      0.98     -0.22     -1.88      1.32    982.26      1.00\n",
            "weight[0,1075]     -0.23      0.99     -0.27     -1.80      1.43   1948.94      1.00\n",
            "weight[0,1076]     -0.26      0.97     -0.28     -1.80      1.42   1411.73      1.00\n",
            "weight[0,1077]     -0.27      0.97     -0.28     -1.88      1.25   1792.36      1.00\n",
            "weight[0,1078]     -0.20      0.92     -0.19     -1.60      1.43   1201.36      1.00\n",
            "weight[0,1079]     -0.19      0.94     -0.20     -1.67      1.36   1699.37      1.00\n",
            "weight[0,1080]     -0.22      0.98     -0.26     -1.78      1.35    898.57      1.00\n",
            "weight[0,1081]      0.22      0.96      0.20     -1.26      1.86   1209.91      1.00\n",
            "weight[0,1082]      0.10      0.99      0.11     -1.53      1.72   1914.36      1.00\n",
            "weight[0,1083]     -0.17      0.99     -0.19     -1.67      1.58    633.94      1.00\n",
            "weight[0,1084]     -0.30      1.01     -0.29     -1.91      1.34   1611.79      1.00\n",
            "weight[0,1085]     -0.21      0.95     -0.25     -1.85      1.23   2070.46      1.00\n",
            "weight[0,1086]     -0.17      1.02     -0.15     -1.72      1.58   2193.84      1.00\n",
            "weight[0,1087]     -0.22      1.05     -0.22     -1.93      1.42   1681.82      1.00\n",
            "weight[0,1088]     -0.27      1.00     -0.28     -2.03      1.24    558.88      1.00\n",
            "weight[0,1089]     -0.08      0.99     -0.09     -1.59      1.66   1529.25      1.00\n",
            "weight[0,1090]     -0.02      0.98     -0.00     -1.60      1.52    872.54      1.00\n",
            "weight[0,1091]     -0.02      1.07     -0.03     -1.80      1.66    840.65      1.00\n",
            "weight[0,1092]      0.29      1.00      0.27     -1.28      1.93   1474.91      1.00\n",
            "weight[0,1093]      0.07      0.92      0.09     -1.29      1.60    927.46      1.00\n",
            "weight[0,1094]      0.11      0.99      0.12     -1.56      1.57   1712.32      1.00\n",
            "weight[0,1095]      0.16      0.88      0.16     -1.30      1.56    909.57      1.00\n",
            "weight[0,1096]      0.22      0.97      0.24     -1.47      1.70   1346.35      1.00\n",
            "weight[0,1097]      0.23      1.01      0.22     -1.49      1.79   1147.12      1.00\n",
            "weight[0,1098]      0.16      0.99      0.13     -1.40      1.88   1284.85      1.00\n",
            "weight[0,1099]      0.05      0.98      0.02     -1.48      1.63   1228.59      1.00\n",
            "weight[0,1100]      0.05      0.99      0.06     -1.49      1.73    758.71      1.00\n",
            "weight[0,1101]      0.14      0.93      0.12     -1.47      1.55   1108.75      1.00\n",
            "weight[0,1102]      0.10      1.00      0.08     -1.40      1.87   2296.99      1.00\n",
            "weight[0,1103]     -0.17      0.97     -0.17     -1.75      1.36   1596.16      1.00\n",
            "weight[0,1104]     -0.31      0.98     -0.32     -1.94      1.27    804.14      1.01\n",
            "weight[0,1105]     -0.16      0.92     -0.20     -1.57      1.42   1428.20      1.00\n",
            "weight[0,1106]     -0.15      1.00     -0.15     -1.70      1.58    901.13      1.00\n",
            "weight[0,1107]     -0.16      0.99     -0.16     -1.67      1.50    814.28      1.00\n",
            "weight[0,1108]     -0.18      0.98     -0.15     -1.90      1.27   1508.25      1.00\n",
            "weight[0,1109]     -0.10      1.01     -0.09     -1.65      1.56   1056.94      1.00\n",
            "weight[0,1110]      0.35      0.97      0.35     -1.22      2.01   1139.44      1.00\n",
            "weight[0,1111]      0.28      1.04      0.24     -1.33      2.05   2246.33      1.00\n",
            "weight[0,1112]      0.38      1.01      0.38     -1.33      1.97   1514.96      1.00\n",
            "weight[0,1113]      0.36      0.92      0.37     -1.08      1.89    764.63      1.00\n",
            "weight[0,1114]      0.35      0.91      0.36     -1.16      1.80    939.22      1.00\n",
            "weight[0,1115]      0.28      0.99      0.22     -1.28      1.91    952.86      1.00\n",
            "weight[0,1116]      0.27      1.01      0.28     -1.48      1.82    479.10      1.00\n",
            "weight[0,1117]      0.24      0.99      0.26     -1.20      2.08    833.94      1.00\n",
            "weight[0,1118]      0.22      0.99      0.21     -1.46      1.72    746.55      1.00\n",
            "weight[0,1119]      0.22      0.99      0.23     -1.34      1.84    995.51      1.00\n",
            "weight[0,1120]      0.29      1.03      0.30     -1.34      2.06   1700.55      1.00\n",
            "weight[0,1121]      0.19      0.99      0.17     -1.47      1.76   1716.12      1.00\n",
            "weight[0,1122]      0.22      0.97      0.28     -1.37      1.67   1084.63      1.00\n",
            "weight[0,1123]      0.23      0.98      0.23     -1.49      1.74    785.61      1.00\n",
            "weight[0,1124]      0.20      0.98      0.21     -1.44      1.70   1481.16      1.00\n",
            "weight[0,1125]      0.23      0.97      0.23     -1.34      1.80   2782.74      1.00\n",
            "weight[0,1126]      0.24      0.98      0.24     -1.16      2.04   1063.82      1.00\n",
            "weight[0,1127]      0.30      0.95      0.30     -1.29      1.82   2197.39      1.00\n",
            "weight[0,1128]      0.26      0.97      0.26     -1.47      1.68    663.18      1.00\n",
            "weight[0,1129]      0.29      1.00      0.27     -1.34      1.82    683.03      1.00\n",
            "weight[0,1130]      0.24      1.00      0.19     -1.29      1.95   1390.31      1.00\n",
            "weight[0,1131]      0.23      1.01      0.23     -1.34      1.94    922.14      1.00\n",
            "weight[0,1132]      0.26      1.02      0.27     -1.39      1.93   1547.95      1.00\n",
            "weight[0,1133]      0.29      1.08      0.29     -1.57      1.88   2413.08      1.00\n",
            "weight[0,1134]      0.27      0.99      0.26     -1.13      2.03   1508.90      1.00\n",
            "weight[0,1135]      0.25      0.95      0.26     -1.23      1.88   1383.45      1.00\n",
            "weight[0,1136]      0.36      0.97      0.33     -1.34      1.89   2104.44      1.00\n",
            "weight[0,1137]      0.22      0.98      0.22     -1.34      1.86   1328.48      1.00\n",
            "weight[0,1138]      0.21      1.03      0.21     -1.41      2.00   1983.13      1.00\n",
            "weight[0,1139]      0.06      1.02      0.10     -1.59      1.68    798.67      1.00\n",
            "weight[0,1140]      0.00      1.03     -0.01     -1.74      1.61   1283.61      1.00\n",
            "weight[0,1141]      0.20      1.03      0.21     -1.42      1.93    969.96      1.00\n",
            "weight[0,1142]      0.20      0.92      0.21     -1.29      1.73   2787.00      1.00\n",
            "weight[0,1143]      0.19      1.05      0.18     -1.38      2.03   1306.95      1.00\n",
            "weight[0,1144]      0.20      1.00      0.19     -1.36      1.91   2637.20      1.00\n",
            "weight[0,1145]      0.10      0.99      0.09     -1.62      1.70   1770.49      1.00\n",
            "weight[0,1146]      0.06      0.96      0.06     -1.57      1.57   1097.35      1.00\n",
            "weight[0,1147]      0.05      1.02      0.04     -1.64      1.76    797.75      1.00\n",
            "weight[0,1148]      0.10      1.03      0.08     -1.43      1.87    866.72      1.00\n",
            "weight[0,1149]      0.08      1.02      0.07     -1.72      1.70   1156.40      1.00\n",
            "weight[0,1150]      0.07      0.99      0.09     -1.53      1.61   1242.26      1.00\n",
            "weight[0,1151]      0.05      1.00      0.05     -1.64      1.49   1571.71      1.00\n",
            "weight[0,1152]      0.06      0.99      0.07     -1.44      1.80   1207.00      1.00\n",
            "weight[0,1153]      0.13      0.96      0.15     -1.54      1.63    624.75      1.00\n",
            "weight[0,1154]      0.04      0.98      0.02     -1.54      1.61    857.72      1.00\n",
            "weight[0,1155]      0.09      1.00      0.11     -1.68      1.57   1657.75      1.00\n",
            "weight[0,1156]      0.29      0.94      0.25     -1.17      1.90    585.90      1.00\n",
            "weight[0,1157]      0.27      0.99      0.29     -1.25      1.93   1157.77      1.00\n",
            "weight[0,1158]      0.31      1.03      0.33     -1.23      2.23   1494.20      1.00\n",
            "weight[0,1159]      0.31      1.02      0.27     -1.46      1.87   1721.52      1.00\n",
            "weight[0,1160]      0.15      0.97      0.16     -1.37      1.73    944.09      1.00\n",
            "weight[0,1161]      0.17      0.95      0.12     -1.40      1.65   1246.07      1.00\n",
            "weight[0,1162]      0.15      0.98      0.14     -1.48      1.61    941.60      1.00\n",
            "weight[0,1163]     -0.00      1.00      0.00     -1.55      1.68   1226.88      1.00\n",
            "weight[0,1164]     -0.05      0.99     -0.04     -1.57      1.70    949.44      1.00\n",
            "weight[0,1165]     -0.10      0.99     -0.11     -1.49      1.76   1158.49      1.00\n",
            "weight[0,1166]     -0.25      1.02     -0.27     -2.15      1.19    922.71      1.00\n",
            "weight[0,1167]     -0.23      0.98     -0.23     -1.90      1.29   1180.42      1.00\n",
            "weight[0,1168]     -0.15      0.93     -0.18     -1.72      1.37   1538.27      1.00\n",
            "weight[0,1169]     -0.18      1.02     -0.20     -1.89      1.45   1079.45      1.00\n",
            "weight[0,1170]     -0.14      0.96     -0.12     -1.76      1.50   1150.72      1.00\n",
            "weight[0,1171]     -0.15      0.93     -0.17     -1.54      1.45    914.80      1.00\n",
            "weight[0,1172]     -0.01      0.99      0.02     -1.61      1.58    906.84      1.00\n",
            "weight[0,1173]     -0.08      0.98     -0.11     -1.70      1.47   1972.52      1.00\n",
            "weight[0,1174]     -0.11      0.97     -0.11     -1.68      1.47   1802.47      1.00\n",
            "weight[0,1175]     -0.04      0.95     -0.07     -1.63      1.51    810.59      1.00\n",
            "weight[0,1176]      0.04      0.95      0.06     -1.54      1.56   2168.84      1.00\n",
            "weight[0,1177]      0.06      1.07      0.07     -1.71      1.78   1122.83      1.00\n",
            "weight[0,1178]     -0.02      0.96     -0.05     -1.64      1.54   1535.63      1.00\n",
            "weight[0,1179]      0.07      0.97      0.06     -1.38      1.74   1853.23      1.00\n",
            "weight[0,1180]      0.11      1.00      0.09     -1.70      1.60   1147.93      1.00\n",
            "weight[0,1181]      0.04      1.01      0.05     -1.67      1.58    919.64      1.00\n",
            "weight[0,1182]      0.05      0.96      0.05     -1.54      1.54   1422.35      1.00\n",
            "weight[0,1183]      0.05      0.98      0.02     -1.49      1.71   1152.69      1.00\n",
            "weight[0,1184]     -0.01      0.97     -0.00     -1.49      1.70    901.10      1.00\n",
            "weight[0,1185]      0.08      0.96      0.09     -1.50      1.62   1435.47      1.00\n",
            "weight[0,1186]     -0.02      0.99     -0.04     -1.83      1.43   1529.89      1.00\n",
            "weight[0,1187]     -0.13      1.00     -0.14     -1.65      1.64    869.97      1.00\n",
            "weight[0,1188]     -0.18      0.97     -0.17     -1.64      1.54    956.38      1.00\n",
            "weight[0,1189]     -0.24      0.92     -0.25     -1.80      1.19   1259.04      1.00\n",
            "weight[0,1190]     -0.24      0.92     -0.22     -1.81      1.22   1331.57      1.00\n",
            "weight[0,1191]     -0.25      0.99     -0.22     -1.85      1.34   1597.68      1.00\n",
            "weight[0,1192]     -0.03      1.00     -0.04     -1.60      1.59    890.88      1.01\n",
            "weight[0,1193]     -0.14      0.99     -0.14     -1.65      1.58   1921.83      1.00\n",
            "weight[0,1194]     -0.16      0.98     -0.13     -1.89      1.41    525.08      1.00\n",
            "weight[0,1195]     -0.14      1.00     -0.15     -1.70      1.57   1846.42      1.00\n",
            "weight[0,1196]      0.06      0.95      0.02     -1.45      1.64    871.02      1.00\n",
            "weight[0,1197]      0.14      0.97      0.15     -1.45      1.74   2244.01      1.00\n",
            "weight[0,1198]     -0.04      0.98     -0.04     -1.56      1.51   1106.20      1.00\n",
            "weight[0,1199]     -0.06      0.94     -0.06     -1.58      1.41    817.51      1.00\n",
            "weight[0,1200]     -0.08      0.94     -0.09     -1.55      1.55   1527.64      1.00\n",
            "weight[0,1201]     -0.18      0.94     -0.21     -1.49      1.67   1102.65      1.00\n",
            "weight[0,1202]     -0.10      0.99     -0.09     -1.72      1.56   2516.44      1.00\n",
            "weight[0,1203]     -0.01      0.99      0.01     -1.64      1.58   2040.45      1.00\n",
            "weight[0,1204]     -0.12      1.02     -0.12     -1.75      1.47   1262.51      1.00\n",
            "weight[0,1205]     -0.16      0.92     -0.15     -1.55      1.50   2722.13      1.00\n",
            "weight[0,1206]     -0.16      0.94     -0.15     -1.86      1.26   2033.18      1.00\n",
            "weight[0,1207]     -0.12      0.98     -0.10     -1.84      1.43   2243.53      1.00\n",
            "weight[0,1208]      0.19      0.99      0.19     -1.33      1.88    788.97      1.00\n",
            "weight[0,1209]     -0.04      0.95     -0.05     -1.60      1.36   1374.70      1.00\n",
            "weight[0,1210]     -0.07      1.04     -0.11     -1.70      1.69   1726.62      1.00\n",
            "weight[0,1211]     -0.19      1.03     -0.20     -1.88      1.44    734.41      1.00\n",
            "weight[0,1212]      0.07      0.96      0.10     -1.47      1.62   1049.24      1.00\n",
            "weight[0,1213]      0.10      1.02      0.11     -1.49      1.73   2385.57      1.00\n",
            "weight[0,1214]      0.04      0.99      0.01     -1.56      1.62   1264.34      1.00\n",
            "weight[0,1215]      0.04      0.97      0.06     -1.75      1.54    737.42      1.00\n",
            "weight[0,1216]      0.10      0.97      0.08     -1.60      1.56   1284.31      1.00\n",
            "weight[0,1217]      0.21      0.96      0.23     -1.43      1.67    752.53      1.00\n",
            "weight[0,1218]      0.13      0.93      0.11     -1.42      1.56    904.14      1.00\n",
            "weight[0,1219]      0.21      0.97      0.19     -1.39      1.74    870.40      1.00\n",
            "weight[0,1220]      0.21      0.96      0.27     -1.33      1.85    917.75      1.00\n",
            "weight[0,1221]      0.20      0.97      0.22     -1.41      1.71   1252.47      1.00\n",
            "weight[0,1222]      0.19      0.97      0.17     -1.29      1.85   2099.81      1.00\n",
            "weight[0,1223]      0.12      1.00      0.10     -1.48      1.86   2786.40      1.00\n",
            "weight[0,1224]      0.05      0.98      0.05     -1.44      1.77   2275.05      1.00\n",
            "weight[0,1225]      0.05      1.01      0.04     -1.59      1.68   1069.10      1.00\n",
            "weight[0,1226]      0.05      0.97      0.03     -1.54      1.55   1585.73      1.00\n",
            "weight[0,1227]     -0.07      0.91     -0.09     -1.47      1.54    836.54      1.00\n",
            "weight[0,1228]     -0.03      0.99     -0.03     -1.58      1.56   1062.48      1.00\n",
            "weight[0,1229]     -0.03      0.97     -0.03     -1.61      1.57    967.51      1.00\n",
            "weight[0,1230]      0.11      0.96      0.14     -1.58      1.53    655.03      1.00\n",
            "weight[0,1231]      0.02      1.00     -0.06     -1.37      1.90   1164.05      1.00\n",
            "weight[0,1232]      0.07      0.99      0.08     -1.52      1.71    658.11      1.00\n",
            "weight[0,1233]      0.09      1.00      0.08     -1.37      1.90   1282.00      1.00\n",
            "weight[0,1234]      0.01      1.04      0.02     -1.61      1.76   1031.66      1.00\n",
            "weight[0,1235]     -0.03      0.98     -0.03     -1.57      1.59   1729.12      1.00\n",
            "weight[0,1236]      0.02      0.94      0.04     -1.63      1.38   1638.22      1.00\n",
            "weight[0,1237]      0.10      1.03      0.10     -1.52      1.72    352.14      1.00\n",
            "weight[0,1238]      0.03      0.95      0.05     -1.31      1.74   1651.43      1.00\n",
            "weight[0,1239]     -0.08      0.98     -0.09     -1.65      1.48   1269.91      1.00\n",
            "weight[0,1240]      0.11      0.99      0.10     -1.55      1.67    384.05      1.00\n",
            "weight[0,1241]      0.12      0.95      0.09     -1.29      1.80   1456.40      1.00\n",
            "weight[0,1242]      0.21      0.98      0.18     -1.37      1.81   1381.32      1.00\n",
            "weight[0,1243]      0.18      0.97      0.15     -1.33      1.88    899.58      1.00\n",
            "weight[0,1244]      0.08      1.02      0.06     -1.65      1.66   1303.88      1.00\n",
            "weight[0,1245]      0.03      1.01      0.03     -1.51      1.72    805.93      1.00\n",
            "weight[0,1246]      0.10      0.98      0.10     -1.60      1.64   1495.75      1.00\n",
            "weight[0,1247]      0.20      0.98      0.18     -1.48      1.73    942.83      1.00\n",
            "weight[0,1248]      0.19      1.02      0.22     -1.29      2.05    876.07      1.00\n",
            "weight[0,1249]      0.20      0.94      0.18     -1.31      1.71   1019.05      1.00\n",
            "weight[0,1250]      0.02      1.00      0.01     -1.63      1.63    614.06      1.00\n",
            "weight[0,1251]      0.09      0.99      0.10     -1.43      1.74   1344.54      1.00\n",
            "weight[0,1252]      0.13      1.05      0.15     -1.51      1.95   1336.65      1.00\n",
            "weight[0,1253]      0.10      0.99      0.09     -1.48      1.73   1202.31      1.00\n",
            "weight[0,1254]      0.17      0.96      0.17     -1.35      1.75   1435.42      1.00\n",
            "weight[0,1255]      0.09      0.98      0.09     -1.48      1.69   1494.61      1.00\n",
            "weight[0,1256]      0.03      0.93      0.01     -1.57      1.45    793.50      1.01\n",
            "weight[0,1257]      0.12      1.01      0.14     -1.50      1.70   1194.85      1.00\n",
            "weight[0,1258]      0.07      0.99      0.07     -1.46      1.68   2073.62      1.00\n",
            "weight[0,1259]     -0.03      0.99     -0.06     -1.73      1.55    962.62      1.00\n",
            "weight[0,1260]      0.01      0.96     -0.03     -1.68      1.47   1010.25      1.00\n",
            "weight[0,1261]     -0.26      1.02     -0.28     -2.07      1.37   2446.83      1.00\n",
            "weight[0,1262]     -0.08      0.93     -0.11     -1.54      1.41   2009.01      1.00\n",
            "weight[0,1263]     -0.11      0.99     -0.10     -1.54      1.69    963.31      1.00\n",
            "weight[0,1264]     -0.11      1.06     -0.11     -2.06      1.50   1008.59      1.00\n",
            "weight[0,1265]     -0.24      0.99     -0.24     -1.77      1.45    920.58      1.00\n",
            "weight[0,1266]     -0.23      1.01     -0.23     -1.93      1.38   3410.54      1.00\n",
            "weight[0,1267]     -0.17      1.00     -0.17     -1.74      1.41   1734.59      1.00\n",
            "weight[0,1268]     -0.16      0.97     -0.14     -1.77      1.36   1260.87      1.00\n",
            "weight[0,1269]     -0.12      1.00     -0.11     -1.76      1.50    714.40      1.00\n",
            "weight[0,1270]     -0.16      0.99     -0.15     -1.76      1.43   1153.48      1.00\n",
            "weight[0,1271]     -0.05      0.98     -0.05     -1.82      1.37   1117.22      1.00\n",
            "weight[0,1272]     -0.06      0.97     -0.08     -1.68      1.48   1408.53      1.00\n",
            "weight[0,1273]     -0.28      0.97     -0.27     -1.88      1.25   1967.31      1.00\n",
            "weight[0,1274]     -0.05      0.94     -0.07     -1.59      1.51    570.12      1.00\n",
            "weight[0,1275]     -0.18      1.00     -0.19     -1.92      1.36   1474.75      1.00\n",
            "weight[0,1276]     -0.46      1.06     -0.45     -2.21      1.22   2560.91      1.00\n",
            "weight[0,1277]     -0.27      1.01     -0.28     -1.89      1.30   1822.95      1.00\n",
            "weight[0,1278]     -0.36      0.96     -0.34     -1.86      1.23   1803.56      1.00\n",
            "weight[0,1279]     -0.06      0.95     -0.05     -1.41      1.67    821.27      1.00\n",
            "weight[0,1280]     -0.00      1.01     -0.04     -1.57      1.76   2117.89      1.00\n",
            "weight[0,1281]      0.00      0.97     -0.01     -1.49      1.68   1312.44      1.00\n",
            "weight[0,1282]     -0.15      1.03     -0.16     -1.87      1.48   1358.88      1.00\n",
            "weight[0,1283]     -0.22      1.01     -0.22     -2.01      1.27   1373.39      1.00\n",
            "weight[0,1284]     -0.08      1.00     -0.06     -1.72      1.48   1746.16      1.00\n",
            "weight[0,1285]     -0.03      0.95     -0.04     -1.59      1.54   1774.37      1.00\n",
            "weight[0,1286]     -0.11      0.96     -0.12     -1.77      1.36    848.97      1.00\n",
            "weight[0,1287]     -0.16      1.00     -0.16     -1.73      1.40   1727.06      1.00\n",
            "weight[0,1288]      0.01      0.98      0.01     -1.53      1.66   1115.61      1.00\n",
            "weight[0,1289]     -0.02      1.01     -0.03     -1.48      1.79    880.69      1.00\n",
            "weight[0,1290]      0.01      1.03     -0.07     -1.61      1.73   1629.83      1.00\n",
            "weight[0,1291]     -0.05      1.02     -0.05     -1.80      1.55    798.20      1.00\n",
            "weight[0,1292]      0.04      1.01      0.05     -1.58      1.75   1391.16      1.00\n",
            "weight[0,1293]      0.23      0.95      0.22     -1.26      1.86   1239.02      1.00\n",
            "weight[0,1294]      0.21      1.01      0.24     -1.55      1.85   1475.46      1.00\n",
            "weight[0,1295]      0.23      1.00      0.22     -1.31      1.83   1249.82      1.00\n",
            "weight[0,1296]      0.24      0.98      0.23     -1.43      1.88   2479.00      1.00\n",
            "weight[0,1297]      0.27      0.95      0.26     -1.26      1.77    837.44      1.01\n",
            "weight[0,1298]      0.29      0.98      0.30     -1.36      1.85   1068.13      1.00\n",
            "weight[0,1299]      0.19      0.97      0.21     -1.34      1.72   1883.88      1.00\n",
            "weight[0,1300]      0.10      1.04      0.09     -1.45      1.96   1093.40      1.00\n",
            "weight[0,1301]      0.15      0.98      0.16     -1.39      1.76   1107.80      1.00\n",
            "weight[0,1302]     -0.04      0.99     -0.04     -1.59      1.65    764.08      1.00\n",
            "weight[0,1303]     -0.17      1.04     -0.20     -1.75      1.63   3031.27      1.00\n",
            "weight[0,1304]     -0.04      0.99     -0.07     -1.69      1.45   1056.53      1.00\n",
            "weight[0,1305]     -0.25      0.97     -0.23     -1.72      1.54    978.41      1.00\n",
            "weight[0,1306]     -0.09      0.99     -0.10     -1.71      1.53   2567.05      1.00\n",
            "weight[0,1307]     -0.03      0.97     -0.05     -1.70      1.40   1507.38      1.00\n",
            "weight[0,1308]     -0.09      0.96     -0.11     -1.62      1.50    953.09      1.00\n",
            "weight[0,1309]     -0.10      0.94     -0.11     -1.66      1.44    729.21      1.00\n",
            "weight[0,1310]     -0.07      0.97     -0.08     -1.87      1.30   1485.05      1.00\n",
            "weight[0,1311]     -0.11      0.96     -0.09     -1.86      1.35   1414.41      1.00\n",
            "weight[0,1312]     -0.10      0.99     -0.12     -1.83      1.35   1565.06      1.00\n",
            "weight[0,1313]     -0.42      1.00     -0.47     -2.06      1.14   1345.80      1.00\n",
            "weight[0,1314]     -0.37      0.98     -0.39     -1.96      1.28   1617.85      1.00\n",
            "weight[0,1315]     -0.38      1.01     -0.38     -1.94      1.30    819.14      1.00\n",
            "weight[0,1316]     -0.43      1.00     -0.47     -2.12      1.18    677.14      1.00\n",
            "weight[0,1317]     -0.41      0.95     -0.45     -1.84      1.26   1953.71      1.00\n",
            "weight[0,1318]     -0.41      1.03     -0.42     -2.04      1.17   1198.74      1.00\n",
            "weight[0,1319]     -0.23      1.00     -0.21     -1.83      1.43   1874.34      1.00\n",
            "weight[0,1320]     -0.26      0.95     -0.29     -1.67      1.46    677.11      1.00\n",
            "weight[0,1321]     -0.26      0.98     -0.29     -1.82      1.33    842.29      1.00\n",
            "weight[0,1322]     -0.07      0.96     -0.10     -1.91      1.26   1478.86      1.00\n",
            "weight[0,1323]     -0.16      0.97     -0.19     -1.63      1.60   1662.85      1.00\n",
            "weight[0,1324]     -0.31      0.95     -0.32     -2.03      1.19    993.15      1.00\n",
            "weight[0,1325]     -0.24      0.98     -0.22     -1.93      1.29    899.29      1.00\n",
            "weight[0,1326]     -0.35      1.01     -0.36     -1.99      1.27   1251.10      1.00\n",
            "weight[0,1327]     -0.38      0.96     -0.40     -1.92      1.17    719.71      1.00\n",
            "weight[0,1328]     -0.31      1.03     -0.29     -2.23      1.20   1232.19      1.00\n",
            "weight[0,1329]     -0.27      0.98     -0.28     -1.82      1.31   1709.38      1.00\n",
            "weight[0,1330]     -0.32      0.93     -0.30     -1.71      1.25   1133.64      1.00\n",
            "weight[0,1331]     -0.26      0.98     -0.25     -1.81      1.28    982.11      1.00\n",
            "weight[0,1332]     -0.25      0.99     -0.25     -1.72      1.44   2015.50      1.00\n",
            "weight[0,1333]     -0.15      0.97     -0.14     -1.86      1.43   1388.53      1.00\n",
            "weight[0,1334]     -0.16      0.94     -0.21     -1.59      1.48    883.74      1.00\n",
            "weight[0,1335]     -0.10      1.01     -0.08     -1.77      1.48   1041.12      1.00\n",
            "weight[0,1336]     -0.09      0.99     -0.06     -1.75      1.52    873.45      1.00\n",
            "weight[0,1337]      0.04      0.99     -0.01     -1.40      1.88   1433.43      1.00\n",
            "weight[0,1338]      0.02      0.92      0.01     -1.42      1.42   1652.46      1.00\n",
            "weight[0,1339]      0.11      0.96      0.11     -1.52      1.61   1860.32      1.00\n",
            "weight[0,1340]      0.10      0.98      0.08     -1.53      1.63   3142.55      1.00\n",
            "weight[0,1341]      0.10      0.93      0.11     -1.32      1.62   1484.19      1.00\n",
            "weight[0,1342]     -0.02      0.99     -0.02     -1.61      1.60   2035.55      1.00\n",
            "weight[0,1343]     -0.03      0.98     -0.03     -1.52      1.73   1432.19      1.00\n",
            "weight[0,1344]      0.01      1.00      0.01     -1.56      1.67   1210.77      1.00\n",
            "weight[0,1345]     -0.05      0.99     -0.05     -1.81      1.45   1086.74      1.00\n",
            "weight[0,1346]     -0.09      0.91     -0.10     -1.60      1.38    905.08      1.00\n",
            "weight[0,1347]     -0.03      1.06      0.01     -1.89      1.56    618.91      1.00\n",
            "weight[0,1348]      0.12      0.98      0.10     -1.46      1.73    707.76      1.00\n",
            "weight[0,1349]     -0.00      0.99     -0.02     -1.53      1.69   2069.04      1.00\n",
            "weight[0,1350]      0.03      0.99      0.02     -1.66      1.55   1283.77      1.00\n",
            "weight[0,1351]     -0.00      1.01     -0.04     -1.47      1.75   1928.01      1.00\n",
            "weight[0,1352]      0.00      1.05     -0.02     -1.63      1.76   1473.82      1.00\n",
            "weight[0,1353]     -0.02      0.96     -0.05     -1.53      1.66   1038.30      1.00\n",
            "weight[0,1354]     -0.01      0.98     -0.04     -1.57      1.58    779.77      1.00\n",
            "weight[0,1355]     -0.05      1.02     -0.02     -1.74      1.55   1296.66      1.00\n",
            "weight[0,1356]     -0.06      0.92     -0.05     -1.62      1.45   1083.22      1.00\n",
            "weight[0,1357]     -0.14      1.08     -0.11     -1.91      1.62   1205.01      1.00\n",
            "weight[0,1358]     -0.11      0.94     -0.11     -1.60      1.45    950.32      1.00\n",
            "weight[0,1359]     -0.13      1.00     -0.14     -1.83      1.54   2023.73      1.00\n",
            "weight[0,1360]     -0.09      0.94     -0.09     -1.66      1.39   1829.76      1.00\n",
            "weight[0,1361]     -0.17      0.93     -0.12     -1.63      1.32    380.01      1.00\n",
            "weight[0,1362]     -0.19      1.04     -0.22     -2.07      1.35   2804.35      1.00\n",
            "weight[0,1363]     -0.22      0.99     -0.21     -2.01      1.21   1688.16      1.00\n",
            "weight[0,1364]     -0.21      1.00     -0.24     -1.72      1.45   1826.43      1.00\n",
            "weight[0,1365]     -0.13      1.06     -0.10     -1.88      1.60   1333.01      1.00\n",
            "weight[0,1366]     -0.08      0.99     -0.05     -1.68      1.63   1044.86      1.00\n",
            "weight[0,1367]     -0.21      1.03     -0.20     -2.00      1.47    932.49      1.00\n",
            "weight[0,1368]     -0.09      0.96     -0.10     -1.73      1.43   1228.35      1.00\n",
            "weight[0,1369]     -0.10      0.99     -0.09     -1.58      1.63   1157.74      1.00\n",
            "weight[0,1370]      0.08      0.97      0.06     -1.42      1.80   1370.98      1.00\n",
            "weight[0,1371]      0.11      1.04      0.08     -1.55      1.81   1495.17      1.00\n",
            "weight[0,1372]      0.05      0.98      0.03     -1.38      1.82   1011.46      1.00\n",
            "weight[0,1373]      0.06      1.02      0.06     -1.57      1.72   1978.63      1.00\n",
            "weight[0,1374]      0.09      1.00      0.09     -1.59      1.70   1887.95      1.00\n",
            "weight[0,1375]      0.09      0.98      0.04     -1.39      1.81   1035.20      1.00\n",
            "weight[0,1376]      0.12      0.99      0.09     -1.65      1.60   1540.31      1.00\n",
            "weight[0,1377]      0.07      0.94      0.07     -1.43      1.59   1191.64      1.00\n",
            "weight[0,1378]      0.13      0.97      0.18     -1.75      1.48   1401.57      1.00\n",
            "weight[0,1379]      0.16      1.00      0.16     -1.35      1.80   1150.47      1.00\n",
            "weight[0,1380]      0.08      0.96      0.08     -1.32      1.83   1116.56      1.00\n",
            "weight[0,1381]      0.03      0.97     -0.01     -1.52      1.57   1821.72      1.00\n",
            "weight[0,1382]      0.00      0.99      0.00     -1.66      1.59   1055.22      1.00\n",
            "weight[0,1383]      0.02      0.98     -0.01     -1.56      1.63   1385.00      1.00\n",
            "weight[0,1384]      0.03      0.99      0.03     -1.48      1.65   1165.85      1.00\n",
            "weight[0,1385]     -0.03      1.05     -0.03     -1.73      1.68   1731.99      1.00\n",
            "weight[0,1386]     -0.05      1.00     -0.04     -1.60      1.58    583.15      1.00\n",
            "weight[0,1387]     -0.11      0.99     -0.12     -1.84      1.43   1595.11      1.00\n",
            "weight[0,1388]     -0.08      0.99     -0.05     -1.59      1.49   1545.42      1.00\n",
            "weight[0,1389]      0.05      1.01      0.06     -1.52      1.72   1528.71      1.00\n",
            "weight[0,1390]      0.08      1.01      0.07     -1.43      1.82   1379.82      1.00\n",
            "weight[0,1391]      0.00      0.97      0.01     -1.50      1.67   2281.88      1.00\n",
            "weight[0,1392]      0.04      0.99      0.03     -1.55      1.61   2152.48      1.00\n",
            "weight[0,1393]      0.29      0.94      0.32     -1.29      1.82   2022.02      1.00\n",
            "weight[0,1394]      0.47      1.01      0.44     -1.12      2.19    769.93      1.00\n",
            "weight[0,1395]      0.43      0.97      0.42     -1.04      2.13   1050.37      1.00\n",
            "weight[0,1396]      0.24      0.94      0.20     -1.33      1.84   1439.31      1.00\n",
            "weight[0,1397]      0.17      0.93      0.18     -1.37      1.60   1008.23      1.00\n",
            "weight[0,1398]      0.20      0.95      0.19     -1.27      1.79    970.65      1.00\n",
            "weight[0,1399]      0.02      0.94     -0.00     -1.55      1.49    941.40      1.00\n",
            "weight[0,1400]      0.07      0.97      0.11     -1.46      1.64   1629.48      1.00\n",
            "weight[0,1401]      0.12      0.95      0.10     -1.58      1.48    879.91      1.00\n",
            "weight[0,1402]     -0.10      0.93     -0.12     -1.61      1.38   1292.37      1.00\n",
            "weight[0,1403]      0.32      0.96      0.32     -1.29      1.87   1765.63      1.00\n",
            "weight[0,1404]      0.17      0.98      0.22     -1.41      1.79   1085.97      1.00\n",
            "weight[0,1405]      0.17      0.99      0.16     -1.39      1.86   1564.34      1.00\n",
            "weight[0,1406]      0.17      0.96      0.17     -1.23      1.82    876.69      1.00\n",
            "weight[0,1407]      0.12      1.00      0.09     -1.52      1.74   1819.40      1.00\n",
            "weight[0,1408]      0.23      0.95      0.20     -1.46      1.70   1642.73      1.00\n",
            "weight[0,1409]      0.35      1.01      0.37     -1.23      2.11   1639.23      1.00\n",
            "weight[0,1410]      0.35      0.98      0.37     -1.35      1.89    511.55      1.00\n",
            "weight[0,1411]      0.37      1.00      0.38     -1.11      2.15   1402.87      1.00\n",
            "weight[0,1412]      0.30      0.98      0.33     -1.30      1.95    882.54      1.00\n",
            "weight[0,1413]      0.10      0.98      0.10     -1.63      1.57   1420.94      1.00\n",
            "weight[0,1414]      0.14      0.94      0.13     -1.31      1.74   1084.52      1.00\n",
            "weight[0,1415]      0.23      0.97      0.25     -1.35      1.80   1017.96      1.00\n",
            "weight[0,1416]      0.19      0.97      0.19     -1.42      1.67    844.87      1.00\n",
            "weight[0,1417]      0.20      0.97      0.16     -1.29      1.84   2761.83      1.00\n",
            "weight[0,1418]      0.12      0.99      0.10     -1.65      1.59    939.57      1.00\n",
            "weight[0,1419]      0.07      1.07      0.09     -1.63      1.79   1133.95      1.00\n",
            "weight[0,1420]      0.14      0.98      0.14     -1.35      1.85   1908.45      1.00\n",
            "weight[0,1421]      0.23      0.93      0.22     -1.19      1.74   2362.01      1.00\n",
            "weight[0,1422]      0.13      1.01      0.15     -1.43      1.85    917.90      1.00\n",
            "weight[0,1423]      0.10      0.96      0.08     -1.60      1.52   1117.33      1.00\n",
            "weight[0,1424]      0.12      0.99      0.13     -1.51      1.64   1311.76      1.00\n",
            "weight[0,1425]      0.09      0.94      0.06     -1.37      1.64    770.53      1.00\n",
            "weight[0,1426]      0.13      1.01      0.18     -1.47      1.87    589.81      1.01\n",
            "weight[0,1427]      0.34      0.92      0.38     -1.08      1.93    939.04      1.00\n",
            "weight[0,1428]      0.09      1.01      0.09     -1.67      1.62    983.49      1.00\n",
            "weight[0,1429]      0.03      0.99      0.03     -1.59      1.59   2409.35      1.00\n",
            "weight[0,1430]     -0.02      1.00     -0.04     -1.73      1.55    894.20      1.00\n",
            "weight[0,1431]     -0.05      0.99     -0.05     -1.66      1.46   2185.41      1.00\n",
            "weight[0,1432]     -0.06      0.99     -0.04     -1.87      1.33   1339.75      1.00\n",
            "weight[0,1433]     -0.08      0.97     -0.07     -1.67      1.47   2126.56      1.00\n",
            "weight[0,1434]      0.09      0.91      0.09     -1.51      1.46   1679.17      1.00\n",
            "weight[0,1435]      0.19      0.96      0.19     -1.25      1.91   1181.47      1.00\n",
            "weight[0,1436]      0.26      1.02      0.31     -1.30      2.05   1451.60      1.00\n",
            "weight[0,1437]      0.17      1.02      0.17     -1.35      1.93   1811.11      1.00\n",
            "weight[0,1438]      0.24      0.95      0.22     -1.33      1.78   1887.18      1.00\n",
            "weight[0,1439]      0.23      0.98      0.29     -1.24      1.93   1494.34      1.00\n",
            "weight[0,1440]      0.13      1.02      0.10     -1.60      1.69   1346.77      1.00\n",
            "weight[0,1441]      0.00      0.96     -0.01     -1.48      1.56   1913.90      1.00\n",
            "weight[0,1442]      0.01      1.01     -0.03     -1.62      1.70   1186.78      1.00\n",
            "weight[0,1443]      0.04      0.92      0.06     -1.46      1.50   1504.88      1.00\n",
            "weight[0,1444]     -0.07      0.93     -0.05     -1.71      1.36   1142.66      1.00\n",
            "weight[0,1445]     -0.11      0.97     -0.11     -1.71      1.46   1391.15      1.00\n",
            "weight[0,1446]     -0.09      0.96     -0.13     -1.71      1.41    646.45      1.00\n",
            "weight[0,1447]     -0.09      1.01     -0.10     -1.85      1.40   1858.08      1.00\n",
            "weight[0,1448]     -0.10      1.00     -0.11     -1.72      1.54    986.50      1.00\n",
            "weight[0,1449]      0.07      1.04      0.08     -1.64      1.73   1164.87      1.00\n",
            "weight[0,1450]     -0.01      0.97     -0.04     -1.59      1.56    864.49      1.00\n",
            "weight[0,1451]     -0.01      1.03     -0.02     -1.85      1.50   1491.42      1.00\n",
            "weight[0,1452]      0.00      1.00      0.03     -1.48      1.76   1448.45      1.00\n",
            "weight[0,1453]     -0.16      0.95     -0.15     -1.61      1.51    787.42      1.00\n",
            "weight[0,1454]     -0.18      1.01     -0.16     -1.73      1.50   1193.72      1.00\n",
            "weight[0,1455]     -0.14      0.96     -0.15     -1.68      1.42    682.15      1.00\n",
            "weight[0,1456]     -0.08      0.91     -0.09     -1.47      1.45   2176.76      1.00\n",
            "weight[0,1457]     -0.10      0.96     -0.09     -1.54      1.57   1474.51      1.00\n",
            "weight[0,1458]     -0.15      0.98     -0.17     -1.79      1.37    997.74      1.00\n",
            "weight[0,1459]     -0.11      0.98     -0.14     -1.75      1.48    945.66      1.00\n",
            "weight[0,1460]     -0.07      0.97     -0.03     -1.60      1.44   1100.39      1.00\n",
            "weight[0,1461]     -0.02      0.93     -0.06     -1.56      1.43    964.25      1.01\n",
            "weight[0,1462]      0.00      1.02      0.00     -1.78      1.59   2555.01      1.00\n",
            "weight[0,1463]      0.03      0.95      0.01     -1.51      1.62   1416.31      1.00\n",
            "weight[0,1464]      0.04      0.93      0.03     -1.54      1.46    688.14      1.00\n",
            "weight[0,1465]      0.04      0.98      0.01     -1.38      1.77   2567.99      1.00\n",
            "weight[0,1466]     -0.03      1.04     -0.06     -1.64      1.66   1088.31      1.00\n",
            "weight[0,1467]     -0.04      1.01     -0.01     -1.82      1.54   1445.41      1.00\n",
            "weight[0,1468]      0.01      1.07      0.00     -1.71      1.74   1619.01      1.00\n",
            "weight[0,1469]      0.08      0.98      0.08     -1.46      1.64    679.54      1.00\n",
            "weight[0,1470]     -0.04      0.97     -0.04     -1.39      1.75   2055.24      1.00\n",
            "weight[0,1471]     -0.10      1.04     -0.09     -1.80      1.54   1824.15      1.00\n",
            "weight[0,1472]      0.10      0.89      0.10     -1.35      1.56   1449.03      1.00\n",
            "weight[0,1473]     -0.03      1.04     -0.04     -1.74      1.68    847.45      1.00\n",
            "weight[0,1474]     -0.06      1.03     -0.09     -1.70      1.65   1543.45      1.00\n",
            "weight[0,1475]      0.29      1.05      0.28     -1.36      2.01   1159.65      1.00\n",
            "weight[0,1476]     -0.02      0.91     -0.03     -1.73      1.37   1847.03      1.00\n",
            "weight[0,1477]     -0.01      1.03     -0.04     -1.77      1.73   2057.42      1.00\n",
            "weight[0,1478]     -0.02      0.97     -0.01     -1.49      1.69    778.26      1.00\n",
            "weight[0,1479]     -0.10      1.05     -0.09     -1.67      1.61   1703.77      1.00\n",
            "weight[0,1480]     -0.01      0.95     -0.03     -1.69      1.41   1081.76      1.00\n",
            "weight[0,1481]      0.03      1.00     -0.04     -1.75      1.49   1742.45      1.00\n",
            "weight[0,1482]     -0.17      1.00     -0.15     -1.92      1.40    875.41      1.00\n",
            "weight[0,1483]      0.08      0.97      0.08     -1.55      1.61   1105.87      1.00\n",
            "weight[0,1484]      0.07      1.04      0.04     -1.61      1.87   1021.01      1.00\n",
            "weight[0,1485]     -0.05      0.95     -0.06     -1.64      1.38   2465.36      1.00\n",
            "weight[0,1486]     -0.02      0.98      0.00     -1.68      1.48    830.69      1.00\n",
            "weight[0,1487]     -0.05      0.98     -0.05     -1.78      1.48   1131.78      1.00\n",
            "weight[0,1488]     -0.05      0.92     -0.06     -1.54      1.41   1857.84      1.00\n",
            "weight[0,1489]      0.03      0.97      0.03     -1.43      1.77    623.39      1.00\n",
            "weight[0,1490]      0.02      0.98      0.00     -1.62      1.58   1745.32      1.00\n",
            "weight[0,1491]     -0.01      0.98     -0.03     -1.47      1.69   2040.38      1.00\n",
            "weight[0,1492]     -0.02      1.01     -0.01     -1.65      1.56    903.65      1.00\n",
            "weight[0,1493]      0.11      0.98      0.09     -1.59      1.63   2398.66      1.00\n",
            "weight[0,1494]     -0.03      0.95     -0.00     -1.52      1.54   1869.29      1.00\n",
            "weight[0,1495]     -0.24      0.99     -0.26     -1.98      1.20    903.94      1.00\n",
            "weight[0,1496]      0.05      0.98      0.09     -1.42      1.84   1819.01      1.00\n",
            "weight[0,1497]     -0.16      1.01     -0.13     -1.96      1.37    936.20      1.00\n",
            "weight[0,1498]     -0.17      1.00     -0.15     -1.65      1.54   1895.01      1.00\n",
            "weight[0,1499]      0.07      0.94      0.08     -1.38      1.67   1092.82      1.00\n",
            "weight[0,1500]     -0.04      1.03     -0.01     -1.74      1.59   1714.57      1.00\n",
            "weight[0,1501]     -0.11      0.99     -0.12     -2.10      1.26    989.48      1.00\n",
            "weight[0,1502]     -0.06      0.98     -0.08     -1.66      1.42   2697.58      1.00\n",
            "weight[0,1503]      0.03      1.00      0.05     -1.74      1.57   1660.84      1.00\n",
            "weight[0,1504]      0.06      0.95      0.06     -1.28      1.70   2436.69      1.00\n",
            "weight[0,1505]      0.03      0.99      0.06     -1.47      1.83   1580.46      1.00\n",
            "weight[0,1506]     -0.03      1.02      0.01     -1.62      1.67   1791.99      1.00\n",
            "weight[0,1507]      0.05      0.95      0.10     -1.56      1.60   1690.79      1.00\n",
            "weight[0,1508]      0.00      0.97     -0.02     -1.65      1.57   1606.92      1.00\n",
            "weight[0,1509]      0.03      0.98      0.04     -1.55      1.69    984.98      1.00\n",
            "weight[0,1510]     -0.02      0.99     -0.03     -1.57      1.60   1063.60      1.00\n",
            "weight[0,1511]      0.03      1.00      0.03     -1.53      1.72    544.07      1.00\n",
            "weight[0,1512]     -0.04      1.00     -0.05     -1.60      1.53   1463.10      1.00\n",
            "weight[0,1513]      0.24      0.95      0.23     -1.25      1.86    647.45      1.00\n",
            "weight[0,1514]      0.01      1.03      0.02     -1.77      1.60   2001.36      1.00\n",
            "weight[0,1515]     -0.09      0.99     -0.09     -1.67      1.60    933.79      1.00\n",
            "weight[0,1516]      0.14      1.05      0.14     -1.50      1.84   2992.14      1.00\n",
            "weight[0,1517]      0.15      1.00      0.14     -1.43      1.81   1206.52      1.00\n",
            "weight[0,1518]      0.06      0.97      0.09     -1.51      1.60    714.38      1.00\n",
            "weight[0,1519]     -0.05      0.97     -0.08     -1.55      1.56   1005.55      1.00\n",
            "weight[0,1520]      0.11      1.10      0.10     -1.65      1.96   2334.21      1.00\n",
            "weight[0,1521]      0.04      1.01      0.05     -1.71      1.63    957.81      1.00\n",
            "weight[0,1522]      0.06      0.98      0.06     -1.45      1.69   1025.19      1.00\n",
            "weight[0,1523]      0.03      1.02      0.04     -1.63      1.66    762.21      1.00\n",
            "weight[0,1524]     -0.05      0.99     -0.02     -1.56      1.65   1003.73      1.00\n",
            "weight[0,1525]     -0.06      1.00     -0.08     -1.71      1.59   1031.92      1.00\n",
            "weight[0,1526]     -0.08      1.06     -0.07     -1.90      1.58   1497.91      1.00\n",
            "weight[0,1527]     -0.17      1.01     -0.15     -1.81      1.49    678.46      1.00\n",
            "weight[0,1528]     -0.09      0.95     -0.08     -1.57      1.58   2206.25      1.00\n",
            "weight[0,1529]     -0.04      1.02     -0.06     -1.80      1.54   1585.69      1.00\n",
            "weight[0,1530]     -0.09      0.99     -0.11     -1.66      1.50   1911.89      1.00\n",
            "weight[0,1531]     -0.04      0.98     -0.01     -1.61      1.60    940.54      1.01\n",
            "weight[0,1532]     -0.07      0.97     -0.04     -1.69      1.37   1248.77      1.00\n",
            "weight[0,1533]      0.04      1.00     -0.02     -1.63      1.61   1478.16      1.00\n",
            "weight[0,1534]      0.11      1.00      0.08     -1.60      1.61   2208.77      1.00\n",
            "weight[0,1535]     -0.01      1.05      0.01     -1.64      1.66    489.76      1.00\n",
            "weight[0,1536]     -0.10      0.97     -0.12     -1.67      1.47   1840.03      1.00\n",
            "weight[0,1537]     -0.13      1.04     -0.13     -1.80      1.67   1555.86      1.00\n",
            "weight[0,1538]     -0.11      1.01     -0.09     -1.67      1.44    964.88      1.00\n",
            "weight[0,1539]     -0.06      0.96     -0.06     -1.70      1.41   2136.56      1.00\n",
            "weight[0,1540]     -0.04      0.97     -0.02     -1.53      1.58   1557.76      1.00\n",
            "weight[0,1541]      0.14      1.01      0.13     -1.49      1.78   2079.70      1.00\n",
            "weight[0,1542]     -0.14      1.00     -0.18     -1.80      1.46    686.98      1.00\n",
            "weight[0,1543]      0.14      1.02      0.13     -1.46      1.86   1487.70      1.00\n",
            "weight[0,1544]     -0.05      1.03     -0.05     -1.66      1.69    857.03      1.00\n",
            "weight[0,1545]      0.04      0.96      0.03     -1.56      1.59   1699.17      1.00\n",
            "weight[0,1546]      0.11      1.05      0.11     -1.59      1.96   2464.73      1.00\n",
            "weight[0,1547]      0.00      0.93      0.01     -1.36      1.62   2355.31      1.00\n",
            "weight[0,1548]     -0.04      1.00     -0.06     -1.76      1.58   1558.95      1.00\n",
            "weight[0,1549]      0.04      0.93      0.05     -1.44      1.60    959.00      1.00\n",
            "weight[0,1550]     -0.06      0.97     -0.09     -1.59      1.67   1159.22      1.00\n",
            "weight[0,1551]      0.06      0.93      0.07     -1.40      1.63   2561.43      1.00\n",
            "weight[0,1552]     -0.01      1.01     -0.02     -1.69      1.55    913.33      1.00\n",
            "weight[0,1553]     -0.03      0.96     -0.02     -1.53      1.60   1776.87      1.00\n",
            "weight[0,1554]     -0.04      1.02     -0.02     -1.60      1.68   1624.97      1.00\n",
            "weight[0,1555]     -0.03      0.98     -0.05     -1.45      1.68   1194.95      1.00\n",
            "weight[0,1556]     -0.00      1.04     -0.00     -1.65      1.70   1089.83      1.00\n",
            "weight[0,1557]     -0.12      1.02     -0.07     -1.72      1.66   1005.06      1.00\n",
            "weight[0,1558]      0.16      0.97      0.13     -1.23      1.90    550.77      1.00\n",
            "weight[0,1559]     -0.03      0.96     -0.06     -1.51      1.59   1455.02      1.00\n",
            "weight[0,1560]      0.08      1.01      0.07     -1.73      1.59   1206.73      1.00\n",
            "weight[0,1561]     -0.06      0.96     -0.05     -1.53      1.68   1026.51      1.00\n",
            "weight[0,1562]      0.03      0.96      0.01     -1.49      1.67    837.51      1.00\n",
            "weight[0,1563]     -0.00      1.04     -0.01     -1.58      1.74    651.09      1.00\n",
            "weight[0,1564]     -0.07      1.00     -0.08     -1.68      1.53   1151.83      1.00\n",
            "weight[0,1565]     -0.02      1.01     -0.04     -1.55      1.68    786.56      1.00\n",
            "weight[0,1566]     -0.08      0.93     -0.09     -1.63      1.39   1780.23      1.00\n",
            "weight[0,1567]      0.12      0.98      0.09     -1.47      1.77   1975.03      1.00\n",
            "weight[0,1568]     -0.12      1.00     -0.12     -1.84      1.45   1193.45      1.00\n",
            "weight[0,1569]      0.07      1.02      0.06     -1.65      1.60   1064.24      1.00\n",
            "weight[0,1570]      0.07      1.01      0.08     -1.59      1.83    839.43      1.00\n",
            "weight[0,1571]     -0.11      1.06     -0.12     -1.68      1.74    704.93      1.00\n",
            "weight[0,1572]     -0.09      0.98     -0.10     -1.73      1.44   1839.77      1.00\n",
            "weight[0,1573]     -0.07      0.98     -0.07     -1.73      1.58    807.10      1.00\n",
            "weight[0,1574]     -0.16      0.97     -0.15     -1.79      1.36   1484.21      1.00\n",
            "weight[0,1575]     -0.04      0.98     -0.05     -1.49      1.74   2119.84      1.00\n",
            "weight[0,1576]     -0.04      1.05     -0.07     -1.75      1.71   1607.07      1.00\n",
            "weight[0,1577]     -0.09      1.03     -0.07     -1.71      1.70    728.29      1.00\n",
            "weight[0,1578]      0.08      1.01      0.06     -1.60      1.67   1147.71      1.00\n",
            "weight[0,1579]      0.02      1.00      0.02     -1.52      1.70   1903.01      1.00\n",
            "weight[0,1580]     -0.05      1.05     -0.03     -1.94      1.51   2264.66      1.00\n",
            "weight[0,1581]      0.14      0.98      0.14     -1.66      1.50   1017.07      1.00\n",
            "weight[0,1582]     -0.13      1.02     -0.15     -1.73      1.63   1532.84      1.00\n",
            "weight[0,1583]     -0.13      0.97     -0.16     -1.75      1.38    457.77      1.00\n",
            "weight[0,1584]      0.16      0.96      0.18     -1.35      1.70   1046.26      1.00\n",
            "weight[0,1585]      0.01      1.00     -0.00     -1.49      1.74   1225.32      1.00\n",
            "weight[0,1586]     -0.03      0.99     -0.04     -1.51      1.83   1463.54      1.00\n",
            "weight[0,1587]      0.09      0.97      0.11     -1.54      1.62   1025.31      1.00\n",
            "weight[0,1588]      0.00      0.93     -0.04     -1.48      1.42   1586.93      1.00\n",
            "weight[0,1589]     -0.00      0.99     -0.04     -1.44      1.74    657.59      1.00\n",
            "weight[0,1590]      0.05      0.96      0.09     -1.49      1.61    887.79      1.00\n",
            "weight[0,1591]      0.05      1.03      0.03     -1.55      1.83   1226.80      1.00\n",
            "weight[0,1592]     -0.04      1.01      0.00     -1.81      1.50    960.86      1.00\n",
            "weight[0,1593]     -0.01      1.00     -0.02     -1.54      1.66    997.45      1.00\n",
            "weight[0,1594]     -0.04      0.97     -0.03     -1.54      1.66   1884.68      1.00\n",
            "weight[0,1595]      0.02      1.03      0.03     -1.64      1.56   1067.12      1.00\n",
            "weight[0,1596]      0.09      1.04      0.11     -1.47      1.83   1960.35      1.00\n",
            "weight[0,1597]     -0.05      0.99     -0.07     -1.52      1.72   1973.76      1.00\n",
            "weight[0,1598]     -0.09      0.97     -0.09     -1.66      1.48   1033.30      1.00\n",
            "weight[0,1599]      0.05      0.97      0.06     -1.55      1.55    998.57      1.00\n",
            "weight[0,1600]     -0.00      1.00      0.03     -1.56      1.77   1665.87      1.00\n",
            "weight[0,1601]      0.00      0.98      0.01     -1.60      1.56   2130.38      1.00\n",
            "weight[0,1602]      0.07      1.00      0.05     -1.62      1.65   1464.52      1.00\n",
            "weight[0,1603]     -0.02      0.95     -0.04     -1.54      1.44   2537.94      1.00\n",
            "weight[0,1604]     -0.04      1.06     -0.04     -1.85      1.51   2396.13      1.00\n",
            "weight[0,1605]     -0.06      0.95     -0.05     -1.58      1.53   1212.36      1.00\n",
            "weight[0,1606]     -0.07      0.99     -0.10     -1.65      1.56   1244.49      1.00\n",
            "weight[0,1607]     -0.04      1.00     -0.02     -1.57      1.61    814.62      1.00\n",
            "weight[0,1608]      0.09      0.96      0.08     -1.37      1.72    775.51      1.00\n",
            "weight[0,1609]      0.10      0.99      0.09     -1.64      1.64   1006.61      1.00\n",
            "weight[0,1610]     -0.04      1.02     -0.05     -1.79      1.49    961.95      1.00\n",
            "weight[0,1611]      0.18      1.04      0.24     -1.45      1.94   1310.58      1.00\n",
            "weight[0,1612]      0.02      0.96     -0.02     -1.53      1.70   1017.67      1.00\n",
            "weight[0,1613]      0.14      1.01      0.15     -1.48      1.86   1336.89      1.00\n",
            "weight[0,1614]     -0.06      0.97     -0.07     -1.52      1.64   1076.63      1.00\n",
            "weight[0,1615]      0.09      0.98      0.13     -1.62      1.55   1566.50      1.00\n",
            "weight[0,1616]     -0.03      1.03     -0.00     -1.66      1.71    748.19      1.00\n",
            "weight[0,1617]     -0.07      0.97     -0.04     -1.64      1.46   1712.08      1.00\n",
            "weight[0,1618]     -0.08      0.92     -0.05     -1.67      1.32   1535.98      1.00\n",
            "weight[0,1619]     -0.02      1.00     -0.02     -1.65      1.65   1318.06      1.00\n",
            "weight[0,1620]     -0.03      0.99     -0.07     -1.51      1.59   1089.70      1.00\n",
            "weight[0,1621]     -0.03      1.00     -0.07     -1.70      1.59   1142.43      1.00\n",
            "weight[0,1622]     -0.13      0.99     -0.14     -1.80      1.45   1348.61      1.00\n",
            "weight[0,1623]      0.01      0.98      0.04     -1.71      1.54   1769.50      1.00\n",
            "weight[0,1624]     -0.02      0.98     -0.03     -1.65      1.56    629.57      1.00\n",
            "weight[0,1625]     -0.07      1.02     -0.11     -1.82      1.48   2571.29      1.00\n",
            "weight[0,1626]      0.01      0.97     -0.02     -1.58      1.54    759.48      1.00\n",
            "weight[0,1627]     -0.00      1.00      0.00     -1.53      1.67   1365.82      1.00\n",
            "weight[0,1628]     -0.05      1.01     -0.03     -1.75      1.49   1930.07      1.00\n",
            "weight[0,1629]      0.06      1.00      0.07     -1.49      1.80   1006.13      1.00\n",
            "weight[0,1630]     -0.03      1.02     -0.01     -1.62      1.74   1806.90      1.00\n",
            "weight[0,1631]     -0.11      0.96     -0.09     -1.65      1.49   1384.22      1.00\n",
            "weight[0,1632]     -0.01      0.97     -0.01     -1.76      1.46   1388.43      1.00\n",
            "weight[0,1633]     -0.03      1.02     -0.07     -1.69      1.55   1245.31      1.00\n",
            "weight[0,1634]     -0.05      0.99     -0.02     -1.65      1.63   2173.07      1.00\n",
            "weight[0,1635]     -0.13      1.00     -0.15     -1.70      1.53   1123.56      1.00\n",
            "weight[0,1636]     -0.03      0.93     -0.00     -1.39      1.65    750.74      1.00\n",
            "weight[0,1637]     -0.05      0.99     -0.10     -1.65      1.50    942.31      1.00\n",
            "weight[0,1638]      0.04      1.03      0.04     -1.68      1.74   1465.26      1.00\n",
            "weight[0,1639]     -0.05      1.03     -0.07     -1.62      1.66    952.79      1.00\n",
            "weight[0,1640]     -0.01      1.02      0.01     -1.56      1.70   1599.09      1.00\n",
            "weight[0,1641]      0.09      0.97      0.10     -1.55      1.57    850.15      1.00\n",
            "weight[0,1642]     -0.03      0.97     -0.01     -1.73      1.50   1078.63      1.00\n",
            "weight[0,1643]     -0.08      1.04     -0.09     -1.84      1.56   2690.56      1.00\n",
            "weight[0,1644]     -0.07      1.01     -0.09     -1.71      1.56   1749.48      1.00\n",
            "weight[0,1645]      0.04      0.98      0.05     -1.67      1.49    784.58      1.00\n",
            "weight[0,1646]     -0.01      0.98     -0.06     -1.50      1.63    817.26      1.00\n",
            "weight[0,1647]     -0.04      0.99     -0.06     -1.69      1.57   2002.15      1.00\n",
            "weight[0,1648]     -0.04      1.03     -0.04     -1.73      1.60   1898.30      1.00\n",
            "weight[0,1649]     -0.07      1.01     -0.08     -1.64      1.62   1358.78      1.00\n",
            "weight[0,1650]     -0.08      0.94     -0.08     -1.61      1.48   2264.26      1.00\n",
            "weight[0,1651]     -0.05      1.00     -0.05     -1.63      1.66   1578.55      1.00\n",
            "weight[0,1652]      0.00      0.98     -0.02     -1.47      1.76    811.18      1.00\n",
            "weight[0,1653]     -0.02      1.00     -0.05     -1.55      1.79   1114.14      1.00\n",
            "weight[0,1654]     -0.08      1.03     -0.06     -1.76      1.59   1193.75      1.00\n",
            "weight[0,1655]      0.02      0.99      0.02     -1.48      1.73    860.77      1.00\n",
            "weight[0,1656]     -0.09      1.02     -0.09     -2.02      1.47   1758.44      1.00\n",
            "weight[0,1657]     -0.11      1.03     -0.12     -1.95      1.45   2295.44      1.00\n",
            "weight[0,1658]      0.15      0.97      0.15     -1.24      2.02   1610.98      1.00\n",
            "weight[0,1659]     -0.07      0.94     -0.04     -1.58      1.42   1442.53      1.00\n",
            "weight[0,1660]     -0.07      0.99     -0.05     -1.97      1.34    973.52      1.00\n",
            "weight[0,1661]     -0.09      1.01     -0.09     -1.58      1.65    654.28      1.00\n",
            "weight[0,1662]     -0.07      1.02     -0.06     -1.63      1.76   1168.53      1.00\n",
            "weight[0,1663]      0.00      1.03     -0.03     -1.62      1.68   1515.70      1.00\n",
            "weight[0,1664]     -0.01      1.01     -0.00     -1.59      1.60   1363.67      1.00\n",
            "weight[0,1665]     -0.05      1.03     -0.06     -1.72      1.60    781.02      1.00\n",
            "weight[0,1666]      0.03      0.96      0.02     -1.61      1.48   1818.57      1.00\n",
            "weight[0,1667]     -0.03      0.97     -0.03     -1.66      1.48   1152.64      1.00\n",
            "weight[0,1668]      0.10      1.00      0.14     -1.52      1.73   1197.64      1.00\n",
            "weight[0,1669]      0.20      0.99      0.22     -1.40      1.80   1405.08      1.00\n",
            "weight[0,1670]     -0.04      1.01     -0.04     -1.89      1.46   1213.15      1.00\n",
            "weight[0,1671]      0.00      1.00      0.02     -1.56      1.75   1480.31      1.00\n",
            "weight[0,1672]     -0.11      1.02     -0.10     -1.67      1.69   1363.93      1.00\n",
            "weight[0,1673]      0.08      1.00      0.08     -1.53      1.75   1009.94      1.00\n",
            "weight[0,1674]      0.18      1.03      0.17     -1.45      1.85    822.15      1.00\n",
            "weight[0,1675]      0.00      1.02     -0.02     -1.55      1.73   1566.70      1.00\n",
            "weight[0,1676]     -0.04      1.00     -0.01     -1.64      1.58    918.56      1.00\n",
            "weight[0,1677]     -0.08      0.98     -0.07     -1.69      1.54   1765.44      1.00\n",
            "weight[0,1678]      0.16      0.94      0.11     -1.22      1.76   1234.96      1.00\n",
            "weight[0,1679]     -0.03      1.02     -0.05     -1.65      1.72   1408.20      1.00\n",
            "weight[0,1680]     -0.01      0.99     -0.01     -1.60      1.63   1092.95      1.00\n",
            "weight[0,1681]      0.00      1.03      0.04     -1.58      1.76   2045.35      1.00\n",
            "weight[0,1682]      0.03      1.04      0.00     -1.69      1.71   2112.41      1.00\n",
            "weight[0,1683]     -0.03      1.08     -0.06     -1.78      1.71   1360.87      1.00\n",
            "weight[0,1684]      0.08      0.97      0.07     -1.29      1.80   1431.05      1.00\n",
            "weight[0,1685]      0.04      0.98      0.04     -1.55      1.67   2567.68      1.00\n",
            "weight[0,1686]      0.03      1.00     -0.00     -1.52      1.74    675.70      1.00\n",
            "weight[0,1687]     -0.01      0.99     -0.01     -1.61      1.54   1189.04      1.00\n",
            "weight[0,1688]      0.03      1.07     -0.01     -1.74      1.69   1363.42      1.00\n",
            "weight[0,1689]      0.09      1.00      0.11     -1.59      1.59    879.15      1.01\n",
            "weight[0,1690]     -0.06      1.03     -0.05     -1.67      1.63   1238.86      1.00\n",
            "weight[0,1691]      0.00      1.01     -0.02     -1.51      1.73   1329.30      1.00\n",
            "weight[0,1692]      0.03      1.03     -0.01     -1.57      1.64   1527.64      1.00\n",
            "weight[0,1693]     -0.02      0.95     -0.04     -1.52      1.57   1232.21      1.00\n",
            "weight[0,1694]      0.02      1.01      0.01     -1.72      1.48   1954.32      1.00\n",
            "weight[0,1695]     -0.11      0.97     -0.10     -1.58      1.53    905.85      1.00\n",
            "weight[0,1696]      0.03      0.98      0.09     -1.70      1.56   1118.79      1.00\n",
            "weight[0,1697]      0.06      1.02      0.03     -1.60      1.70   1941.07      1.00\n",
            "weight[0,1698]      0.17      1.04      0.14     -1.59      1.73   1477.63      1.00\n",
            "weight[0,1699]     -0.07      1.02     -0.05     -1.61      1.73   1591.61      1.00\n",
            "weight[0,1700]      0.00      1.00     -0.04     -1.68      1.58    809.61      1.00\n",
            "weight[0,1701]     -0.02      0.99     -0.00     -1.65      1.57   1608.58      1.00\n",
            "weight[0,1702]      0.13      0.97      0.15     -1.59      1.61   1089.06      1.00\n",
            "weight[0,1703]      0.11      0.95      0.09     -1.53      1.63   1257.40      1.00\n",
            "weight[0,1704]      0.08      0.99      0.12     -1.40      1.82    910.63      1.00\n",
            "weight[0,1705]     -0.08      0.99     -0.05     -1.73      1.50   1653.06      1.00\n",
            "weight[0,1706]      0.02      1.00      0.03     -1.82      1.58   1475.94      1.00\n",
            "weight[0,1707]      0.09      1.08      0.10     -1.68      1.81   1279.41      1.00\n",
            "weight[0,1708]      0.03      1.00      0.06     -1.54      1.65    571.03      1.00\n",
            "weight[0,1709]      0.06      0.95      0.05     -1.25      1.99   1589.08      1.00\n",
            "weight[0,1710]     -0.16      1.01     -0.17     -1.88      1.48   1654.59      1.00\n",
            "weight[0,1711]      0.10      1.00      0.09     -1.41      1.77   1583.47      1.00\n",
            "weight[0,1712]     -0.04      1.03     -0.01     -1.82      1.55   1269.82      1.00\n",
            "weight[0,1713]     -0.02      1.01     -0.03     -1.80      1.50    845.23      1.00\n",
            "weight[0,1714]      0.04      1.04      0.02     -1.62      1.76    639.32      1.00\n",
            "weight[0,1715]     -0.18      0.98     -0.18     -1.78      1.46   1359.58      1.00\n",
            "weight[0,1716]      0.37      1.01      0.33     -1.26      1.93    890.66      1.00\n",
            "weight[0,1717]     -0.07      0.99     -0.09     -1.54      1.63   2308.27      1.00\n",
            "weight[0,1718]     -0.17      0.98     -0.19     -1.92      1.24   1994.67      1.00\n",
            "weight[0,1719]      0.13      0.99      0.14     -1.46      1.81   3933.02      1.00\n",
            "weight[0,1720]     -0.03      1.03     -0.01     -1.76      1.65    969.74      1.00\n",
            "weight[0,1721]     -0.08      0.96     -0.04     -1.63      1.49   1295.71      1.00\n",
            "weight[0,1722]      0.08      0.99      0.05     -1.53      1.69   2632.87      1.00\n",
            "weight[0,1723]     -0.03      0.93     -0.02     -1.63      1.40   1423.37      1.00\n",
            "weight[0,1724]     -0.04      0.97     -0.01     -1.37      1.84   2365.56      1.00\n",
            "weight[0,1725]     -0.05      0.99     -0.08     -1.76      1.44   1164.06      1.00\n",
            "weight[0,1726]      0.02      0.99      0.03     -1.64      1.57    756.52      1.00\n",
            "weight[0,1727]      0.34      0.95      0.31     -1.11      1.98   1731.07      1.00\n",
            "weight[0,1728]      0.26      0.94      0.27     -1.25      1.82   1962.10      1.00\n",
            "weight[0,1729]     -0.30      1.01     -0.30     -1.88      1.40   1282.61      1.00\n",
            "weight[0,1730]      0.06      0.98      0.09     -1.56      1.72   1598.68      1.00\n",
            "weight[0,1731]      0.18      0.99      0.18     -1.39      1.84    624.65      1.01\n",
            "weight[0,1732]     -0.07      0.96     -0.06     -1.63      1.47   1946.41      1.00\n",
            "weight[0,1733]      0.09      0.94      0.08     -1.39      1.63    858.93      1.00\n",
            "weight[0,1734]     -0.00      0.99      0.03     -1.61      1.64   1071.37      1.00\n",
            "weight[0,1735]      0.02      0.93      0.01     -1.59      1.50    765.22      1.00\n",
            "weight[0,1736]      0.19      0.98      0.18     -1.39      1.79   1769.41      1.00\n",
            "weight[0,1737]      0.02      0.94      0.03     -1.61      1.49   1388.48      1.00\n",
            "weight[0,1738]     -0.02      0.99     -0.00     -1.47      1.75    908.05      1.00\n",
            "weight[0,1739]     -0.04      0.94     -0.03     -1.62      1.44   2022.60      1.00\n",
            "weight[0,1740]     -0.05      0.96     -0.07     -1.56      1.59   1353.57      1.00\n",
            "weight[0,1741]     -0.20      1.00     -0.21     -1.76      1.53   1755.09      1.00\n",
            "weight[0,1742]      0.04      1.05      0.03     -1.63      1.78    677.98      1.00\n",
            "weight[0,1743]     -0.18      0.94     -0.21     -1.57      1.57   1149.29      1.00\n",
            "weight[0,1744]      0.03      0.94      0.05     -1.57      1.51   1029.64      1.00\n",
            "weight[0,1745]      0.19      0.94      0.19     -1.29      1.77   1644.62      1.00\n",
            "weight[0,1746]     -0.03      0.95     -0.00     -1.43      1.60    996.40      1.00\n",
            "weight[0,1747]     -0.03      0.95     -0.04     -1.68      1.46   2102.24      1.00\n",
            "weight[0,1748]     -0.27      0.91     -0.28     -1.72      1.20   1958.08      1.00\n",
            "weight[0,1749]     -0.06      0.98     -0.05     -1.67      1.54   1037.62      1.00\n",
            "weight[0,1750]      0.29      1.01      0.27     -1.45      1.87    663.87      1.00\n",
            "weight[0,1751]     -0.00      0.96     -0.01     -1.45      1.70   3070.58      1.00\n",
            "weight[0,1752]      0.11      0.95      0.13     -1.24      1.88   2831.41      1.00\n",
            "weight[0,1753]     -0.31      1.01     -0.33     -2.01      1.35   1863.53      1.00\n",
            "weight[0,1754]     -0.06      0.98     -0.01     -1.47      1.71   2096.67      1.00\n",
            "weight[0,1755]      0.06      1.02      0.05     -1.54      1.77   1204.13      1.00\n",
            "weight[0,1756]     -0.02      0.99     -0.02     -1.49      1.58   1805.19      1.00\n",
            "weight[0,1757]     -0.02      0.99     -0.00     -1.71      1.47    631.81      1.00\n",
            "weight[0,1758]      0.01      0.99      0.04     -1.59      1.60   1296.26      1.00\n",
            "weight[0,1759]      0.17      1.00      0.17     -1.62      1.61   1285.84      1.00\n",
            "weight[0,1760]      0.35      0.98      0.32     -1.39      1.75    800.06      1.00\n",
            "weight[0,1761]     -0.08      1.00     -0.12     -1.49      1.73   1507.79      1.00\n",
            "weight[0,1762]      0.19      0.96      0.18     -1.46      1.66   2604.75      1.00\n",
            "weight[0,1763]      0.14      0.97      0.18     -1.31      1.87   1444.42      1.00\n",
            "weight[0,1764]     -0.26      1.00     -0.29     -1.81      1.45   1065.79      1.00\n",
            "weight[0,1765]     -0.02      1.08     -0.00     -1.78      1.63   1722.33      1.00\n",
            "weight[0,1766]     -0.41      1.00     -0.42     -2.11      1.21    795.13      1.00\n",
            "weight[0,1767]      0.13      0.96      0.16     -1.36      1.79   1286.65      1.00\n",
            "weight[0,1768]     -0.04      1.03     -0.04     -1.84      1.48   1644.90      1.00\n",
            "weight[0,1769]      0.14      1.01      0.16     -1.42      1.86   1876.15      1.00\n",
            "weight[0,1770]     -0.12      0.95     -0.14     -1.69      1.43   1614.63      1.00\n",
            "weight[0,1771]     -0.01      0.99     -0.00     -1.78      1.43   1170.69      1.00\n",
            "weight[0,1772]      0.39      0.98      0.35     -1.28      1.90    936.76      1.00\n",
            "weight[0,1773]     -0.62      0.96     -0.62     -2.15      1.08   1440.54      1.00\n",
            "weight[0,1774]      0.12      1.00      0.13     -1.57      1.77   1028.17      1.00\n",
            "weight[0,1775]      0.11      0.92      0.10     -1.41      1.57    920.40      1.00\n",
            "weight[0,1776]      0.16      0.99      0.16     -1.55      1.69    582.94      1.00\n",
            "weight[0,1777]      0.00      0.96     -0.01     -1.74      1.41   2063.85      1.00\n",
            "weight[0,1778]      0.21      1.06      0.21     -1.37      2.10   1861.88      1.00\n",
            "weight[0,1779]     -0.09      0.97     -0.14     -1.63      1.53   1913.50      1.00\n",
            "weight[0,1780]     -0.11      0.93     -0.07     -1.61      1.47   1096.63      1.00\n",
            "weight[0,1781]     -0.05      0.99     -0.06     -1.75      1.53    994.15      1.00\n",
            "weight[0,1782]      0.71      0.99      0.72     -1.15      2.09    762.68      1.00\n",
            "weight[0,1783]     -0.00      1.00      0.01     -1.63      1.66   1058.32      1.00\n",
            "weight[0,1784]     -0.07      0.98     -0.08     -1.64      1.50    763.77      1.00\n",
            "weight[0,1785]      0.17      0.98      0.18     -1.40      1.75   2140.83      1.00\n",
            "weight[0,1786]     -0.20      1.04     -0.16     -1.97      1.35   2080.05      1.00\n",
            "weight[0,1787]     -0.15      0.93     -0.14     -1.56      1.45   1044.19      1.00\n",
            "weight[0,1788]     -0.20      0.92     -0.20     -1.74      1.29    951.55      1.00\n",
            "weight[0,1789]     -0.03      0.98     -0.02     -1.46      1.64   1761.94      1.00\n",
            "weight[0,1790]      0.09      0.97      0.12     -1.57      1.53   1989.07      1.00\n",
            "weight[0,1791]     -0.21      1.00     -0.23     -1.86      1.41   1713.05      1.00\n",
            "weight[0,1792]     -0.00      0.96     -0.01     -1.55      1.57    772.39      1.00\n",
            "weight[0,1793]     -0.30      1.04     -0.27     -1.82      1.35   1708.83      1.00\n",
            "weight[0,1794]     -0.63      0.97     -0.64     -2.17      1.00    726.32      1.00\n",
            "weight[0,1795]      0.10      0.97      0.09     -1.52      1.57   2494.96      1.00\n",
            "weight[0,1796]     -0.07      1.02     -0.08     -1.75      1.54    993.71      1.00\n",
            "weight[0,1797]      0.33      0.94      0.34     -1.23      1.87   1505.78      1.00\n",
            "weight[0,1798]      0.10      0.97      0.12     -1.34      1.77    618.22      1.01\n",
            "weight[0,1799]      0.05      0.99      0.04     -1.66      1.54   1089.31      1.00\n",
            "weight[0,1800]     -0.10      0.98     -0.07     -1.63      1.50    946.06      1.00\n",
            "weight[0,1801]      0.42      0.98      0.42     -1.15      1.97   2084.55      1.00\n",
            "weight[0,1802]      0.05      0.96      0.01     -1.59      1.54   1506.99      1.00\n",
            "weight[0,1803]     -0.18      0.98     -0.14     -1.75      1.50    880.25      1.00\n",
            "weight[0,1804]     -0.14      0.99     -0.13     -1.80      1.42   1320.40      1.00\n",
            "weight[0,1805]     -0.15      1.04     -0.16     -1.81      1.52   2189.98      1.00\n",
            "weight[0,1806]     -0.12      0.97     -0.11     -1.76      1.41   1950.75      1.00\n",
            "weight[0,1807]     -0.08      1.01     -0.08     -1.63      1.65   1213.33      1.00\n",
            "weight[0,1808]      0.09      1.01      0.10     -1.49      1.80   1472.76      1.00\n",
            "weight[0,1809]     -0.19      0.99     -0.18     -1.86      1.36   1035.20      1.00\n",
            "weight[0,1810]      0.11      0.98      0.09     -1.43      1.78    667.80      1.00\n",
            "weight[0,1811]      0.03      1.04      0.01     -1.69      1.74   2056.26      1.00\n",
            "weight[0,1812]     -0.42      0.92     -0.45     -1.92      1.03   1408.00      1.00\n",
            "weight[0,1813]     -0.14      0.99     -0.14     -1.68      1.50   1281.73      1.00\n",
            "weight[0,1814]      0.06      1.02      0.07     -1.54      1.69    785.72      1.00\n",
            "weight[0,1815]      0.27      0.97      0.26     -1.27      1.90   2049.18      1.00\n",
            "weight[0,1816]      0.07      0.95      0.03     -1.52      1.59   1582.85      1.00\n",
            "weight[0,1817]      0.07      1.00      0.04     -1.55      1.71   1696.49      1.00\n",
            "weight[0,1818]      0.04      0.99      0.02     -1.56      1.66   1622.91      1.00\n",
            "weight[0,1819]      0.19      0.97      0.16     -1.31      1.82    848.69      1.00\n",
            "weight[0,1820]     -0.10      0.99     -0.08     -1.80      1.48   1526.46      1.00\n",
            "weight[0,1821]     -0.02      0.97      0.01     -1.53      1.65   1068.06      1.00\n",
            "weight[0,1822]     -0.19      1.01     -0.19     -1.81      1.46    972.37      1.00\n",
            "weight[0,1823]     -0.01      1.01     -0.02     -1.72      1.53   2202.91      1.00\n",
            "weight[0,1824]      0.02      1.00      0.01     -1.63      1.67   1040.54      1.00\n",
            "weight[0,1825]     -0.27      0.98     -0.29     -1.60      1.60   1326.78      1.00\n",
            "weight[0,1826]      0.06      1.00      0.07     -1.54      1.78   1217.00      1.00\n",
            "weight[0,1827]     -0.08      1.00     -0.05     -1.67      1.56   1811.11      1.00\n",
            "weight[0,1828]     -0.07      1.01     -0.07     -1.56      1.81   1453.26      1.00\n",
            "weight[0,1829]      0.05      1.03      0.07     -1.54      1.79   1037.44      1.00\n",
            "weight[0,1830]      0.26      0.99      0.27     -1.28      1.98    670.26      1.00\n",
            "weight[0,1831]      0.13      0.94      0.12     -1.45      1.63   1039.46      1.00\n",
            "weight[0,1832]     -0.36      0.90     -0.38     -1.97      0.99   1431.26      1.00\n",
            "weight[0,1833]      0.17      1.02      0.18     -1.79      1.59    788.79      1.00\n",
            "weight[0,1834]     -0.19      0.94     -0.18     -1.70      1.35   1800.29      1.00\n",
            "weight[0,1835]     -0.22      1.01     -0.20     -1.75      1.53   1056.34      1.00\n",
            "weight[0,1836]     -0.10      1.01     -0.16     -1.71      1.57   1037.25      1.00\n",
            "weight[0,1837]      0.02      0.99      0.03     -1.67      1.55    977.48      1.00\n",
            "weight[0,1838]      0.13      0.98      0.11     -1.44      1.79   1400.24      1.00\n",
            "weight[0,1839]     -0.48      1.00     -0.48     -2.07      1.13   1106.86      1.00\n",
            "weight[0,1840]     -0.10      1.02     -0.07     -1.95      1.54    544.11      1.00\n",
            "weight[0,1841]      0.06      0.95      0.06     -1.48      1.64   2047.56      1.00\n",
            "weight[0,1842]      0.01      1.00     -0.02     -1.61      1.69   1900.60      1.00\n",
            "weight[0,1843]      0.40      0.93      0.39     -1.10      1.92   2145.82      1.00\n",
            "weight[0,1844]     -0.09      0.99     -0.12     -1.72      1.50   1078.85      1.00\n",
            "weight[0,1845]      0.08      0.91      0.06     -1.35      1.61   2014.02      1.00\n",
            "weight[0,1846]      0.14      1.01      0.12     -1.39      1.96    918.88      1.00\n",
            "weight[0,1847]     -0.27      0.95     -0.28     -1.81      1.23    681.86      1.00\n",
            "weight[0,1848]     -0.15      0.98     -0.17     -1.78      1.37   1518.54      1.00\n",
            "weight[0,1849]      0.01      0.98     -0.04     -1.61      1.57   1004.77      1.00\n",
            "weight[0,1850]     -0.10      1.02     -0.11     -1.75      1.57   1444.03      1.00\n",
            "weight[0,1851]     -0.17      1.07     -0.19     -2.15      1.39    953.11      1.00\n",
            "weight[0,1852]      0.02      0.96      0.01     -1.58      1.57   2047.67      1.00\n",
            "weight[0,1853]     -0.21      0.99     -0.23     -1.97      1.33    669.59      1.01\n",
            "weight[0,1854]      0.25      1.04      0.29     -1.31      2.10   1853.06      1.00\n",
            "weight[0,1855]      0.24      0.99      0.26     -1.36      1.83   1253.29      1.00\n",
            "weight[0,1856]      0.05      0.95      0.08     -1.53      1.48   1509.09      1.00\n",
            "weight[0,1857]      0.19      1.00      0.23     -1.37      1.83    822.26      1.00\n",
            "weight[0,1858]     -0.42      0.91     -0.43     -1.73      1.25    897.41      1.00\n",
            "weight[0,1859]     -0.04      0.98     -0.06     -1.69      1.46   1862.77      1.00\n",
            "weight[0,1860]      0.11      0.97      0.09     -1.45      1.75    729.83      1.00\n",
            "weight[0,1861]      0.34      0.99      0.31     -1.16      2.03    724.45      1.00\n",
            "weight[0,1862]     -0.08      1.01     -0.07     -1.80      1.54    923.24      1.00\n",
            "weight[0,1863]      0.08      1.03      0.09     -1.68      1.67    987.84      1.00\n",
            "weight[0,1864]      0.01      0.90      0.02     -1.50      1.43   1256.59      1.00\n",
            "weight[0,1865]     -0.06      0.95     -0.09     -1.69      1.43   1265.11      1.00\n",
            "weight[0,1866]      0.02      0.97      0.06     -1.61      1.49   2360.69      1.00\n",
            "weight[0,1867]      0.17      0.97      0.20     -1.44      1.66    523.99      1.00\n",
            "weight[0,1868]      0.11      0.97      0.08     -1.64      1.48   1197.43      1.00\n",
            "weight[0,1869]     -0.11      1.02     -0.14     -1.83      1.51    602.35      1.00\n",
            "weight[0,1870]      0.20      1.02      0.22     -1.29      2.01   1473.99      1.00\n",
            "weight[0,1871]     -0.30      0.98     -0.30     -1.93      1.26   2702.52      1.00\n",
            "weight[0,1872]     -0.03      0.95     -0.02     -1.34      1.65    870.67      1.00\n",
            "weight[0,1873]      0.08      0.98      0.08     -1.51      1.70    705.03      1.00\n",
            "weight[0,1874]      0.05      1.02      0.09     -1.48      1.84    534.82      1.00\n",
            "weight[0,1875]      0.32      1.04      0.32     -1.59      1.78   1348.68      1.00\n",
            "weight[0,1876]      0.01      1.01      0.02     -1.78      1.63    934.81      1.00\n",
            "weight[0,1877]     -0.15      0.91     -0.14     -1.55      1.45   1070.17      1.00\n",
            "weight[0,1878]      0.11      1.01      0.10     -1.48      1.93   1228.66      1.00\n",
            "weight[0,1879]      0.14      0.95      0.12     -1.35      1.74    925.75      1.00\n",
            "weight[0,1880]     -0.22      1.03     -0.26     -1.84      1.50   1335.05      1.00\n",
            "weight[0,1881]      0.34      0.98      0.33     -1.43      1.83   1589.40      1.00\n",
            "weight[0,1882]     -0.20      0.97     -0.21     -1.89      1.34    614.41      1.00\n",
            "weight[0,1883]      0.19      1.04      0.16     -1.40      1.97    958.26      1.00\n",
            "weight[0,1884]      0.15      1.06      0.12     -1.59      1.81   1368.73      1.00\n",
            "weight[0,1885]     -0.09      0.98     -0.13     -1.67      1.57    695.75      1.01\n",
            "weight[0,1886]     -0.00      1.02      0.01     -1.44      1.86   1397.83      1.00\n",
            "weight[0,1887]     -0.07      0.99     -0.09     -1.68      1.56   1138.46      1.00\n",
            "weight[0,1888]      0.12      1.01      0.13     -1.60      1.70   1101.49      1.00\n",
            "weight[0,1889]      0.19      0.96      0.17     -1.29      1.81   2425.03      1.00\n",
            "weight[0,1890]      0.07      1.08      0.04     -1.49      2.00    956.76      1.00\n",
            "weight[0,1891]      0.17      1.06      0.17     -1.73      1.75    962.61      1.00\n",
            "weight[0,1892]     -0.25      0.96     -0.28     -1.72      1.41   1443.27      1.00\n",
            "weight[0,1893]      0.09      1.02      0.17     -1.65      1.73    997.97      1.00\n",
            "weight[0,1894]      0.01      0.98      0.02     -1.49      1.70   1301.44      1.00\n",
            "weight[0,1895]      0.20      1.07      0.19     -1.57      1.84   1189.02      1.00\n",
            "weight[0,1896]     -0.42      0.96     -0.42     -1.92      1.27   1691.97      1.00\n",
            "weight[0,1897]     -0.06      1.04     -0.04     -1.74      1.58   1127.33      1.00\n",
            "weight[0,1898]      0.05      0.95      0.06     -1.45      1.60   1038.20      1.00\n",
            "weight[0,1899]     -0.06      1.01     -0.10     -1.91      1.44   1004.98      1.00\n",
            "weight[0,1900]      0.01      0.94      0.02     -1.38      1.60   1699.17      1.00\n",
            "weight[0,1901]     -0.12      0.95     -0.12     -1.64      1.50   1871.87      1.00\n",
            "weight[0,1902]      0.03      0.99      0.03     -1.76      1.46    841.83      1.00\n",
            "weight[0,1903]      0.05      0.95      0.02     -1.36      1.63    944.01      1.00\n",
            "weight[0,1904]      0.01      0.98      0.06     -1.44      1.62   2182.46      1.00\n",
            "weight[0,1905]      0.02      1.03      0.02     -1.72      1.73   1659.63      1.00\n",
            "weight[0,1906]     -0.06      0.95     -0.10     -1.49      1.64    927.72      1.00\n",
            "weight[0,1907]     -0.24      0.98     -0.26     -1.75      1.41    758.83      1.00\n",
            "weight[0,1908]     -0.01      1.01      0.03     -1.54      1.71   2244.26      1.00\n",
            "weight[0,1909]      0.20      0.96      0.20     -1.43      1.70   1207.68      1.00\n",
            "weight[0,1910]      0.04      1.04      0.06     -1.70      1.66   1105.76      1.00\n",
            "weight[0,1911]      0.19      1.00      0.14     -1.40      1.88   2225.80      1.00\n",
            "weight[0,1912]      0.02      1.03      0.03     -1.60      1.78   1138.19      1.00\n",
            "weight[0,1913]      0.10      1.01      0.09     -1.49      1.78   1133.82      1.00\n",
            "weight[0,1914]      0.08      1.03      0.10     -1.56      1.78   1463.17      1.00\n",
            "weight[0,1915]      0.49      0.96      0.51     -0.96      2.11   1080.42      1.00\n",
            "weight[0,1916]      0.10      0.97      0.12     -1.66      1.54   1094.71      1.00\n",
            "weight[0,1917]      0.00      0.96      0.02     -1.54      1.53   1185.96      1.00\n",
            "weight[0,1918]      0.09      0.97      0.10     -1.61      1.62   1132.70      1.00\n",
            "weight[0,1919]      0.00      0.96      0.02     -1.63      1.47   1716.87      1.00\n",
            "weight[0,1920]     -0.11      1.01     -0.08     -1.75      1.52   1372.58      1.00\n",
            "weight[0,1921]     -0.08      1.03     -0.08     -1.88      1.54   1096.15      1.00\n",
            "weight[0,1922]     -0.23      0.98     -0.23     -1.91      1.33   1415.19      1.00\n",
            "weight[0,1923]      0.29      1.00      0.35     -1.43      1.85    652.11      1.00\n",
            "weight[0,1924]      0.07      1.01      0.09     -1.59      1.66   1212.15      1.00\n",
            "weight[0,1925]     -0.22      0.95     -0.23     -1.57      1.53   2567.93      1.00\n",
            "weight[0,1926]     -0.05      0.98     -0.03     -1.64      1.62   1235.68      1.00\n",
            "weight[0,1927]      0.09      1.09      0.06     -1.71      1.81   1657.47      1.00\n",
            "weight[0,1928]     -0.04      0.99     -0.04     -1.84      1.43   1079.91      1.00\n",
            "weight[0,1929]     -0.11      0.97     -0.15     -1.72      1.43   1097.81      1.00\n",
            "weight[0,1930]     -0.01      0.98     -0.04     -1.64      1.64   1222.58      1.00\n",
            "weight[0,1931]     -0.02      1.01     -0.04     -1.70      1.61    737.56      1.00\n",
            "weight[0,1932]     -0.11      1.01     -0.06     -1.78      1.47    901.35      1.00\n",
            "weight[0,1933]     -0.02      0.97     -0.00     -1.58      1.54   1159.13      1.01\n",
            "weight[0,1934]     -0.04      0.96     -0.01     -1.65      1.45   1978.05      1.00\n",
            "weight[0,1935]     -0.11      1.01     -0.10     -1.72      1.58    665.08      1.00\n",
            "\n",
            "Number of divergences: 0\n"
          ]
        }
      ],
      "source": [
        "mcmc.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7E8wrOl9uL4O"
      },
      "outputs": [],
      "source": [
        "def create_predictions(samples: dict, dataset: torch.tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    # Create tensor of zeros with shape (number of samples x number of examples)\n",
        "    obs_samples = torch.zeros(samples[\"weight\"].shape[0], dataset.shape[0])\n",
        "    # Iterate through every set of samples\n",
        "    for i in range(samples[\"weight\"].shape[0]):\n",
        "        W_sample = samples[\"weight\"][i]\n",
        "        b_sample = samples[\"bias\"][i]\n",
        "\n",
        "        model_logits = (torch.matmul(dataset, W_sample.permute(1, 0)) + b_sample).squeeze()\n",
        "        probs = torch.sigmoid(model_logits)\n",
        "\n",
        "        obs_samples[i] = probs\n",
        "    # Calculate the mean prediction per example\n",
        "    mean_predictions = obs_samples.mean(dim=0)\n",
        "    # Classify the prediction \n",
        "    class_predictions = (obs_samples.mean(dim=0)>0.5).int()\n",
        "\n",
        "    return class_predictions, obs_samples, mean_predictions\n",
        "\n",
        "\n",
        "# Draw ungrouped samples\n",
        "samples_ungrouped = mcmc.get_samples()\n",
        "\n",
        "# Training predictions\n",
        "train_class_predictions, train_prediction_samples, train_mean_predictions = create_predictions(samples_ungrouped, X_train)\n",
        "# Test predictions\n",
        "test_class_predictions, test_prediction_samples,test_mean_predictions = create_predictions(samples_ungrouped, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_classification_metrics(true_labels:torch.Tensor, predicted_labels:torch.Tensor, prob_scores:torch.Tensor) -> dict:\n",
        "    metrics = {}\n",
        "    \n",
        "    # Accuracy\n",
        "    metrics['Accuracy'] = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "    # Precision\n",
        "    metrics['Precision'] = precision_score(true_labels, predicted_labels)\n",
        "\n",
        "    # Recall\n",
        "    metrics['Recall'] = recall_score(true_labels, predicted_labels)\n",
        "\n",
        "    # F1 Score\n",
        "    metrics['F1 Score'] = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "    # ROC Curve\n",
        "    metrics['ROC Score']  = roc_curve(true_labels, prob_scores)\n",
        "\n",
        "    # AUC-ROC\n",
        "    metrics['AUC-ROC'] = roc_auc_score(true_labels, prob_scores)\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8581560283687943\n",
            "Precision: 0.25\n",
            "Recall: 0.125\n",
            "F1 Score: 0.16666666666666666\n",
            "AUC-ROC: 0.5890000000000001\n"
          ]
        }
      ],
      "source": [
        "metrics_test = calculate_classification_metrics(y_test, test_class_predictions, test_mean_predictions)\n",
        "df_test = pd.DataFrame(metrics_test).drop(columns = ['ROC Score'], axis =1).drop_duplicates()\n",
        "df_test.index = ['Test']\n",
        "for metric, value in metrics_test.items():\n",
        "    if metric != 'ROC Score':\n",
        "        print(f\"{metric}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bayesian MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the fully connected neural network\n",
        "# Fully Connected Neural Network (MLP) with conditional dropout during inference\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "        # Define dropout layers with fixed probabilities\n",
        "        self.dropout1 = nn.Dropout(p=0.25)\n",
        "        self.dropout2 = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        # First fully connected layer\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x) if training else x  # Apply dropout1 only if training=True\n",
        "        \n",
        "        # Second fully connected layer\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x) if training else x  # Apply dropout2 only if training=True\n",
        "        \n",
        "        # Output layer\n",
        "        x = self.out(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Function to evaluate the model and return accuracy\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            all_preds.append(predicted.cpu())\n",
        "            true_labels.append(labels.cpu())\n",
        "    \n",
        "    all_preds = torch.cat(all_preds)\n",
        "    true_labels = torch.cat(true_labels)\n",
        "    \n",
        "    accuracy = accuracy_score(true_labels, all_preds)\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Acquisition functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uniform acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uniform(model, dataset, pool_indices, n_query, T=100, training=True):\n",
        "    \"\"\"\n",
        "    Uniformly random selection of data points from the unlabeled pool.\n",
        "    \n",
        "    Args:\n",
        "    pool_indices (list): List of indices available in the pool.\n",
        "    n_query (int): Number of queries to make.\n",
        "    \n",
        "    Returns:\n",
        "    list: Indices of the selected data points.\n",
        "    \"\"\"\n",
        "    # Directly use the pool_indices to select data points\n",
        "    selected_indices = np.random.choice(pool_indices, size=n_query, replace=False)\n",
        "    \n",
        "    return selected_indices.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Max Entropy acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predictions_from_pool(model, dataset, pool_indices, T=100, training=True):\n",
        "    \"\"\"\n",
        "    Run MC dropout prediction on model using graphs from the pool and return the output.\n",
        "    \"\"\"\n",
        "    # Randomly select indices from the pool\n",
        "    random_subset = np.random.choice(pool_indices, size=min(2000, len(pool_indices)), replace=False)\n",
        "    \n",
        "    # Fetch the actual data from the dataset\n",
        "    inputs = torch.stack([dataset[i][0] for i in random_subset]).to(device)  # Extract input features\n",
        "    \n",
        "    # Perform prediction\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(T):\n",
        "            model.train(training)  # Enable/disable dropout\n",
        "            output = torch.softmax(model(inputs, training=training), dim=-1)  # Forward pass with MC Dropout\n",
        "            outputs.append(output.cpu().numpy())\n",
        "    \n",
        "    outputs = np.stack(outputs)\n",
        "    print(\"outputs shape: \",outputs.shape)\n",
        "    \n",
        "    return outputs, random_subset\n",
        "\n",
        "def shannon_entropy_function(model, dataset, pool_indices, T=100, E_H=False, training=True):\n",
        "    \"\"\"\n",
        "    Compute the Shannon entropy and optionally E_H if needed for BALD.\n",
        "    \"\"\"\n",
        "    outputs, random_subset = predictions_from_pool(model, dataset, pool_indices, T, training)\n",
        "    pc = outputs.mean(axis=0)\n",
        "    H = (-pc * np.log(pc + 1e-10)).sum(axis=-1)  # Prevent log(0)\n",
        "\n",
        "    if E_H:\n",
        "        E = -np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)\n",
        "        return H, E, random_subset\n",
        "    return H, random_subset\n",
        "\n",
        "def max_entropy(model, dataset, pool_indices, n_query=10, T=100, training=True):\n",
        "    \"\"\"\n",
        "    Choose pool points that maximize the predictive entropy.\n",
        "    \"\"\"\n",
        "    acquisition, random_subset = shannon_entropy_function(model, dataset, pool_indices, T, training=training)\n",
        "    idx = (-acquisition).argsort()[:n_query]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def active_learning_loop(model, dataset, initial_indices, pool_indices, val_indices, test_indices, strategy, n_query=10, epochs=100):\n",
        "    accuracies = []\n",
        "\n",
        "    # Extract the features (X) and labels (y) from the dataset\n",
        "    X_all, y_all = dataset.tensors\n",
        "    \n",
        "    # Create data loaders with selected indices\n",
        "    train_loader = DataLoader(TensorDataset(X_all[initial_indices], y_all[initial_indices]), batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(X_all[val_indices], y_all[val_indices]), batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(TensorDataset(X_all[test_indices], y_all[test_indices]), batch_size=32, shuffle=False)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Initial training\n",
        "    print(\"Initial Training Phase\")\n",
        "    train_model(model, train_loader, criterion, optimizer, epochs=20)\n",
        "    val_acc = evaluate_model(model, val_loader)\n",
        "    accuracies.append(val_acc)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Active Learning Epoch {epoch + 1}\")\n",
        "        train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
        "        val_acc = evaluate_model(model, val_loader)\n",
        "        accuracies.append(val_acc)\n",
        "\n",
        "        # Acquire new samples based on the selected strategy (e.g., max_entropy, uniform)\n",
        "        new_indices = strategy(model, dataset, pool_indices, n_query)\n",
        "        initial_indices = np.concatenate([initial_indices, new_indices])\n",
        "        pool_indices = np.setdiff1d(pool_indices, new_indices)\n",
        "\n",
        "        # Recreate train_loader with updated indices\n",
        "        train_loader = DataLoader(TensorDataset(X_all[initial_indices], y_all[initial_indices]), batch_size=32, shuffle=True)\n",
        "        print(f\"New training data size: {len(initial_indices)}, Remaining pool size: {len(pool_indices)}\")\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    test_acc = evaluate_model(model, test_loader)\n",
        "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    return accuracies\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Training Phase\n",
            "Epoch [1/20], Loss: 0.5545\n",
            "Epoch [2/20], Loss: 0.2821\n",
            "Epoch [3/20], Loss: 0.2293\n",
            "Epoch [4/20], Loss: 0.1463\n",
            "Epoch [5/20], Loss: 0.1048\n",
            "Epoch [6/20], Loss: 0.0772\n",
            "Epoch [7/20], Loss: 0.0584\n",
            "Epoch [8/20], Loss: 0.0489\n",
            "Epoch [9/20], Loss: 0.0378\n",
            "Epoch [10/20], Loss: 0.0166\n",
            "Epoch [11/20], Loss: 0.0148\n",
            "Epoch [12/20], Loss: 0.0095\n",
            "Epoch [13/20], Loss: 0.0076\n",
            "Epoch [14/20], Loss: 0.0037\n",
            "Epoch [15/20], Loss: 0.0047\n",
            "Epoch [16/20], Loss: 0.0026\n",
            "Epoch [17/20], Loss: 0.0047\n",
            "Epoch [18/20], Loss: 0.0021\n",
            "Epoch [19/20], Loss: 0.0029\n",
            "Epoch [20/20], Loss: 0.0022\n",
            "Accuracy: 0.8200\n",
            "Active Learning Epoch 1\n",
            "Epoch [1/10], Loss: 0.0027\n",
            "Epoch [2/10], Loss: 0.0014\n",
            "Epoch [3/10], Loss: 0.0012\n",
            "Epoch [4/10], Loss: 0.0008\n",
            "Epoch [5/10], Loss: 0.0005\n",
            "Epoch [6/10], Loss: 0.0005\n",
            "Epoch [7/10], Loss: 0.0003\n",
            "Epoch [8/10], Loss: 0.0017\n",
            "Epoch [9/10], Loss: 0.0005\n",
            "Epoch [10/10], Loss: 0.0007\n",
            "Accuracy: 0.8200\n",
            "outputs shape:  (100, 364, 2)\n",
            "New training data size: 110, Remaining pool size: 354\n",
            "Active Learning Epoch 2\n",
            "Epoch [1/10], Loss: 0.0586\n",
            "Epoch [2/10], Loss: 0.0011\n",
            "Epoch [3/10], Loss: 0.0103\n",
            "Epoch [4/10], Loss: 0.0037\n",
            "Epoch [5/10], Loss: 0.0043\n",
            "Epoch [6/10], Loss: 0.0023\n",
            "Epoch [7/10], Loss: 0.0011\n",
            "Epoch [8/10], Loss: 0.0012\n",
            "Epoch [9/10], Loss: 0.0028\n",
            "Epoch [10/10], Loss: 0.0003\n",
            "Accuracy: 0.8300\n",
            "outputs shape:  (100, 354, 2)\n",
            "New training data size: 120, Remaining pool size: 344\n",
            "Active Learning Epoch 3\n",
            "Epoch [1/10], Loss: 0.0428\n",
            "Epoch [2/10], Loss: 0.0024\n",
            "Epoch [3/10], Loss: 0.0098\n",
            "Epoch [4/10], Loss: 0.0035\n",
            "Epoch [5/10], Loss: 0.0058\n",
            "Epoch [6/10], Loss: 0.0024\n",
            "Epoch [7/10], Loss: 0.0004\n",
            "Epoch [8/10], Loss: 0.0018\n",
            "Epoch [9/10], Loss: 0.0002\n",
            "Epoch [10/10], Loss: 0.0011\n",
            "Accuracy: 0.8400\n",
            "outputs shape:  (100, 344, 2)\n",
            "New training data size: 130, Remaining pool size: 334\n",
            "Active Learning Epoch 4\n",
            "Epoch [1/10], Loss: 0.0783\n",
            "Epoch [2/10], Loss: 0.0508\n",
            "Epoch [3/10], Loss: 0.0036\n",
            "Epoch [4/10], Loss: 0.0022\n",
            "Epoch [5/10], Loss: 0.0237\n",
            "Epoch [6/10], Loss: 0.0058\n",
            "Epoch [7/10], Loss: 0.0218\n",
            "Epoch [8/10], Loss: 0.0077\n",
            "Epoch [9/10], Loss: 0.0012\n",
            "Epoch [10/10], Loss: 0.0008\n",
            "Accuracy: 0.8200\n",
            "outputs shape:  (100, 334, 2)\n",
            "New training data size: 140, Remaining pool size: 324\n",
            "Active Learning Epoch 5\n",
            "Epoch [1/10], Loss: 0.0685\n",
            "Epoch [2/10], Loss: 0.0203\n",
            "Epoch [3/10], Loss: 0.0021\n",
            "Epoch [4/10], Loss: 0.0008\n",
            "Epoch [5/10], Loss: 0.0429\n",
            "Epoch [6/10], Loss: 0.0045\n",
            "Epoch [7/10], Loss: 0.0002\n",
            "Epoch [8/10], Loss: 0.0012\n",
            "Epoch [9/10], Loss: 0.0004\n",
            "Epoch [10/10], Loss: 0.0002\n",
            "Accuracy: 0.8500\n",
            "outputs shape:  (100, 324, 2)\n",
            "New training data size: 150, Remaining pool size: 314\n",
            "Active Learning Epoch 6\n",
            "Epoch [1/10], Loss: 0.2093\n",
            "Epoch [2/10], Loss: 0.0337\n",
            "Epoch [3/10], Loss: 0.0028\n",
            "Epoch [4/10], Loss: 0.0032\n",
            "Epoch [5/10], Loss: 0.0047\n",
            "Epoch [6/10], Loss: 0.0637\n",
            "Epoch [7/10], Loss: 0.0061\n",
            "Epoch [8/10], Loss: 0.0053\n",
            "Epoch [9/10], Loss: 0.0016\n",
            "Epoch [10/10], Loss: 0.0008\n",
            "Accuracy: 0.8400\n",
            "outputs shape:  (100, 314, 2)\n",
            "New training data size: 160, Remaining pool size: 304\n",
            "Active Learning Epoch 7\n",
            "Epoch [1/10], Loss: 0.0714\n",
            "Epoch [2/10], Loss: 0.0210\n",
            "Epoch [3/10], Loss: 0.0050\n",
            "Epoch [4/10], Loss: 0.0006\n",
            "Epoch [5/10], Loss: 0.0017\n",
            "Epoch [6/10], Loss: 0.0019\n",
            "Epoch [7/10], Loss: 0.0003\n",
            "Epoch [8/10], Loss: 0.0019\n",
            "Epoch [9/10], Loss: 0.0005\n",
            "Epoch [10/10], Loss: 0.0003\n",
            "Accuracy: 0.8400\n",
            "outputs shape:  (100, 304, 2)\n",
            "New training data size: 170, Remaining pool size: 294\n",
            "Active Learning Epoch 8\n",
            "Epoch [1/10], Loss: 0.1759\n",
            "Epoch [2/10], Loss: 0.0100\n",
            "Epoch [3/10], Loss: 0.0206\n",
            "Epoch [4/10], Loss: 0.0023\n",
            "Epoch [5/10], Loss: 0.0052\n",
            "Epoch [6/10], Loss: 0.0038\n",
            "Epoch [7/10], Loss: 0.0039\n",
            "Epoch [8/10], Loss: 0.0011\n",
            "Epoch [9/10], Loss: 0.0011\n",
            "Epoch [10/10], Loss: 0.0003\n",
            "Accuracy: 0.8600\n",
            "outputs shape:  (100, 294, 2)\n",
            "New training data size: 180, Remaining pool size: 284\n",
            "Active Learning Epoch 9\n",
            "Epoch [1/10], Loss: 0.0716\n",
            "Epoch [2/10], Loss: 0.0133\n",
            "Epoch [3/10], Loss: 0.0037\n",
            "Epoch [4/10], Loss: 0.0025\n",
            "Epoch [5/10], Loss: 0.0018\n",
            "Epoch [6/10], Loss: 0.0010\n",
            "Epoch [7/10], Loss: 0.0090\n",
            "Epoch [8/10], Loss: 0.0014\n",
            "Epoch [9/10], Loss: 0.0017\n",
            "Epoch [10/10], Loss: 0.0028\n",
            "Accuracy: 0.8400\n",
            "outputs shape:  (100, 284, 2)\n",
            "New training data size: 190, Remaining pool size: 274\n",
            "Active Learning Epoch 10\n",
            "Epoch [1/10], Loss: 0.0573\n",
            "Epoch [2/10], Loss: 0.0071\n",
            "Epoch [3/10], Loss: 0.0040\n",
            "Epoch [4/10], Loss: 0.0054\n",
            "Epoch [5/10], Loss: 0.0114\n",
            "Epoch [6/10], Loss: 0.0012\n",
            "Epoch [7/10], Loss: 0.0066\n",
            "Epoch [8/10], Loss: 0.0005\n",
            "Epoch [9/10], Loss: 0.0004\n",
            "Epoch [10/10], Loss: 0.0004\n",
            "Accuracy: 0.8500\n",
            "outputs shape:  (100, 274, 2)\n",
            "New training data size: 200, Remaining pool size: 264\n",
            "Active Learning Epoch 11\n",
            "Epoch [1/10], Loss: 0.0009\n",
            "Epoch [2/10], Loss: 0.0005\n",
            "Epoch [3/10], Loss: 0.0004\n",
            "Epoch [4/10], Loss: 0.0003\n",
            "Epoch [5/10], Loss: 0.0003\n",
            "Epoch [6/10], Loss: 0.0002\n",
            "Epoch [7/10], Loss: 0.0004\n",
            "Epoch [8/10], Loss: 0.0002\n",
            "Epoch [9/10], Loss: 0.0002\n",
            "Epoch [10/10], Loss: 0.0001\n",
            "Accuracy: 0.8500\n",
            "outputs shape:  (100, 264, 2)\n",
            "New training data size: 210, Remaining pool size: 254\n",
            "Active Learning Epoch 12\n",
            "Epoch [1/10], Loss: 0.1008\n",
            "Epoch [2/10], Loss: 0.0189\n",
            "Epoch [3/10], Loss: 0.0191\n",
            "Epoch [4/10], Loss: 0.0010\n",
            "Epoch [5/10], Loss: 0.0053\n",
            "Epoch [6/10], Loss: 0.0056\n",
            "Epoch [7/10], Loss: 0.0061\n",
            "Epoch [8/10], Loss: 0.0016\n",
            "Epoch [9/10], Loss: 0.0036\n",
            "Epoch [10/10], Loss: 0.0008\n",
            "Accuracy: 0.8500\n",
            "outputs shape:  (100, 254, 2)\n",
            "New training data size: 220, Remaining pool size: 244\n",
            "Active Learning Epoch 13\n",
            "Epoch [1/10], Loss: 0.1530\n",
            "Epoch [2/10], Loss: 0.0169\n",
            "Epoch [3/10], Loss: 0.0069\n",
            "Epoch [4/10], Loss: 0.0071\n",
            "Epoch [5/10], Loss: 0.0023\n",
            "Epoch [6/10], Loss: 0.0023\n",
            "Epoch [7/10], Loss: 0.0015\n",
            "Epoch [8/10], Loss: 0.0016\n",
            "Epoch [9/10], Loss: 0.0006\n",
            "Epoch [10/10], Loss: 0.0008\n",
            "Accuracy: 0.8300\n",
            "outputs shape:  (100, 244, 2)\n",
            "New training data size: 230, Remaining pool size: 234\n",
            "Active Learning Epoch 14\n",
            "Epoch [1/10], Loss: 0.0005\n",
            "Epoch [2/10], Loss: 0.0005\n",
            "Epoch [3/10], Loss: 0.0003\n",
            "Epoch [4/10], Loss: 0.0009\n",
            "Epoch [5/10], Loss: 0.0016\n",
            "Epoch [6/10], Loss: 0.0002\n",
            "Epoch [7/10], Loss: 0.0015\n",
            "Epoch [8/10], Loss: 0.0002\n",
            "Epoch [9/10], Loss: 0.0001\n",
            "Epoch [10/10], Loss: 0.0002\n",
            "Accuracy: 0.8400\n",
            "outputs shape:  (100, 234, 2)\n",
            "New training data size: 240, Remaining pool size: 224\n",
            "Active Learning Epoch 15\n",
            "Epoch [1/10], Loss: 0.0741\n",
            "Epoch [2/10], Loss: 0.0181\n",
            "Epoch [3/10], Loss: 0.0026\n",
            "Epoch [4/10], Loss: 0.0231\n",
            "Epoch [5/10], Loss: 0.0015\n",
            "Epoch [6/10], Loss: 0.0021\n",
            "Epoch [7/10], Loss: 0.0032\n",
            "Epoch [8/10], Loss: 0.0008\n",
            "Epoch [9/10], Loss: 0.0079\n",
            "Epoch [10/10], Loss: 0.0017\n",
            "Accuracy: 0.8300\n",
            "outputs shape:  (100, 224, 2)\n",
            "New training data size: 250, Remaining pool size: 214\n",
            "Active Learning Epoch 16\n",
            "Epoch [1/10], Loss: 0.0648\n",
            "Epoch [2/10], Loss: 0.0075\n",
            "Epoch [3/10], Loss: 0.0042\n",
            "Epoch [4/10], Loss: 0.0087\n",
            "Epoch [5/10], Loss: 0.0079\n",
            "Epoch [6/10], Loss: 0.0004\n",
            "Epoch [7/10], Loss: 0.0026\n",
            "Epoch [8/10], Loss: 0.0013\n",
            "Epoch [9/10], Loss: 0.0034\n",
            "Epoch [10/10], Loss: 0.0005\n",
            "Accuracy: 0.8400\n",
            "outputs shape:  (100, 214, 2)\n",
            "New training data size: 260, Remaining pool size: 204\n",
            "Active Learning Epoch 17\n",
            "Epoch [1/10], Loss: 0.2377\n",
            "Epoch [2/10], Loss: 0.0107\n",
            "Epoch [3/10], Loss: 0.0028\n",
            "Epoch [4/10], Loss: 0.0023\n",
            "Epoch [5/10], Loss: 0.0005\n",
            "Epoch [6/10], Loss: 0.0139\n",
            "Epoch [7/10], Loss: 0.0061\n",
            "Epoch [8/10], Loss: 0.0009\n",
            "Epoch [9/10], Loss: 0.0004\n",
            "Epoch [10/10], Loss: 0.0015\n",
            "Accuracy: 0.8400\n",
            "outputs shape:  (100, 204, 2)\n",
            "New training data size: 270, Remaining pool size: 194\n",
            "Active Learning Epoch 18\n",
            "Epoch [1/10], Loss: 0.0583\n",
            "Epoch [2/10], Loss: 0.0029\n",
            "Epoch [3/10], Loss: 0.0031\n",
            "Epoch [4/10], Loss: 0.0067\n",
            "Epoch [5/10], Loss: 0.0011\n",
            "Epoch [6/10], Loss: 0.0006\n",
            "Epoch [7/10], Loss: 0.0003\n",
            "Epoch [8/10], Loss: 0.0007\n",
            "Epoch [9/10], Loss: 0.0001\n",
            "Epoch [10/10], Loss: 0.0012\n",
            "Accuracy: 0.8600\n",
            "outputs shape:  (100, 194, 2)\n",
            "New training data size: 280, Remaining pool size: 184\n",
            "Active Learning Epoch 19\n",
            "Epoch [1/10], Loss: 0.0342\n",
            "Epoch [2/10], Loss: 0.0053\n",
            "Epoch [3/10], Loss: 0.0026\n",
            "Epoch [4/10], Loss: 0.0008\n",
            "Epoch [5/10], Loss: 0.0013\n",
            "Epoch [6/10], Loss: 0.0005\n",
            "Epoch [7/10], Loss: 0.0016\n",
            "Epoch [8/10], Loss: 0.0002\n",
            "Epoch [9/10], Loss: 0.0005\n",
            "Epoch [10/10], Loss: 0.0002\n",
            "Accuracy: 0.8400\n",
            "outputs shape:  (100, 184, 2)\n",
            "New training data size: 290, Remaining pool size: 174\n",
            "Active Learning Epoch 20\n",
            "Epoch [1/10], Loss: 0.0017\n",
            "Epoch [2/10], Loss: 0.0003\n",
            "Epoch [3/10], Loss: 0.0012\n",
            "Epoch [4/10], Loss: 0.0036\n",
            "Epoch [5/10], Loss: 0.0011\n",
            "Epoch [6/10], Loss: 0.0001\n",
            "Epoch [7/10], Loss: 0.0002\n",
            "Epoch [8/10], Loss: 0.0002\n",
            "Epoch [9/10], Loss: 0.0008\n",
            "Epoch [10/10], Loss: 0.0001\n",
            "Accuracy: 0.8600\n",
            "outputs shape:  (100, 174, 2)\n",
            "New training data size: 300, Remaining pool size: 164\n",
            "Accuracy: 0.8794\n",
            "Final Test Accuracy: 0.8794\n"
          ]
        }
      ],
      "source": [
        "X = data.drop(columns=['vital.status'])  # Drop the target column to get the features\n",
        "y = data['vital.status']  # Target variable (0 = survived, 1 = died)\n",
        "\n",
        "# Standardize the feature values\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to torch tensors\n",
        "X = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "y = torch.tensor(y.values, dtype=torch.long)  # Ensure target is of type long for classification\n",
        "\n",
        "# Step 2: Split the data into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Combine all data for the active learning dataset\n",
        "X_all = torch.cat([X_train, X_test])\n",
        "y_all = torch.cat([y_train, y_test])\n",
        "dataset = TensorDataset(X_all, y_all)\n",
        "\n",
        "# Initial labeled indices, pool, validation, and test indices\n",
        "initial_indices = np.random.choice(len(X_train), size=100, replace=False)  # Start with 100 random samples\n",
        "pool_indices = np.setdiff1d(np.arange(len(X_train)), initial_indices)  # Remaining unlabeled pool\n",
        "val_indices = np.random.choice(pool_indices, size=100, replace=False)  # Validation set from the pool\n",
        "pool_indices = np.setdiff1d(pool_indices, val_indices)\n",
        "test_indices = np.arange(len(X_train), len(X_all))  # Test set is the entire X_test\n",
        "\n",
        "\n",
        "dataset = TensorDataset(X_all, y_all)\n",
        "\n",
        "# Initial labeled indices, pool, validation, and test indices\n",
        "initial_indices = np.random.choice(len(X_train), size=100, replace=False)  # Start with 100 random samples\n",
        "pool_indices = np.setdiff1d(np.arange(len(X_train)), initial_indices)  # Remaining unlabeled pool\n",
        "val_indices = np.random.choice(pool_indices, size=100, replace=False)  # Validation set from the pool\n",
        "pool_indices = np.setdiff1d(pool_indices, val_indices)\n",
        "test_indices = np.arange(len(X_train), len(X_all))  # Test set is the entire X_test\n",
        "\n",
        "# Model and active learning setup\n",
        "input_size = X_train.shape[1]  # Number of features\n",
        "hidden_size = 128\n",
        "num_classes = len(torch.unique(y_train)) \n",
        "\n",
        "model=MLP(input_size, hidden_size, num_classes).to(device)\n",
        "# Example usage of BALD with the updated MLP and dropout during inference\n",
        "strategy = max_entropy  # You can switch this to max_entropy or uniform as needed\n",
        "\n",
        "# Run the active learning loop\n",
        "accuracies = active_learning_loop(model, dataset, initial_indices, pool_indices, val_indices, test_indices, strategy, n_query=10, epochs=20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
